\section{Стохастические модели с дискретным временем}
Приступим к изучению \emph{временных рядов}, то есть случайных процессов с 
множеством индекосв \(T = \Z_{+}\). Пусть единица времени измерения есть сутки, 
а \(S = (S_{n})_{n \geq 0}\)~--- это какой-то финансовый индекс (например, 
раночная цена акции или же обменный курс валют). Практика показывает, что 
\(S_{n}\) ведёт себя весьма нерегулярно. Это привело Луи Башелье к идее 
использования аппарата теории вероятностей для изучения эмпирических феноменов, 
характеризующихся статистической неопределённостью, но при этом обладающих 
статистической устойчивостью частот.

Как обычно, будем считать, что все наблюдения проводятся на некотором 
вероятностном прострнастве \((\Omega, \F, \Pr)\). Однако время и динамика 
являются неотъемлимыми частями финансовой теории, в связи с чем целесообразно 
специфицировать вероятностное пространство, добавив 
\hyperref[def-filtration]{фильтрацию} \((\F_{n})_{n \geq 0}\). Интуитивно можно 
понимать, что \(\F_{n}\) есть доступная наблюдателю ``информация'' о рынке 
вплоть до момента времени \(n\). Получаемая четвёрка \((\Omega, \F, (\F_{n})_{n 
\geq 0}, \Pr)\) называется \emph{фильтрованным вероятностным пространством} или 
же \emph{стохастическим базисом}. Впрочем, во многих случаях целесообразно 
вводить не одну вероятностную меру, а целое семейство \(\mathcal{P} = \{\Pr\}\) 
(это связано с тем, что бывает трудно выбрать какую-то конкретную меру 
\(\Pr\)). Полученный набор объектов \((\Omega, \F, (\F_{n})_{n \geq 0}, 
\mathcal{P})\) можно назвать \emph{фильтрованным статистическим экспериментом}.

Разумно предположить, что \(S_{n}\) полностью задаётся тем, что произошло до 
момента времени \(n\) включительно. Формально говоря, \(S_{n}\) является 
\(\F_{n}\)-измеримой случайной величиной.\footnote{То есть для любого \(B \in 
\B(\R)\) \(S_{n}^{-1}(B) \in \F_{n}\).}

Теперь предположим, что для всех \(n \geq 0\) \(S_{n} > 0\). Как можно 
охарактеризовать \(S_{n}\)? Есть два способа.

Первый метод похож на метод сложных процентов (то есть проценты выплачиваются 
непрерывно). В нём \(S_{n} = S_{0}e^{H_{n}}\), где \(H_{n} = h_{0} + h_{1} + 
\ldots + h_{n}\), \(h_{0} = 0\), а \(h_{n}\) есть \(\F_{n}\)-измеримая 
случайная величина. Несложно 
понять, что
\[
	H_{n} = \ln\frac{S_{n}}{S_{0}}, \quad h_{n} = \ln\frac{S_{n}}{S_{n - 1}} = 
	\ln\left(1 + \frac{\Delta S_{n}}{S_{n - 1}}\right), \text{ где } \Delta 
	S_{n} = S_{n} - S_{n - 1}.
\]

Теперь введём следующие обозначения:
\[
	\hat{h}_{n} = \frac{\Delta S_{n}}{S_{n - 1}}, \quad \hat{H}_{n} = \sum_{k = 
	1}^{n} \hat{h}_{k}.
\]

Тогда 
\[
	S_{n} = S_{0} \prod_{k = 1}^{n}e^{h_{k}} = S_{0} \prod_{k = 1}^{n} (1 + 
	\hat{h}_{k}) = S_{0} \prod_{k = 1}^{n} (1 + \Delta\hat{H}_{k}) = 
	S_{0}e^{\hat{H}_{n}} \prod_{k = 1}^{n} (1 + \Delta\hat{H}_{k}) 
	e^{-\Delta\hat{H}_{k}}.
\]

Полученное разложение задаёт второй метод, который похож на метод простых 
процентов.

Теперь введём обозначение
\[
	\mathcal{E}(\hat{H})_{n} \equiv e^{\hat{H}_{n}} \prod_{k = 1}^{n} (1 + 
	\Delta\hat{H}_{k}) e^{-\Delta\hat{H}_{k}}.
\]

Получаемая случайная последовательность \(\mathcal{E}(\hat{H}) \equiv 
(\mathcal{E}(\hat{H})_{n})_{n \geq 0}\) называется \emph{экспонентой Долеан} 
или \emph{стохастической экспонентой}, порождённой случайной 
последовательностью \(\hat{H} = (\hat{H}_{n})_{n \geq 0}\). Несложно понять, 
что в первом методе используется обычная экспонента: \(S_{n} = 
S_{0}e^{H_{n}}\), а во втором~--- стохастическая: \(S_{n} = S_{0} 
\mathcal{E}(\hat{H})_{n}\).

Теперь покажем взаимосвязь между \(H_{n}\) и \(\hat{H}_{n}\). Заметим, что
\[
	\hat{H}_{n} = \sum_{k = 1}^{n} \hat{h}_{k} = \sum_{k = 1}^{n}(e^{h_{k}} - 
	1) = \sum_{k = 1}^{n}(e^{\Delta H_{k}} - 1).
\]

Тогда
\[
	\hat{H}_{n} = H_{n} + (\hat{H}_{n} - H_{n}) = H_{n} + \sum_{k = 
	1}^{n}(e^{\Delta H_{k}} - 1 - \Delta H_{k}).
\]

Также понятно, что
\[
	H_{n} = \sum_{k = 1}^{n} h_{k} = \sum_{k = 1}^{n} \ln(1 + \hat{h}_{k}) = 
	\sum_{k = 1}^{n} \ln(1 + \Delta \hat{H}_{k}).
\]

Теперь покажем, что для \(\mathcal{E}(\hat{H})\) верно следующее разностное 
уравнение, от которого и пошло название:
\[
	\Delta\mathcal{E}(\hat{H})_{n} = \mathcal{E}(\hat{H})_{n - 
	1}\Delta\hat{H}_{n}, \quad \mathcal{E}(\hat{H})_{0} = 1.
\]

Действительно,
\begin{align*}
	\Delta\mathcal{E}(\hat{H})_{n} &= \mathcal{E}(\hat{H})_{n} - 
	\mathcal{E}(\hat{H})_{n - 1} = e^{\hat{H}_{n}} \prod_{k = 1}^{n} (1 + 
	\Delta\hat{H}_{k}) e^{-\Delta\hat{H}_{k}} - e^{\hat{H}_{n - 1}} \prod_{k = 
	1}^{n - 1} (1 + \Delta\hat{H}_{k}) e^{-\Delta\hat{H}_{k}} = \\
	&= (e^{\Delta\hat{H}_{n}}(1 + \Delta\hat{H}_{n})e^{-\Delta\hat{H}_{n}} - 
	1)e^{\hat{H}_{n - 1}} \prod_{k = 1}^{n - 1} (1 + \Delta\hat{H}_{k}) 
	e^{-\Delta\hat{H}_{k}} = \\
	&= \Delta\hat{H}_{n}e^{\hat{H}_{n - 1}} \prod_{k = 1}^{n - 1} (1 + 
	\Delta\hat{H}_{k}) e^{-\Delta\hat{H}_{k}} = \mathcal{E}(\hat{H})_{n - 
	1}\Delta\hat{H}_{n}.
\end{align*}
\begin{remark}
	Несложно заметить, что при малых \(h_{k}\) \(\hat{h}_{k} \approx h_{k}\), 
	причём
	\[
		\hat{h}_{k} - h_{k} = \frac{1}{2}h_{k}^{2} + \frac{1}{6}h_{k}^{3} + 
		\ldots
	\]
\end{remark}

Впрочем, на данный момент ограничимся описанием распределения \(S = (S_{n})_{n 
\geq 0}\) и \(H = (H_{n})_{n \geq 0}\). С точки зрения классической теории 
вероятностей и продвинутой ``статистики нормального распределения'' было бы 
хорошо, если бы последовательность \(H = (H_{n})_{n \geq 0}\) была гауссовской. 
Если \(H_{n} = h_{1} + \ldots + h_{n}\), \(n \geq 1\), то распределение 
\(H_{n}\) полностью бы задавалось распределением последовательности \(h = 
(h_{n})_{n \geq 1}\). Однако она полностью задаётся двумя параметрами: 
матожиданием \(\mu_{n} = \E{h_{n}}\) и ковариациями \(\cov(h_{m}, h_{n})\).

Предположение о нормальности существенно упрощает решение многих задач, 
связанных с свойствами распределений. Например, 
\hyperref[normal-correlation-theorem]{теорема о нормальной корреляции} даёт 
формулу для вычисления условного математического ожидания \(\tilde{h}_{n 
+ 1} = \E{h_{n + 1} \given h_{1}, \ldots, h_{n}}\):
\[
	\tilde{h}_{n + 1} = \mu_{n + 1} + \sum_{k = 1}^{n}a_{k}(h_{k} - \mu_{k}),
\] 
где \(a_{k}\)~--- коэффициенты, задаваемые матрицей ковариаций. Оказывается, 
что \(\tilde{h}_{n + 1}\) является \emph{оптимальной} в среднеквадратичном 
смысле оценкой \(h_{n + 1}\) по \(h_{1}, \ldots, h_{n}\), то есть матожидание 
квадрата отклонения минимально.

Что будет в случае, если все \(h_{k}\) независимы? В таком случае матрица 
ковариаций диагональна и
\[
	\tilde{h}_{n + 1} = \E{h_{n + 1}} + \sum_{k = 1}^{n}\frac{\cov(h_{n + 1}, 
	h_{k})}{\D{h_{k}}}(h_{k} - \E{h_{k}}).
\]

Формула для ошибки оценивания будет иметь вид (проверьте!):
\[
	\Delta_{n + 1} = \E{(\tilde{h}_{n + 1} - h_{n + 1})^{2}} = \D{h_{n + 1}} - 
	\sum_{k = 1}^{n}\frac{\cov^{2}(h_{n + 1}, h_{i})}{\D{h_{i}}}
\]

Теперь вспомним один факт, связанный с нормальным распределением: с 
вероятностью около 90\% значение случайной величины \(\xi \sim \mathcal{N}(\mu, 
\sigma^{2})\) будет лежать в интервале \([\mu - 1,65\sigma, \mu + 
1,65\sigma]\). Тогда, пользуясь тем, что \(h_{n + 1} - \tilde{h}_{n + 1} 
\sim \mathcal{N}(0, \Delta_{n + 1})\), получаем, что
\[
	\Pr{|h_{n + 1} - \tilde{h}_{n + 1}| \leq 1,65\sqrt{\Delta_{n + 1}}} \approx 
	0,90.
\]

Отсюда получаем, что в 90\% случаев прогнозируемое значение \(\tilde{S}_{n + 
1}\) величины рыночной цены (по наблюдениям \(h_{1}, \ldots, h_{n}\)) лежит в 
интервале 
\[
	[S_{n}e^{\tilde{h}_{n + 1} - 1,65\sqrt{\Delta_{n + 1}}}, 
	S_{n}e^{\tilde{h}_{n + 1} + 1,65\sqrt{\Delta_{n + 1}}}].
\]

Впрочем, к гипотезе нормальности нужно относиться с осторожностью. Практика 
показывает, что
\begin{itemize}
	\item Число выборочных значений, не попадающих в интервал 
	\([\overline{h}_{n} - k\hat{\sigma}_{n}, \overline{h}_{n} + 
	k\hat{\sigma}_{n}]\), \(k = 1, 2, 3\), где 
	\begin{gather*}
		\overline{h}_{n} = \frac{1}{n}\sum_{k = 1}^{n} h_{k} \text{~--- 
		выборочное среднее,} \\
		\hat{\sigma}_{n} = \frac{1}{n - 1}\sum_{k = 1}^{n} (h_{k} - 
		\overline{h}_{n})^{2}\text{~--- выборочное стандартное отклонение,} 
	\end{gather*}
	значительно больше, чем это должно быть при гипотезе нормальности. Это 
	означает, что ``хвосты'' эмпирических распределений убывают значительно 
	медленнее, чем у гауссовского распределения (тяжёлые хвосты).
	\item Может оказаться так, что \emph{эксцесс}, или коэффициент вытянутости:
	\[
		\hat{k}_{n} = \frac{\hat{m}_{4}}{\hat{m}_{2}^{2}} - 3,
	\]
	где \(\hat{m}_{k}\) есть выборочный \(k\)-й момент, получается 
	положительным (хотя для нормального распределения он должен быть нулевым). 
	Это означает сильную вытянутость пика плотности распределения в окрестности 
	центральных значений.
\end{itemize}

Пожалуй, самым сильным предположением (относительно структуры распределения 
величин \(h = (h_{n})\)) является, помимо нормальности, предположение 
\emph{независимости и одинаковой распределённости} этих величин. В таком случае 
анализ цен легко проводится с помощью обычных методов теории вероятностей. 
Однако при таком предположении сразу же рушится надежда на то, что прошлые 
данные хоть как-то влияют на будущие.

Предположим, что в модели
\[
	S_{n} = S_{0}e^{H_{n}}, \quad H_{n} = h_{1} + \ldots + h_{n},
\]
случайные величины \(h_{n}\) имеют конечные абсолютные первые моменты: 
\(\E{|h_{n}|} < +\infty\). 

\emph{Разложение Дуба}, о котором пойдёт речь дальше, предполагает изучение 
последовательности \(H = (H_{n})\) в зависимости от свойств фильтрации 
\((F_{n})_{n \geq 0}\), то есть потока информаций, доступных наблюдателю. 
Положим \(\F_{0} = \{\emptyset, \Omega\}\).

Так как \(\E{|h_{n}|} < +\infty\), \(n \geq 1\), то определены условные 
математические ожидания \(\E{h_{n} \given \F_{n - 1}}\). Тогда
\[
	H_{n} = \sum_{k = 1}^{n} h_{k} = \sum_{k = 1}^{n} \E{h_{k} \given \F_{k - 
	1}} + \sum_{k = 1}^{n} (h_{k} - \E{h_{k} \given \F_{k - 1}}).
\]

Если ввести обозначения
\begin{align*}
	A_{n} &= \sum_{k = 1}^{n} \E{h_{k} \given \F_{k - 1}}, \\
	M_{n} &= \sum_{k = 1}^{n} (h_{k} - \E{h_{k} \given \F_{k - 1}}),
\end{align*}
то для \(H = (H_{n})_{n \geq 0}, H_{0} = 0\) справедливо разложение Дуба
\[
	H_{n} = A_{n} + M_{n}, \quad n \geq 0,
\]
где
\begin{itemize}
	\item \(A = (A_{n})_{n \geq 0}, A_{0} = 0\) является \emph{предсказуемой} 
	случайной последовательностью. Другими словами, для любого \(n \geq 1\) 
	\(A_{n}\) есть \(\F_{n - 1}\)-измеримой случайная величина.
	\item \(M = (M_{n})_{n \geq 0}, M_{0} = 0\) является \emph{мартингалом}, то 
	есть для любого \(n \geq 1\) \(\E{M_{n} \given \F_{n - 1}} = M_{n - 1}\), 
	причём \(M_{n}\) есть \(\F_{n}\)-измеримые величины и \(\E{|M_{n}|} < 
	\infty\).
\end{itemize}
\begin{remark}
	Предположим, что наряду с фильтрацией \((\F_{n})\) задана 
	\emph{подфильтрация} \((\mathcal{G}_{n})\), где \(\mathcal{G}_{n} \subseteq 
	\F_{n}\) и \(\mathcal{G}_{n} \subseteq \mathcal{G}_{n + 1}\). Аналогичным 
	образом можно написать разложение \(H = (H_{n})\) относительно потока 
	\((\mathcal{G}_{n})\):
	\[
		H_{n} = \sum_{k = 1}^{n} \E{h_{k} \given \mathcal{G}_{k - 1}} + \sum_{k 
		= 1}^{n} (h_{k} - \E{h_{k} \given \mathcal{G}_{k - 1}}).
	\]
	
	Последовательность \(A = (A_{n})\) с элементами
	\[
		A_{n} = \sum_{k = 1}^{n} \E{h_{k} \given \mathcal{G}_{k - 1}}
	\]
	будет \((\mathcal{G}_{n})\)-предсказуемой (то есть \(A_{n}\) являются
	\(\mathcal{G}_{n - 1}\)-измеримыми). Однако \(M = (M_{n})\), задаваемая по 
	правилу
	\[
		M_{n} = \sum_{k = 1}^{n} (h_{k} - \E{h_{k} \given \mathcal{G}_{k - 1}}).
	\]
	не является мартингалом относительно фильтрации \((\mathcal{G}_{n})\), так 
	как \(h_{k}\) являются измеримыми относительно сигма-алгебры \(\F_{k}\), 
	что не означает \(\mathcal{G}_{k}\)-измеримость.
\end{remark}

У этого разложения есть хорошее свойство: оно единственно. Действительно, пусть 
\(H_{n} = A'_{n} + M'_{n}\)~--- другое разложение с \((\F_{n})\)-предсказуемой 
последовательностью \(A' = (A'_{n}), A'_{0} = 0\) и мартингалом \(M' = (M'_{n}, 
\F_{n})\). Заметим, что
\[
	A'_{n + 1} - A'_{n} = H_{n + 1} - H_{n} - (M'_{n + 1} - M'_{n}) = (A_{n + 
	1} - A_{n}) + (M_{n + 1} - M_{n}) - (M'_{n + 1} - M'_{n}).
\]

Теперь возьмём от обеих частей условное матожидание \(\E{\cdot \given 
\F_{n}}\). Тогда мы получим, что
\[
	\E{A'_{n + 1} - A'_{n} \given \F_{n}} = \E{A_{n + 1} - A_{n} \given \F_{n}}.
\]
Так как \(A_{n + 1}\) и \(A_{n}\) являются \(\F_{n}\)-измеримыми, то \(A'_{n + 
1} - A'_{n} = A_{n + 1} - A_{n}\). Пользуясь тем, что \(A_{0} = A'_{0} = 0\), 
получаем желаемое.

Стоит заметить, что если в рассматриваемой модели \(\E{h_{k} \given \F_{k - 1}} 
= 0\) для всех \(k \geq 1\), то сама последовательность \(H = (H_{n})\) будет 
являться мартингалом.

Разложение Дуба не такое тривиальное, каким оно может показаться на первый 
взгляд.
\begin{example}
	Рассмотрим последовательность iid случайных величин \(\{\xi_{n}\}_{n \in 
	\N}\) таких, что
	\[
		\Pr{\xi_{n} = 1} = \Pr{\xi_{n}= -1} = \frac{1}{2}.
	\]
	
	Далее, пусть \(X_{n} = \xi_{1} + \ldots + \xi_{n}\). Другими словами, пусть 
	есть простейшее случайное блуждание. Разложение Дуба для \(H_{n} = 
	|X_{n}|\), \(n \geq 0\), \(|X_{0}| = 0\) будет устроено следующим образом:
	\[
		h_{n} = \Delta H_{n} = |X_{n}| - |X_{n - 1}| = |X_{n - 1} + \xi_{n}| - 
		|X_{n - 1}|.
	\]
	Далее, пользуясь свойствами условного математического ожидания и 
	независимостью \(\xi_{n}\) и \(X_{n - 1}\), получаем, что
	\begin{align*}
		\Delta M_{n} &= h_{n} - \E{h_{n} \given \F_{n - 1}} = |X_{n - 1} + 
		\xi_{n}| - |X_{n - 1}| - \E{|X_{n - 1} + \xi_{n}| - |X_{n - 1}| \given 
		\F_{n - 1}} = \\
		&= |X_{n - 1} + \xi_{n}| - \E{|X_{n - 1} + \xi_{n}| \given X_{n - 1}} = 
		(\sgn{X_{n - 1}})\xi_{n}.
	\end{align*}
	
	Отсюда получаем, что мартингал в разложении Дуба имеет вид
	\[
		M_{n} = \sum_{k = 1}^{n} (\sgn{X_{k - 1}})\xi_{k} = \sum_{k = 1}^{n} 
		(\sgn{X_{k - 1}})\Delta X_{k}.
	\]
	
	Далее,
	\[
		\E{h_{n} \given \F_{n - 1}} = \E{|X_{n - 1} + \xi_{n}| - |X_{n - 1}| 
		\given \F_{n - 1}} = \E{|X_{n - 1} + \xi_{n}| \given \F_{n - 1}} - 
		|X_{n - 1}|.
	\]
	
	Несложно заметить, что если \(X_{n - 1} \neq 0\), то это условное 
	матожидание обращается в ноль. Если же \(X_{n -1} = 0\), то оно равно 
	единице. Тогда 
	\[
		\sum_{k = 1}^{n} \E{h_{k} \given \F_{k - 1}} = \#\{1 \leq k \leq n: 
		X_{k - 1} = 0\}.
	\]
	
	Пусть \(L_{n}(0) = \#\{0 \leq k \leq n - 1: X_{k} = 0\}\)~--- число нулей 
	последовательности \((X_{k})_{0 \leq k \leq n - 1}\). Тогда по разложению 
	Дуба
	\[
		|X_{n}| = \sum_{k = 1}^{n} (\sgn{X_{k - 1}})\Delta X_{k} + L_{n}(0).
	\]
	
	Теперь воспользуемся тем, что у мартингала матожидание постоянно и равно 
	нулю. Тогда
	\[
		\E{L_{n}(0)} = \E{|X_{n}|}.
	\]
	
	Согласно ЦПТ \(X_{n}/\sqrt{n} \sim \mathcal{N}(0, 1)\). Следовательно,
	\[
		\E{|X_{n}|} \sim \sqrt{\frac{2n}{\pi}} \implies \E{L_{n}(0)} \sim 
		\sqrt{\frac{2n}{\pi}}.
	\]
	
	Полученная формула~--- это известный результат о среднем числе нулей в 
	симметричном случайном блуждании Бернулли.
\end{example}
\section{Гауссовские и условно-гауссовские модели}
На абсолютно \emph{эффективных рынках} наилучшим прогнозом будущего уровня цены 
финансового актива является текущая цена этого актива. Поэтому понятие 
мартингала стало одним из основных при исследовании динамики эволюции цен как 
стохастических последовательностей или процессов с определёнными свойствами их 
распределений. Однако при проведении конкретных расчетов одного лишь знания 
``мартингальности распределений'' слишком мало~--- нужна более ``тонкая'' 
структура этих распределений, что приводит к необходимости детального 
рассмотрения самых разнообразных вероятностно-статистических моделей с целью 
выявления тех из них, свойства распределений которых лучше всего согласуются со 
свойствами эмпирических распределений, построенных по статистическим данным.

Предположение гауссовости распределений величин \(h_{1}, \ldots, h_{n}\), 
конечно, выглядит привлекательным и с точки зрения теоретического анализа, и с 
точки зрения ``статистики нормального распределения''. Но это предположение не 
всегда соответсвтует истинной картине поведения цен. Но какую альтернативу 
можно привести? Для этого вспомним разложение Дуба. Как известно, оно 
определяется с привлечением условных матожиданий вида \(\E{h_{n} \given \F_{n - 
1}}\). Тогда было бы разумно предположить, что не безусловные, а 
\emph{условные} распределения являются гауссовскими:
\[
	\Law(h_{n} \mid \F_{n - 1}) = \mathcal{N}(\mu_{n}, \sigma_{n}^{2})
\]
с некоторыми \(\F_{n - 1}\)-измеримыми величинами \(\mu_{n} = \mu_{n}(\omega)\) 
и \(\sigma_{n}^{2} = \sigma_{n}^{2}(\omega)\).

Оказывается, что \(\E{h_{n} \given \F_{n - 1}} = \mu_{n}\) и \(\D{h_{n} \given 
\F_{n - 1}} = \sigma_{n}^{2}\) (это следует из регулярности условного 
распределения~--- за доказательством обращайтесь к первому тому Ширяева). Тем 
самым видем смысл этих параметров~--- условное среднее и условная дисперсия 
распределения \(\Law(h_{n} \mid \F_{n - 1})\).

Само же распределение \(\Law(h_{n})\) является \emph{взвесью} условных 
гауссовских распределений \(\Law(h_{n} \mid \F_{n - 1})\) с усреднением по 
распределению величин \(\mu_{n}\) и \(\sigma_{n}^{2}\). % Почему?

Обычно наряду с \(h = (h_{n})\) вводится ``стандартная'' условно-гауссовская 
последовательность \(\epsilon = (\epsilon_{n})_{n \geq 1}\) 
\(\F_{n}\)-измеримых случайных величин таких, что
\[
	\Law(\epsilon_{n} \mid \F_{n - 1}) = \mathcal{N}(0, 1), \text{ где } \F_{0} 
	= \{\emptyset, \Omega\}.
\]

Оказывается, что это будет последовательность \emph{независимых} случайных 
величин с стандартным нормальным распределением, так как
\[
	\Law(\epsilon_{n} \mid \epsilon_{1}, \ldots, \epsilon_{n - 1}) = 
	\mathcal{N}(0, 1).
\]

Если \(\omega_{n}^{2} \neq 0\) поточечно для всех \(n \geq 1\), то величины 
\(\epsilon_{n}\), задаваемые по правилу \(\epsilon_{n} \equiv (h_{n} - 
\mu_{n})/\omega_{n}\), будут задавать стандартную гауссовскую 
последовательность. Тогда можно считать, что рассматриваемые 
условно-гауссовские последовательности представимы в виде
\[
	h_{n} = \mu_{n} + \sigma_{n}\epsilon_{n},
\]
где \(\epsilon = (\epsilon_{n})\)~--- последовательность независимых 
\(\F_{n}\)-измеримых случайных величин с распределением \(\mathcal{N}(0, 1)\).

Понятно, что более подробное изучение свойств последовательности \(h = 
(h_{n})\) зависит от структуры \(\mu_{n}\) и \(\sigma_{n}^{2}\). Именно это и 
делается в представляемых ниже моделях.

В теории временных рядов есть целый арсенал разнообразных \emph{линейных} 
моделей, среди которых в первую очередь нужно назвать следующие:
\begin{itemize}
	\item Модель \emph{скользящего среднего} порядка \(q\) \(\mathrm{MA}(q)\),
	\item Модель \emph{авторегрессии} порядка \(p\) \(\mathrm{AR}(p)\),
	\item  Модель \emph{авторегрессии и скользящего среднего} порядка \((p, 
	q)\) \(\mathrm{ARMA}(p, q)\).
\end{itemize}
Эти модели широко исследуются в теории временных рядов, особенно в 
предположении \emph{стационарности}. Вообще, для чего вводятся линейные модели? 
Они весьма просты, но при этом ими можно неплохо приближать весьма широкий 
класс стационарных последовательностей.

Вот только не все временные ``эконометрические'' ряды являются стационарными. 
Анализ показывает, что часто в данных вырисовываются три составляющие:
\begin{itemize}
	\item Медленно меняющийся (например, ``инфляционный'') тренд \(x\),
	\item Периодические или же апериодические циклы \(y\),
	\item Нерегулярная, флуктуирующая (``стохастическая'' или ``хаотическая'') 
	компонента \(z\).
\end{itemize}
В наблюдаемые данные \(h\) они могут входить весьма разнообразными способами. 
Образно это можно представить так: \(h = x * y * z\), где вместо \(*\) могут 
выступать сложение \(+\), умножение \(\times\) и так далее.

Ниже мы рассмотрим некоторые \emph{линейные} (а затем и нелинейные) модели, 
преследуя цель дать представление об их структуре, особенностях, свойствах, 
применяемых в анализе данных. 

Не стоит забывать, что конечной целью анализа статистических данных является 
\emph{прогнозирование} дальнейшнего поведения. Качество этого прогнозирования 
зависит от удачного выбора модели, точности оценивания определяющих её 
параметров и качества экстраполяционного оценивания.

Во всех рассматриваемых далее моделях будем считать, что задана некоторая 
``базисная'' последовательность \(\epsilon = (\epsilon_{n})\), которую в теории 
временных рядов обычно считают \emph{белым шумом} и идентифицируют с источником 
случайности, определяющим стохастический характер исследуемых 
вероятностно-статистических объектов. При этом (в ``\(L^{2}\)-теории'') 
говорят, что последовательность является \emph{белым шумом в широком смысле}. 
\begin{definition}
	Последовательность \(\epsilon = (\epsilon_{n})\) называется белым шумом в 
	широком смысле, если 
	\[
		\forall m, n \in \Z, m \neq n : \E{\epsilon_{n}} = 0, 
		\E{\epsilon_{n}^{2}} 
		< \infty, \E{\epsilon_{m}\epsilon_{n}} = 0.
	\]
	
	Другими словами, белый шум в широком смысле~--- это квадратично 
	интегрируемая последовательность некоррелированных случайных величин с 
	нулевыми средними.
\end{definition}
Ещё вводят белый шум \emph{в узком смысле}, который обычно называют просто 
\emph{белым (гауссовским) шумом}. 
\begin{definition}
	Белый шум~--- это гауссовская последовательность, являющаяся белым шумом в 
	широком смысле.
\end{definition}
Другими словами, это последовательность независимых случайных величин с 
нормальными распределениями \(\mathcal{N}(0, \sigma_{n}^{2})\). Далее будем 
считать, что \(\sigma_{n}^{2} \equiv 1\). В таком случае обычно говорят, что 
\(\epsilon\) есть стандартная гауссовская последовательность.

\subsection{Модель скользящего среднего \(\mathrm{MA}(q)\)}

В модели скользящего среднего порядка \(q\), описывающей эволюцию 
последовательности \(h = (h_{n})\), предполагается следующие способ 
формирования значений \(h_{n}\) по белому шуму в широком смысле \(\epsilon = 
(\epsilon_{n})\):
\[
	h_{n} = (\mu + b_{1}\epsilon_{n - 1} + \ldots + b_{q}\epsilon_{n - q}) + 
	b_{0}\epsilon_{n},
\]
где параметр \(q\) определяет порядок зависомости от ``прошлого'', а 
\(\epsilon_{n}\) играет роль величин, ``обновляющих'' информацию, содержащуюся 
в \(\F_{n - 1} = \sigma(\epsilon_{n - 1}, \epsilon_{n - 2}, \ldots)\).

Далее, для компактности вводят \emph{лаговый оператор} \(L\), действующий по 
правилу \(Lx_{n} = x_{n - 1}\). Так как \(L(Lx_{n}) = x_{n - 2}\), то разумно 
ввести обозначение
\[
	L^{2}x_{n} \equiv L(Lx_{n}) = x_{n - 2},
\]
и, в общем случае, \(L^{k}x_{n} = x_{n - k}\).

Отметим следующие свойства лагового оператора:
\begin{align*}
	L(cx_{n}) &= cLx_{n}, \\
	L(x_{n} + y_{n}) &= Lx_{n} + Ly_{n}, \\
	(c_{1}L + c_{2}L^{2})x_{n} &= c_{1}Lx_{n} + c_{2}L^{2}x_{n} = c_{1}x_{n - 
	1} + c_{2}x_{n - 2}, \\
	(1 - \lambda_{1}L)(1 - \lambda_{2}L)x_{n} &= x_{n} - (\lambda_{1} + 
	\lambda_{2})x_{n - 1} + (\lambda_{1}\lambda_{2})x_{n - 2}.
\end{align*}

Пользуясь этими свойствами, модели \(\mathrm{MA}(q)\) можно придать следующую 
форму: \(h_{n} = \mu + \beta(L)\epsilon_{n}\), где \(\beta(L) = b_{0} + b_{1}L 
+ \ldots + b_{q}L^{q}\).

Теперь положим \(q = 1\). В таком случае
\[
	h_{n} = \mu + b_{0}\epsilon_{n} + b_{1}\epsilon_{n - 1}.
\]

Несложно проверить, что
\begin{align*}
	\E{h_{n}} = \mu,& \D{h_{n}} = b_{0}^{2} + b_{1}^{2} \\
	\cov(h_{n + 1}, h_{n}) = b_{0}b_{1}, & \cov(h_{n + k}, h_{n}) = 0, k > 1.
\end{align*}

Это означает, что \(h = (h_{n})\)~--- это последовательность с коррелированными 
соседними значениями (\(h_{n}\) и \(h_{n + 1}\)), причём корреляция значений 
\(h_{n + k}\) и \(h_{n}\) при \(k > 1\) равна нулю.

Из соотношений сверху следует, что у элементов последовательности \(h = 
(h_{n})\) матожидание, дисперсия и ковариация не зависят от \(n\) (впрочем, это 
определяется предположением стандартности последовательности \(\epsilon\) и 
тем, что \(b_{k}\) не зависят от \(n\)). Отсюда следует, что последовательность 
\(h = (h_{n})\) является стационарной в широком смысле (просто по определению). 
Если же добавить то, что \(\epsilon\) является гауссовской, то и \(h\) тоже 
будет гауссовской. Это означает, что все её параметры полностью задаются 
средним, дисперсией и ковариацией. Но тогда \(h\) будет стационарной и в узком 
смысле, так как для произвольных \(n\), \(k\) и \(i_{1}, \ldots, i_{n}\)
\[
	\Law(h_{i_{1}}, \ldots, h_{i_{n}}) = \Law(h_{i_{1} + k}, \ldots, h_{i_{n} + 
	k})
\]

Теперь покажем одно интересное свойство модели \(\mathrm{MA}(1)\). Пусть 
\((h_{1}, \ldots, h_{n})\)~--- некоторая реализация, полученная в результате 
наблюдений величин \(h_{k}\) в моменты времени \(k = 1, \ldots, n\). Далее, 
пусть \(\overline{h}_{n} = (\sum_{k = 1}^{n} h_{k})/n\)~--- это временное 
среднее. Со статистической точки зрения обращение к ``статистике'' 
\(\overline{h}_{n}\) представляет тот интерес, что \(\overline{h}_{n}\) 
является естественным кандидатом для оценивания среднего \(\mu\). 

Оказывается, что для стационарной в широком смысле последовательности \(h_{n}\) 
есть хороший критерий эргодичности, который похож на 
\hyperref[slutsky-criterion]{условие Слуцкого} (на самом деле это оно и есть):
\begin{theorem}
	Пусть \(h = (h_{n})\)~--- стационарная в широком смысле последовательность, 
	а \(R(k) = \cov(h_{n + k}, h_{n})\). Тогда
	\[
		\lim\limits_{n \to \infty} \E{(\overline{h}_{n} - \mu)^{2}} = 0 \iff 
		\lim\limits_{n \to \infty} \frac{1}{n}\sum_{k = 1}^{n} R(k) = 0.
	\]
\end{theorem}
\begin{proof}
	Без ограничения общности скажем, что \(\mu = \E{h_{n}} = 0\).
	
	Пусть \(\E{(\overline{h}_{n} - \mu)^{2}} \to 0\). Тогда по неравенству 
	Коши-Буняковского:
	\[
		\left|\frac{1}{n}\sum_{k = 1}^{n} R(k)\right|^{2} = 
		\left|\E{\frac{h_{0}}{n}\sum_{k = 1}^{n}h_{k}}\right|^{2} \leq 
		\E{h_{0}^{2}}\E{\left|\frac{1}{n}\sum_{k = 1}^{n}h_{k}\right|^{2}} 
		\xrightarrow[n \to \infty]{} 0.
	\]
	
	Теперь докажем в другую сторону. Заметим, что
	\[
		\E{\left|\frac{1}{n}\sum_{k = 1}^{n}h_{k}\right|^{2}} = 
		\frac{1}{n^{2}}\E{\sum_{k = 1}^{n} h_{k}^{2} + 2\sum_{i = 1}^{n}\sum_{j 
		= 1}^{i -1} h_{i}h_{j}} = \frac{2}{n^{2}}\sum_{i = 1}^{n}\sum_{j = 
		0}^{i - 1} R(j) - \frac{1}{n}R(0)
	\]
	
	Зафиксируем произвольное \(\delta > 0\) и найдём \(n(\delta)\) такое, что 
	для любого \(l \geq n(\delta)\) выполнено, что
	\[
		\left|\frac{1}{l}\sum_{k = 0}^{l}h_{k}\right| \leq \delta.
	\]
	
	Тогда для \(n \geq n(\delta)\) имеем
	\begin{align*}
		\left|\frac{1}{n^{2}}\sum_{i = 1}^{n}\sum_{j = 0}^{i - 1} R(j)\right| 
		&= \left|\frac{1}{n^{2}}\sum_{i = 1}^{n(\delta)}\sum_{j = 0}^{i - 1} 
		R(j) + \frac{1}{n^{2}}\sum_{i = n(\delta) + 1}^{n}\sum_{j = 0}^{i - 1} 
		R(j)\right| \leq \\
		&\leq \left|\frac{1}{n^{2}}\sum_{i = 1}^{n(\delta)}\sum_{j = 0}^{i - 1} 
		R(j)\right| + \left|\frac{1}{n^{2}}\sum_{i = n(\delta) + 1}^{n}i\cdot 
		\frac{1}{i}\sum_{j = 0}^{i - 1} R(j)\right| \leq \\
		&\leq \frac{1}{n^{2}}\left|\sum_{i = 1}^{n(\delta)}\sum_{j = 0}^{i - 1} 
		R(j)\right| + \delta
	\end{align*}
	
	Теперь вспомним, что \(R(0) = const < \infty\). Тогда
	\[
		\varlimsup\limits_{n \to \infty} \E{\left|\frac{1}{n}\sum_{k = 
		1}^{n}h_{k}\right|^{2}} \leq \delta.
	\]
	
	Устремляя \(\delta\) к нулю, получаем желаемое.
\end{proof}

Тем самым мы получили весьма полезное свойство: \(\mathrm{MA}(1)\) эргодична в 
среднеквадратичном, то есть среднее по времени стремится в среднеквадратичном 
смысле к среднему по ансамблю \(\mu\).

Теперь вспомним про корреляционную функцию. Для \(\mathrm{MA}(1)\) она будет 
иметь вид
\[
	r(k) = \frac{\cov(h_{n + k}, h_{n})}{\sqrt{\D{h_{n + k}}\D{h_{n}}}} = 
	\frac{R(k)}{R(0)} = 
	\begin{cases}
	1, & k = 0 \\
	\frac{b_{0}b_{1}}{b_{0}^{2} + b_{1}^{2}}, & k = 1 \\
	0, & k > 1
	\end{cases}
\]

Вернёмся к общему случаю \(\mathrm{MA}(q)\). В качестве упражнения оставим 
вывод следующих тождеств:
\begin{gather*}
	\E{h_{n}} = \mu, \quad \D{h_{n}} = \sum_{k = 0}^{q} b_{k}^{2}, \\
	R(k) = \begin{cases}
	\sum_{i = 0}^{q - k} b_{i}b_{k + i},& k \leq q \\
	0,& k > q
	\end{cases}
\end{gather*}

Пользуясь этими формулами, моделью \(\mathrm{MA}(q)\) можно пытаться 
моделировать поведение последовательностей \(h = (h_{n})\), у которых 
корреляция величин \(h_{n}\) и \(h_{n + k}\), где \(k > q\), нулевая. Но как 
это делать? Общий принцип подгонки следующий:
\begin{itemize}
	\item Для начала, по выборке \((h_{1}, \ldots, h_{n})\) строятся некоторые 
	эмпирические характеристики: например,
	\begin{gather}
		\overline{h}_{n} = \frac{1}{n}\sum_{k = 1}^{n} h_{k}\text{~--- 
		выборочное среднее} \\
		\hat{\sigma}_{n}^{2} = \frac{1}{n - 1}\sum_{k = 1}^{n} (h_{k} - 
		\overline{h}_{n})\text{~--- выборочная дисперсия} \\
		r_{n}(k) = \frac{1}{n\hat{\sigma}_{n}^{2}}\sum_{i = k + 1}^{n}(h_{i} - 
		\overline{h}_{n})(h_{i - k} - \overline{h}_{n})\text{~--- выборочное 
		среднее}
	\end{gather}
	\item Далее, используя выражения для теоретических характеристик, 
	производится варьирование параметров с целью подгонки теоретических 
	значений под эмпирические.
	\item В конце провдодится \emph{оценка кажества} подгонки, основываясь на 
	знании эмпирических характеристик и их отклонений от теоретических 
	распределений.
\end{itemize}

Ранее мы смотрели на модели с конечным \(q\). Было бы разумно ввести обобщение, 
которое позволяет и бесконечное \(q\): \(\mathrm{MA}(\infty)\). Устроено оно 
так:
\[
	h_{n} = \mu + \sum_{j = 0}^{\infty} b_{j}\epsilon_{n - j}.
\]

Но, конечно, должны быть какие-то условия (как минимум, на сходимость). Если 
потребовать сходимость ряда \(\sum b_{j}^{2}\), то ряд в формуле для \(h_{n}\) 
будет сходиться в среднеквадратичном смысле.

Для этой модели
\[
	\E{h_{n}} = \mu, \quad \D{h_{n}} = \sum_{k = 0}^{\infty} b_{k}^{2}, \quad 
	R(k) = \sum_{i = 0}^{\infty} b_{k + i}b_{i}.
\]

В теории стационарных случайных процессов принято говорить, что \(h_{n}\) есть 
``результат реакции физически осуществимого фильтра с импульсной переходной 
функцией \(b = (b_{n})\), когда на вход подается последовательность \(\epsilon 
= (\epsilon_{n})\)''.

Оказывается, что в определённом смысле ``регулярная'' стационарная (в широком 
смысле) последовательность может быть представлена моделью 
\(\mathrm{MA}(\infty)\). Результат, связанный с этим фактом, называется 
\emph{разложением Вольда} стационарных последовательностей на <<сингулярную>> и 
<<регулярную>> части. За подробностями обращайтесь ко второму тому Ширяева.

\subsection{Авторегрессионная модель \(\mathrm{AR}(p)\)}
Поехали дальше. Авторегрессионая модель порядка \(p\) определяется следующим 
образом:
\[
	h_{n} = \mu_{n} + \sigma\epsilon_{n}, \quad \mu_{n} = a_{0} + a_{1}h_{n - 
	1} + \ldots + a_{p}h_{n - p}.
\]

Можно сказать, что модель \(\mathrm{AR}(p)\) подчиняется \emph{разностному 
уравнению} порядка \(p\):
\[
	h_{n} = a_{0} + a_{1}h_{n - 1} + \ldots + a_{p}h_{n - p} + 
	\sigma\epsilon_{n}.
\]

Если совпользоваться лаговым оператором, то это уравнение можно записать в виде:
\[
	(1 - a_{1}L - \ldots - a_{p}L^{p})h_{n} = a_{0} + \sigma\epsilon_{n}.
\]

Тогда, введя пару обознаений, уравнение приобретает вид
\[
	\alpha(L)h_{n} = \omega_{n}, \quad \omega_{n} = a_{0} + \sigma\epsilon_{n}, 
	\quad \alpha(L) = 1 - a_{1}L - \ldots - a_{p}L^{p}
\]

В отличие от модели со скользящий средним, в этой модели нужно задавать 
\emph{начальные} условия \((h_{1 - p}, h_{2 - p}, \ldots, h_{0})\). Обычно их 
обнуляют, хотя можно считать их случайными и не зависящими от \(\epsilon\). В 
эргодических случаях асимптотическое поведение последовательности при \(n \to 
\infty\) не зависит от начальных условий, и в этом смысле их конкретизация не 
столь существенна.

Рассмотрим модель \(\mathrm{AR}(1)\) поподробнее. Выглядит она следующим 
образом:
\[
	h_{n} = a_{0} + a_{1}h_{n - 1} + \sigma\epsilon_{n}.
\]

Этоа модель выделяется из общего класса \(\mathrm{AR}(p)\) моделей тем, что из 
<<прошлых>> величин \(h_{n - 1}, \ldots, h_{n - p}\), \(h_{n}\) зависит только 
от ближайшего (по времени) значения \(h_{n - 1}\). Если добавить к этой модели 
независимость последовательности \(\epsilon = (\epsilon_{n})\), то получится 
\emph{конструктивный} пример марковской цепи.
