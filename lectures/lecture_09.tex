\section{Некоторые широко используемые статистики в <<$\theta$-проблеме>> скорейшего 
обнаружения}\label{sec-4}

\textbf{1.} Предположим сейчас, что
\[
x_1,x_2,\ldots,x_{\theta-1},x_{\theta},x_{\theta+1},\ldots
\]
-- это наблюдения над \textit{независимыми} случайными величинами
\[
\xi_1,\xi_2,\ldots,\xi_{\theta-1},\xi_{\theta},\xi_{\theta+1},\ldots,
\]
такими, что в~моменты времени $k=1,2,\ldots,\theta-1$ они имеют
распределение с~плотностью~$f_\infty(x)$, а в~моменты
$k=\theta,\theta+1,\ldots$~-- распределение с~плотностью~$f_0(x)$.

Параметр $\theta$ считается заранее не известным и принимающим
значения $0,1,\ldots,\infty$. Значения $x_{\theta-1}$ при
$\theta=0$ и $\theta=1$ будем считать равными нулю. Заметим, что
искусственное введение этих моментов будет оправдано
рассмотрениями переходов от дискретных схем к~соответствующим
непрерывным (во времени) схемам.

Таким образом, случаи $\theta=0$ и $\theta=1$ отвечают наблюдениям
с~плотностью~$f_{0}(x)$. Если же $\theta=\infty$, то все
наблюдения имеют плотность распределения
вероятностей~$f_{\infty}(x)$.

Для $n\ge1$, $\theta>1$ положим
\begin{equation}
\label{71}%
p_{\theta}(x_1,\ldots,x_n)
    =f_{\infty}(x_1)\cdots f_{\infty}(x_{\theta-1})
    f_0(x_{\theta})\cdots f_0(x_n).
\end{equation}
При $\theta=0$ и $\theta=1$
\begin{equation}
\label{72}%
p_{\theta}(x_1,\ldots,x_n)
    =f_{0}(x_1)\cdots f_{0}(x_n)
\end{equation}
и при $\theta=\infty$ в~\eqref{71}
\begin{equation}
\label{73}%
p_{\infty}(x_1,\ldots,x_n)
    =f_{\infty}(x_1)\cdots f_{\infty}(x_n).
\end{equation}

Введем следующую статистику:
\begin{align}
\label{74}%
\gamma_n
 &
    =\sup_{\theta\ge0}\frac{p_{\theta}(x_1,\ldots,x_n)}{p_{\infty}(x_1,\ldots,x_n)}\notag\\
 &
    =\max
    \biggl\{
    \frac{p_{0}(x_1,\ldots,x_n)}{p_{\infty}(x_1,\ldots,x_n)},
    \frac{p_{0}(x_1,\ldots,x_n)}{p_{\infty}(x_1,\ldots,x_n)},
    \frac{p_{0}(x_2,\ldots,x_n)}{p_{\infty}(x_2,\ldots,x_n)},
    \ldots,\notag\\
 &\hphantom{\max\biggl\{}
    \ldots
    \frac{p_{0}(x_n)}{p_{\infty}(x_n)},
    \frac{p_{\infty}(x_1,\ldots,x_n)}{p_{\infty}(x_1,\ldots,x_n)}
    \biggr\}
    \notag\\
 &
    =\max
    \biggl\{
    1,
    \max_{1\le\theta\le n}\prod_{k=\theta}^n\frac{f_0(x_k)}{f_{\infty}(x_k)}
    \biggr\}
\end{align}
и положим
\[
T_n
    =\log\gamma_n
    =\max\biggl\{0,\max_{1\le\theta\le n}\sum_{k=\theta}^n\log 
    \frac{f_0(x_k)}{f_{\infty}(x_k)}\biggr\}
    =\max\biggl\{0,\max_{1\le\theta\le n}\sum_{k=\theta}^n\zeta_k\biggl\},
\]
где
\[
\zeta_k=\log\frac{f_0(x_k)}{f_{\infty}(x_k)}.
\]
Если положить $Z_0=0$ и
\[
    Z_n=\sum_{k=1}^n\zeta_k,
    \qquad
    n\ge1,
\]
то
\begin{align*}
T_n
 &
    =\max\Bigl\{0,\max_{1\le\theta\le n}\bigl[Z_n-Z_{\theta-1}\bigr]\Bigr\}\\
 &
    =\max\Bigl\{0,Z_n-\min_{0\le\theta\le n-1}Z_{\theta}\bigr\}\\
 &
    =Z_n-\min_{0\le\theta\le n}Z_{\theta}.
\end{align*}

Статистики
\begin{equation}
\label{75}%
    T_n=Z_n-\min_{0\le\theta\le n}Z_{\theta}
\end{equation}
и
\[
\gamma_n=e^{T_n}
\]
играют в~задачах скорейшего обнаружения исключительно важную роль.
Напомним, что в~задаче различения двух гипотез ($\mathrm{H}_0$ и
$\mathrm{H}_{\infty}$) ключевую роль играет отношение
правдоподобия
\begin{equation}
\label{76}%
L_n
    =\frac{f_0(x_1)\cdots f_0(x_n)}{f_{\infty}(x_1)\cdots f_{\infty}(x_n)}
    \quad
    (L_0=1).
\end{equation}
Из~\eqref{74} видим, что с~помощью отношения правдоподобия
статистика $\gamma_n$ может быть представлена в~виде
\begin{equation}
\label{77}%
    \gamma_n=\max_{0\le\theta\le n}\frac{L_n}{L_{\theta}}.
\end{equation}
Введенные статистики $T_n$, $n\ge1$, обладают важным рекуррентным
свойством:
\begin{equation}
\label{78}%
    T_n=\max\left[0,T_{n-1}+\zeta_n\right],
\end{equation}
из которого, в~частности, видно, что в~рассматриваемом случае
независимых наблюдений статистики $T_n$, $n\ge1$, образуют
марковскую цепь (по каждой из мер $\Pr_0$ и $\Pr_{\infty}$
в~пространстве последовательностей $x=(x_1,x_2,\ldots)$).
Аналогичное верно и для статистик~$L_n$,~$n\ge1$:
\begin{equation}
\label{79}%
    L_n=L_{n-1}e^{\zeta_n}.
\end{equation}
Статистики $\gamma=(\gamma_n)_{n\ge1}$ принято называть
\textit{обобщенными} отношениями правдоподобия. В~определенных
постановках задач скорейшего обнаружения на этих статистиках
основаны оптимальные или асимптотически оптимальные решения.

Статистики $T=(T_n)_{n\ge1}$ называют статистиками CUSUM
(Cumulative Sums). Это название объясняется тем, что там, где
$T_k>0$, $k\le n$, статистика $T_n$ может быть представлена в~виде
$T_n=\sum_{k=1}^n\zeta_k$, т.\,е. является кумулятивной суммой,
а~если $T_{n-1}+\zeta_n<0$, то из~\eqref{78} видим, что $T_n$
полагается равным нулю, а не значению $T_{n-1}+\zeta_n$ (как
в~случае $T_{n-1}+\zeta_n>0$).

\textbf{2.} Обратимся к~еще одной важной статистике
$\psi=(\psi_n)$, $n\ge1$, определяемой следующим образом:
\begin{equation}
\label{80}%
    \psi_n=\sum_{\theta=1}^n\prod_{k=\theta}^n\frac{f_0(x_k)}{f_{\infty}(x_k)}.
\end{equation}
Поскольку
\[
\prod_{k=\infty}^n\frac{f_0(x_k)}{f_{\infty}(x_k)}
    =\frac{L_n}{L_{\theta-1}}
    =e^{Z_n-Z_{\theta-1}},
\]
то
\begin{equation}
\label{81}%
    \psi_n=\sum_{\theta=1}^n\frac{L_n}{L_{\theta-1}},
\end{equation}
или
\begin{equation}
\label{82}%
    \psi_{n}=\sum_{\theta=1}^ne^{Z_n-Z_{\theta-1}}.
\end{equation}
Из~\eqref{81} (с~$\psi_0=0$)
\begin{equation}
\label{83}%
    \psi_n=(1+\psi_{n-1})e^{\zeta_n},
    \qquad
    n\ge1.
\end{equation}
(Ср. с~рекуррентными соотношениями~\eqref{78}
для~$(T_n)_{n\ge1}$.)

В~случае нормальных плотностей
\begin{equation}
\label{84}%
    f_0(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-r_0)^2/(2\sigma^2)},
    \qquad
    f_{\infty}(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-r_{\infty})^2/(2\sigma^2)}
\end{equation}
имеем
\begin{equation}
\label{85}%
    \zeta_n=\frac{r_0-r_{\infty}}{\sigma^2}\Bigl(x_n-\frac{r_0+r_{\infty}}{2}\bigr)
\end{equation}
и
\begin{align}
\label{86}%
T_n
 &
    =\max\biggl[0,T_{n-1}+\frac{r_0-r_{\infty}}{\sigma^2}
    \Bigl(x_n-\frac{r_0+r_{\infty}}{2}\Bigr)\biggr],\\
\label{87}%
\psi_n
 &
    =(1+\psi_{n-1})e^{(r_0-r_{\infty})/(\sigma^2)}
    \Bigl(x_n-\frac{r_0+r_{\infty}}{2}\Bigr).
\end{align}
Здесь
\begin{align*}
\E_{\infty}\zeta_n
 &
    =-\frac{(r_0-r_{\infty})^2}{2\sigma^2}\\
\intertext{и} \E_{0}\zeta_n
 &
    =\frac{(r_0-r_{\infty})^2}{2\sigma^2}.
\end{align*}
Отсюда становится ясно, что при $n<\theta$ статистика $T_n$
принимает неотрицательные значения, близкие к~нулю. Если же
$n>\theta$, то $T_n$ начинает (в~среднем) возрастать. На
рис.~\ref{fig3} и~\ref{fig4} изображены графики статистик
$(T_n)_{n\ge0}$ и $(\psi_n)_{n\ge0}$ для реализации
последовательности из 200~независимых нормально распределенных
случайных величин с~дисперсией $\sigma^2=100$, у которых в~момент
$\theta=100$ сменилось значение математического ожидания c~$r_0=1$
на~$r_{\infty}=0$.

Приведенные рисунки наглядно показывают, что статистики $T_n$ и
$\psi_n$ позволяют ``ухватывать'' момент появления ``разладки''
(в~момент~$\theta$). Именно эти наблюдения послужили основанием
применения этих статистик к~реальным данным. Разумеется для этой
цели надо знать или уметь ``хорошо'' оценивать величины
$\zeta_n=f_0(x_n)/f_{\infty}(x_n)$, когда плотности $f_0(x)$ и
$f_{\infty}(x)$ точно не известны.

Среди других методов, в~которых не требуется знание плотностей,
укажем на ``метод экспоненциального сглаживания'', состоящий
в~следующем.

По наблюдениям $(x_1,x_2,\ldots)$ строятся статистики
$(Y_n)_{n\ge0}$ по формулам
\begin{equation}
\label{88}%
    Y_n=(1-\beta)Y_{n-1}+\beta x_n,
\end{equation}
где $Y_0=0$ и $\beta$~-- некоторый подбираемый параметр,
$0<\beta<1$. Заметим, что в~силу~\eqref{88}
\begin{equation}
\label{89}%
    Y_n=\beta\sum_{i=1}^n(1-\beta)^{n-i}x_i.
\end{equation}
Это выражение поясняет, почему методы обнаружения, основанные на
$(Y_n)_{n\ge1}$, называют ``методами экспоненциального
сглаживания''. На рис.~\ref{fig5} изображен график статистики
$(Y_n)_{n\ge0}$ для реализации последовательности из 200
независимых нормально распределенных случайных величин
с~дисперсией $\sigma^2=100$, у которых в~момент $\theta=100$
сменилось значение математического ожидания c~$r_0=1$ на
$r_{\infty}=0$. Параметр сглаживания полагался равным $\beta=0.8$.


В~случае различения двух гипотез мы рассмотрели две постановки
задач. Одна (Нейман--Пирсон, раздел~\ref{sec-2}) носила, так
сказать, ретроспективный характер, поскольку решение выносилось по
заданным наблюдениям $(x_1,x_2,\ldots,x_N)$. Вторая постановка
(А.~Вальда) исходила из того, что наблюдения поступают
последовательно, шаг за шагом, и решение принимается по поведению
текущих данных. В~$\theta$-задаче также принято различать две
постановки~-- ретроспективная и последовательная. В~первом случае
наблюдения $(x_1,x_2,\ldots,x_N)$ заданы \textit{целиком} и
считается, что
\[
\theta\in\{1,2,\ldots,N,\infty\}.
\]

Во втором случае наблюдения $(x_1,x_2,\ldots,x_N)$ поступают
последовательно и, вообще говоря,
\[
\theta\in\{1,2,\ldots,\infty\}.
\]
Введенные статистики $T_n$ и $\psi_n$ оказываются полезными
в~обоих сформулированных случаях.

\textbf{3.} Введенные статистики $L_n$, $T_n$ и $\psi_n$
предназначены для тех ситуаций обнаружения, когда относительно
параметра $\theta$ не делается никаких априорных предположений.

Обратимся к~тому случаю, когда относительно параметра $\theta$
есть некоторая априорная информация, которая, естественно, должна
помогать скорейшему обнаружению истинного значения~$\theta$.

Будем предполагать, что $\theta$ является \textit{случайной
величиной}, принимающей значения $0,1,\ldots$ с~вероятностями
\begin{equation}
\label{90}%
  \begin{aligned}
    &  \Pr(\theta=0)=\pi,\\
    &  \Pr(\theta=n\mid\theta>0)=pq^{n-1},\quad n=1,2,\ldots,
  \end{aligned}
\end{equation}
где $\pi\in[0,1]$, $q=1-p$.

Как и раньше, будем считать, что в~случае отсутствия разладки
наблюдения имеют плотность $f_{\infty}(x)$, а при ее наличии~--
плотность~$f_0(x)$.

При каждом фиксированном значении $\theta$ из соображений
предполагаемой независимости наблюдений плотность
\begin{equation}
\label{91}%
    p_{\theta}(x_1,\ldots,x_n)=\left\{%
\begin{array}{ll}
    f_0(x_1)\cdots f_0(x_n), & \hbox{$\theta=0,1$,} \\
    f_{\infty}(x_1)\cdots f_{\infty}(x_n), & \hbox{$\theta>n$,} \\
    f_{\infty}(x_1)\cdots f_{\infty}(x_{\theta-1})
    f_0(x_{\theta})\cdots f_0(x_n), & \hbox{$2\le\theta\le n$.} \\
\end{array}%
\right.
\end{equation}
Пусть $p^{\pi}(x_1,\ldots,x_n)$ обозначает плотность
$(x_1,\ldots,x_n)$ в~предположении, что $\theta$ является
случайной величиной. По формуле полной вероятности
\begin{align}
\label{92}%
p^{\pi}(x_1,\ldots,x_n)
 &
    =[\pi+p(1-\pi)]f_0(x_1)\cdots f_0(x_k)\notag\\
 &\quad
    +(1-\pi)\sum_{k=1}^npq^kf_\infty(x_1)\cdots f_\infty(x_k)
    f_0(x_{k+1})\cdots f_0(x_n)\notag\\
 &\quad
    +(1-\pi)q^nf_{\infty}(x_1)\cdots f_{\infty}(x_n).
\end{align}
Следующая статистика $(\pi_n)_{n\ge1}$ с~\begin{equation}
\label{93}%
    \pi_n=\Pr(\theta\le n\mid x_1,\ldots,x_n)
\end{equation}
будет играть исключительно важную роль во многих постановках задач
скорейшего обнаружения.

По формуле Байеса находим, что
\begin{equation}
\label{94}%
\pi_{n+1}
    =\frac
    {\pi_n f_0(x_{n+1})+(1-\pi_n)pf_\infty(x_{n+1})}
    {\pi_n f_0(x_{n+1})
     +(1-\pi_n)pf_\infty(x_{n+1})
     +(1-\pi_n)p(1-p)f_\infty(x_{n+1})
    }.
\end{equation}
Положим также
\begin{equation}
\label{95}%
    \varphi_n=\frac{\pi_n}{1-\pi_n}.
\end{equation}
Из~\eqref{94} выводим, что
\begin{equation}
\label{96}%
    \varphi_{n+1}=(p+\varphi_n)\,\frac{f_0(x_n)}{qf_{\infty}(x_n)}.
\end{equation}

\begin{remark}\rm
Формулу~\eqref{96} можно легко получить и непосредственно, не
обращаясь к~формуле~\eqref{94}. Действительно, пусть для простоты
$\pi=0$. Тогда
\begin{align*}
\varphi_n
 &
    =\frac
      {\sum_{k=1}^np_k(x_1,\ldots,x_n)\Pr(\theta=k)}
      {p_{\infty}(x_1,\ldots,x_n)\Pr(\theta>n)}\\
 &
    =\frac
    {\sum_{k=1}^n
       q^{k-1}p
       f_{\infty}(x_1)\cdots f_{\infty}(x_{k-1})
       f_{0}(x_k)\cdots f_{0}(x_n)
    }
    {f_{\infty}(x_1)\cdots f_{\infty}(x_n)q^n}\\
 &
    =\sum_{k=1}^npq^{k-1-n}\,
     \frac{f_{0}(x_k)\cdots f_{0}(x_n)}{f_{\infty}(x_k)\cdots f_{\infty}(x_n)}\\
 &
    =\frac pq\,
    \frac{f_0(x_n)}{f_{\infty}(x_n)}
    +\frac1q
    \sum_{k=1}^{n-1}pq^{k-1-(n-1)}
    \frac{f_{0}(x_k)\cdots f_{0}(x_n)}{f_{\infty}(x_k)\cdots f_{\infty}(x_n)}\\
 &
    =\frac pq\,
    \frac{f_{0}(x_n)}{f_{\infty}(x_n)}
    +\frac1q\,
    \frac{f_0(x_n)}{f_{\infty}(x_n)}
    \sum_{k=1}^{n-1}pq^{k-1-(n-1)}
    \frac{f_{0}(x_k)\cdots f_{0}(x_{n-1})}{f_{\infty}(x_k)\cdots 
    f_{\infty}(x_{n-1})}\\
 &
    =\frac{f_0(x_n)}{qf_{\infty}(x_n)}\,
    [\hspace{1pt}p+\varphi_{n-1}],
\end{align*}
что и есть требуемая формула~\eqref{96}.
\end{remark}

Поскольку
\begin{equation}
\label{97}%
    \pi_n=\frac{\varphi_n}{1+\varphi_n},
\end{equation}
то формула~\eqref{94} легко может быть получена из~\eqref{96}.

Положим
\begin{equation}
\label{98}%
    \zeta_n(p)=\log\frac{f_0(x)}{qf_{\infty}(x)}.
\end{equation}
Тогда из~\eqref{96}
\begin{equation}
\label{99}%
    \varphi_{n+1}=(p+\varphi_n)e^{\zeta_n(p)},
\end{equation}
и если
\begin{equation}
\label{100}%
    \psi_n(p)=\frac{\varphi_n}{p},
\end{equation}
то из~\eqref{99} находим, что
\begin{equation}
\label{101}%
    \psi_{n+1}(p)=(1+\psi_{n+1}(p))\,e^{\zeta_n(p)}.
\end{equation}
Это соотношение весьма близко к~соотношению~\eqref{83}. Более
того, при $p\to\infty$ (т.\,е. при~$q\to1$)
\[
\zeta_n(p)\to\zeta_n
\]
и
\[
\psi_n(p)\to\psi_n.
\]
Это показывает, что введенная выше ad hoc статистика $\psi_n$
возникает весьма естественным образом из статистики
$\psi_n(p)=\varphi_n/p$, где $\varphi_n=\pi_n/1-\pi_n$ при
$p\to0$. Заметим, что если $\pi=0$, то
\[
\E\theta=\frac{1}{p}.
\]
Таким образом, предельный переход по $p\to0$ означает, что
делается предположение, что появление разладки следует ожидать
нескоро.

\section{Об основных постановках задач скорейшего обнаружения для 
броуновского движения}\label{sec-6}

\textbf{1.} В статистике случайных процессов хорошо развита та ее
часть, в~которой исходными предположениями является то, что
рассматриваемые процессы являются \textit{стационарными}. В основе
теории таких процессов лежат ковариационно-спектральные
характеристики. Типичными примерами задач статистики таких
процессов являются, например, задача оценивания корреляционной
функции и спектральной плотности, задача оценивания среднего
значения у наблюдаемого процесса и~т.\,п. (см. \cite[гл.~VI,
\S\,4]{11}).

Рассматриваемые далее задачи скорейшего обнаружения относятся к
сугубо \textit{нестационарным} процессам. Это обстоятельство
приводит и к новым постановкам задач и к необходимости развития
соответствующей теории их решения.

Мы концентрируем наше внимание на тех задачах статистики
нестационарных процессов, которые принято называть задачами
о~``разладке''. При этом значительный материал будет посвящен
моделям, основанным на броуновском движении, что объясняется и
тем, что такие модели представляют практический интерес, и тем,
что для них во многих случаях удается получить прозрачные и точные
результаты. (В этой связи напомним рассмотренную выше
в~разделе~\ref{sec-5} задачу последовательного различения двух
гипотез относительно сноса у наблюдаемого \textit{броуновского
	движения} (со сносом), в~которой были получены \textit{точные}
результаты относительно структуры характеристик оптимальных правил
остановки. Напомним также, что в~других случаях (см.
раздел~\ref{sec-3}) для соответствующих характеристик были
получены \textit{лишь приближенные} результаты.)

\textbf{2.} Будем предполагать, что $B=(B_t)_{t\ge0}$~--
броуновское движение, заданное на некотором вероятностном
пространстве $\left(\Omega,\F,\Pr\right)$.

Предполагается, что наблюдаемый процесс $X=(X_t)_{t\ge0}$ имеет
следующую структуру:
\begin{equation}
\label{188}%
X_t=\mu(t-\theta)^{+}+\sigma B_t
\end{equation}
или, эквивалентно,
\begin{equation}
\label{189}%
dX_t=\left\{%
\begin{array}{ll}
\sigma\,dB_t,         & t<\theta, \\
\mu\,dt+\sigma\,dB_t, & t\ge\theta, \\
\end{array}%
\right.
\end{equation}
где $\sigma>0$, $\mu\neq0$ и $\theta$~-- некоторый момент со
значениями в~$[0,\infty]$.

Относительно природы момента $\theta$ (есть ли это просто
``параметр'' или есть ли это ``случайная величина'') далее будут
делаться вполне конкретные предположения. Сейчас же важно лишь то,
что момент $\theta$~-- это тот момент, когда у наблюдаемого
процесса $X$ меняется снос у броуновского движения (с~$0$
на~$\mu$). Момент $\theta$ называется моментом появления
\textit{разладки}. При этом ``нормальный'' ход процесса~$X$,
характеризующийся тем, что $dX_t=\sigma\,dB_t$, переходит
в~``разлаженный'': $dX_t=\mu\,dt+\sigma\,dB_t$, где снос
$\mu\ne0$. (Несколько более общая на первый взгляд задача
с~$dX_t=\mu_\infty\,dt+\sigma\,dB_t$, $t<\theta$, и
$dX_t=\mu_0\,dt+\sigma\,dB_t$, $t\ge\theta$, сводится к
рассмотренной, если положить $\mu_\infty=0$, $\mu_0=\mu$.)

Случай, когда $\theta=0$, соответствует тому, что с~самого начала
(т.\,е. с~момента~$t=0$) идет ``разлаженный'' процесс. Случай
$\theta=\infty$ соответствует тому, что ``разладка'' не появляется
и вовсе, следовательно, у наблюдаемого процесса~$X$ все время
$dX_t=\sigma\,dB_t$.

Будем обозначать $\Pr_{\theta}=\Law(X\mid\theta)$~-- распределение
вероятностей процесса $X=(X_t)_{t\ge0}$ из~\eqref{188}, когда
``разладка'' происходит в~момент времени~$\theta$. В частности,
$\Pr_{\infty}$ есть распределение вероятностей процесса~$X$, когда
``разладка'' не происходит.

Во всем дальнейшем важно (уже рассмотренное в~предыдущих разделах)
понятие марковского момента, или момента остановки.

Мы говорили, что случайная величина $\tau=\tau(\omega)$ со
значениями в~$[0,\infty]$ является \textit{моментом остановки}
(\textit{марковским моментом}), если при каждом $t\ge0$ событие
\begin{equation}
\label{190}%
\{\omega:\tau(\omega)\le t\}\in\F_t,
\end{equation}
где $\F_t=\F_t^X$ ($\,=\sigma(X_s,\,s\le t)$) есть
$\sigma$-алгебра событий, порождаемых значениями $X_s$, $s\ge t$.

Наглядный смысл этого условия состоит в~том, что для каждого
$t\ge0$ решение вопроса о~том, чтобы прекратить наблюдения или их
продолжать зависит лишь от информации о~процессе $X$, полученной
на интервале времени $[0,t]$ и не зависит от ``будущего''.

Для наших целей такие моменты $\tau$ удобно интерпретировать как
моменты подачи сигнала тревоги о~появлении ``разладки''.

\textbf{3.} Рассмотрим два события:
\[
\{\tau<\theta\}\,\mbox{ и }\,\{\tau\ge\theta\}.
\]
Первое событие~-- это событие, что произошла ложная тревога
($\tau<\theta$). Когда же происходит второе событие, то
естественно интересоваться насколько велико время запаздывания
$\tau-\theta$ при ``правильной'' подаче сигнала о~появлении
``разладки''.

Сформулируем несколько вариантов оптимизационных задач
о~``разладке''.

\underline{Вариант $\mathbf{A}$}. Будем считать, что
$\theta=\theta(\omega)$ есть случайная величина со значениями
в~$[0,\infty]$, не зависящими от $B$ и такая, что
\begin{equation}
\label{191}%
\begin{aligned}
& \Pr(\theta=0)=\pi,\\
& \Pr(\theta>t\mid\theta>0)=e^{-\lambda t},
\end{aligned}
\end{equation}
где $\pi\in[0,1)$ и $\lambda$ являются известными.

Зафиксируем некоторую константу $\alpha\in(0,1]$ и обозначим
\[
\mathcal{M}_{(\alpha)}=\{\tau\colon\Pr(\tau<\theta)\le\alpha\}
\]
-- класс тех моментов остановки $\tau$ (относительно
$(\F_t^X)_{t\ge0}$), для которых вероятность ложной тревоги
$\Pr(\tau<\theta)$ меньше или равна~$\alpha$.

Требуется найти такой момент $\tau^*_{(\alpha)}$ (если он
существует), для которого
\begin{equation}
\label{192}%
\E\bigl(\tau_{(\alpha)}^*-\theta\mid\tau_{(\alpha)}^*\ge\theta\bigr)
=\inf_{\tau\in\mathcal{M}_{(\alpha)}}\E(\tau-\theta\mid\tau\ge\theta).
\end{equation}
Для решения этой \textit{условно-вариационной} задачи полезно
рассмотреть следующую байесовскую постановку задачи
о~``разладке''.

Пусть
\begin{equation}
\label{193}%
\mathbf{A}(c)
=\inf_{\tau\in\mathcal{M}}
\bigl[\Pr(\tau<\theta)+c\E(\tau-\theta)^+\bigr],
\end{equation}
где $\mathcal{M}$~-- класс (конечных) моментов остановки, $c>0$~--
константа и
$\E(\tau-\theta)^+=\E(\tau-\theta\mid\tau\ge\theta)\Pr(\tau\ge\theta)$.

Момент $\tau^*_{\langle c\rangle}$ будем называть оптимальным,
если
\begin{equation}
\label{194}%
\Pr\left(\tau^*_{\langle c\rangle}<\theta\right)+
c\E\left(\tau^*_{\langle c\rangle}-\theta\right)^+=\mathbf{A}(c).
\end{equation}
Подчеркнем, что величина $\mathbf{A}(c)$ (как и многие другие)
зависит, естественно, от параметров $\pi$~и~$\lambda$.

В следующих трех вариантах ($\mathbf{B}$,~$\mathbf{C}$
и~$\mathbf{D}$) величина $\theta$ будет просто параметром,
принимающим числовые значения в~$[0,\infty]$.

\underline{Вариант $\mathrm{B}$}. Фиксируется некоторое число
$T>0$ и рассматривается класс
\begin{equation}
\label{195}%
\mathcal{M}_T=\{\tau:\E_{\infty}\tau\ge T\},
\end{equation}
являющийся классом тех моментов остановки~$\tau$, для которых
среднее время $\E_{\infty}\tau$ до ложной тревоги (т.\,е. когда
$\theta=\infty$) равно~$T$.

Пусть
\begin{equation}
\label{196}%
\mathbf{B}(T)=\inf_{\tau\in\mathcal{M}_T}\frac{1}{T}\int_0^{\infty}\E_{\theta}
\left(\tau-\theta\right)^+d\theta.
\end{equation}
Мы называем момент $\tau^*_T$ оптимальным (если он существует)
в~классе~$\mathcal{M}_T$, если
\begin{equation}
\label{197}%
\frac{1}{T}\int_0^{\infty}\E_{\theta}
\left(\tau_T^*-\theta\right)^+d\theta=\mathbf{B}(T).
\end{equation}
В связи с~рассматриваемым критерием отметим, что интегрирование по
мере Лебега в~\eqref{196} можно рассматривать как интегрирование
по \textit{обобщенному равномерному} распределению на
$\R_+=[0,\infty)$. Это объясняет, почему критерий~\eqref{196}
иногда называют \textit{обобщенным байесовским критерием}, имея
в~виду, что параметр $\theta$ из $[0,\infty)$ можно рассматривать
как случайную величину с~обобщенным равномерным априорным
распределением на~$\R_+$.

\underline{Вариант $\mathbf{C}$}. Пусть
\begin{equation}
\label{198}%
\mathbf{C}(T)
=\inf_{\tau\in\mathcal{M}_T}\sup_{\theta\ge0}
\E_{\theta}\left(\tau-\theta\mid\tau\ge\theta\right).
\end{equation}
Мы говорим, что момент $\tau_T^*\in\mathcal{M}_T$ является
оптимальным (если он существует) в~варианте~$\mathbf{C}$, если
\begin{equation}
\label{199}%
\sup_{\theta\ge0}
\E_{\theta}\left(\tau_T^*-\theta\mid\tau_T^*\ge\theta\right)
=\mathbf{C}(T).
\end{equation}

\underline{Вариант $\mathbf{D}$}. Пусть
\begin{equation}
\label{200}%
\mathbf{D}(T)=\inf_{\tau\in\mathcal{M}_T}\sup_{\theta\ge0}\esssup_{\omega}
\E_{\theta}\left(\left(\tau-\theta\right)^+\mid\F_{\theta}\right)(\omega),
\end{equation}
где $\F_{\theta}=\sigma\left( X_s,s\le\theta\right)$ и
$\esssup_{\omega}$ есть операция взятия существенного супремума
(см. далее замечание).

Момент $\tau_T^*\in\mathcal{M}_T$ называется оптимальным (если он
существует) в~варианте~$\mathbf{D}$, если
\begin{equation}
\label{201}%
\sup_{\theta\ge0}\esssup_{\omega}
\E_{\theta}\left(\left(\tau_T^*-\theta\right)^+\mid\F_{\theta}\right)(\omega)=\mathbf{D}(T).
\end{equation}

\textbf{Замечание.} Пусть $f=f(\omega)$~-- неотрицательная
случайная величина на вероятностном пространстве
$\left(\Omega,\F,\Pr\right)$. Ее \textit{существенным супремумом} (или
существенной верхней гранью) называют нижнюю грань тех~$C$, для
которых $\Pr(f(\omega)>C)=0$. Это значение обозначают
$\|f\|_{\infty}$, $\esssup_{\omega}f(\omega)$ или
$\supvrai_{\omega}f(\omega)$. Так что
\[
\|f\|_{\infty}=\inf\{0\le C\le\infty\colon\Pr\left( f>C\right)=0\}.
\]

\textbf{4.} В приведенных вариантах $\mathbf{A}$, $\mathbf{B}$,
$\mathbf{C}$ и $\mathbf{D}$ задач о~скорейшем обнаружении мы
предполагали, что после объявления сигнала тревоги (в момент
$\tau$) процесс наблюдения заканчивается. На самом же деле
в~реальной практике дело обстоит так, что после объявления тревоги
системы обнаружения продолжают  свое функционирование, ожидая,
скажем, появления следующей разладки.

Приводимый ниже вариант $\mathbf{E}$ задачи скорейшего обнаружения
носит, в~отличие от вариантов~$\mathbf{A}$, $\mathbf{B}$,
$\mathbf{C}$ и~$\mathbf{D}$, многократный характер. При этом
целесообразно считать, что сам процесс наблюдения начался
``давно'', он мог прерываться ложными тревогами и ``разладка''
появляется на ``фоне установившегося режима наблюдения''.

Чтобы сделать этот новый  подход более прозрачным целесообразно
обратиться к конкретной системе наблюдения.

В качестве таковой мы рассмотрим систему наблюдения, в~основе
которой заложены элементы вальдовской процедуры последовательного
анализа различения двух гипотез. Если речь идет о~построении
``хороших'' систем обнаружения ``разладок'', то интуитивно
понятно, что локально, в~каждый момент времени, надо стремиться к
тому, чтобы ``эффективно'' суметь различить две гипотезы~-- ``есть
разладка'' или ``нет разладки''.

Из материала раздела~\ref{sec-5} о~различении двух гипотез
относительно сноса ($\mu_0$ или $\mu_{\infty}$) у наблюдаемого
броуновского движения мы знаем, что достаточной статистикой
является отношение правдоподобия
\begin{equation}
\label{202}%
L_t(X)=\frac{d\Pr_0}{d\Pr_{\infty}}(X,t),
\end{equation}
которое в~предположениях~\eqref{L89} и~\eqref{L90} (с~заменой
$dB_t$ на~$\sigma\,dB_t$) имеет следующий вид (см.~\eqref{L102}):
\begin{equation}
\label{203}%
L_t(X)
=\exp\left\{X_t\,\frac{\mu_0-\mu_{\infty}}{\sigma^2}-
\frac12\,\frac{\mu_0^2-\mu_{\infty}^2}{\sigma^2}\,t\right\}.
\end{equation}
Полагая
\[
Z_t=\log L_t(X)
\]
находим, что
\begin{equation}
\label{204}%
Z_t=X_t\frac{\mu_0-\mu_{\infty}}{\sigma^2}-
\frac12\frac{\mu_0^2-\mu_{\infty}^2}{\sigma^2}t.
\end{equation}
Отсюда ясно, что если
\[
dX_t=\mu_0\,dt+\sigma\,dB_t
\]
(т.\,е. имеет место гипотеза $\mathrm{H}_0$), то
\begin{equation}
\label{205}%
dZ_t=\frac{(\mu_0-\mu_{\infty})^2}{2\sigma^2}\,dt+\frac{\mu_0-\mu_{\infty}}{\sigma}\,dB_t
\end{equation}
и если
\[
dX_t=\mu_{\infty}\,dt+\sigma\,dB_t
\]
(т.\,е. имеет место гипотеза $\mathrm{H}_{\infty}$), то
\begin{equation}
\label{206}%
dZ_t=-\frac{(\mu_0-\mu_{\infty})^2}{2\sigma^2}\,dt+\frac{\mu_0-\mu_{\infty}}{\sigma}\,dB_t
\end{equation}
Положим
\[
\rho=\frac{(\mu_0-\mu_{\infty})^2}{2\sigma^2}.
\]
Тогда при гипотезе $\mathrm{H}_0$
\begin{equation}
\label{207}%
dZ_t=\rho\,dt+\sqrt{2\rho}\,dB_t
\end{equation}
и при гипотезе $\mathrm{H}_{\infty}$
\begin{equation}
\label{208}%
dZ_t=-\rho\,dt+\sqrt{2\rho}\,dB_t.
\end{equation}
Поскольку по распределению процессы $\left(\sqrt{\rho}B_t\right)_{t\ge0}$
и $\left( B_{t\rho}\right)_{t\ge0}$ совпадают, то из~\eqref{207}
и~\eqref{208} следует, что с~точки зрения рассматриваемых далее
свойств по распределению можно, сделав замену времени $t\to\rho
t$, считать, что $\rho=1$.

Итак, пусть при гипотезе $\mathrm{H}_0$
\begin{equation}
\label{209}%
dZ_t=dt+\sqrt{2}\,dB_t
\end{equation}
и при гипотезе $\mathrm{H}_{\infty}$
\begin{equation}
\label{210}%
dZ_t=-dt+\sqrt{2}\,dB_t.
\end{equation}

Идея использования вальдовской процедуры с~целью обнаружения
``разладки'' состоит в~следующем (см. рис.~\ref{fig6}). Выберем
два порога $A$ и~$B$, $A<0<B$, и начинаем наблюдать процесс
\[
Z_t=\log L_t(X),\qquad Z_0=0,
\]
с
\begin{equation}
\label{211}%
dZ_t
=\frac{\mu_0-\mu_{\infty}}{\sigma^2}\,dX_t
-\frac12\frac{\mu_0^2-\mu_{\infty}^2}{\sigma^2}\,dt
\end{equation}
(см.~\eqref{204}). Приняв соглашение, что $\rho=1$, можно считать,
что
\[
\mu_0=\sqrt{2},\quad
\mu_{\infty}=0,\quad
\sigma^2=1.
\]
Так что
\begin{equation}
\label{212}%
dZ_t=\sqrt{2}\,dX_t-dt,
\end{equation}
где при гипотезе $\mathrm{H}_0$
\begin{equation}
\label{213}%
dX_t=dB_t,
\end{equation}
а при гипотезе $\mathrm{H}_\infty$
\begin{equation}
\label{214}%
dX_t=\sqrt{2}\,dt+dB_t.
\end{equation}
Если первый выход значений процесса $(Z_t)_{t\ge0}$ из интервала
$(A,B)$ происходит (в~момент~$\tau_1$) на нижнем уровне~$A$, то
выносится ``внутреннее'' решение, что ``разладки'' нет и
происходит мгновенный возврат процесса $(Z_t)_{t\ge0}$ в~нуль
с~возобновлением наблюдений за этим процессом, управляемым
согласно динамике~\eqref{211} с~новым ``начальным'' значением
$Z_{\tau_1}$ и~т.\,д.

В том случае, когда так сконструированный наблюдаемый процесс
выходит на  верхнюю границу~$B$, то объявляется ``тревога''
о~появлении ``разладки'', которая может оказаться как правильной
(если ``разладка'' действительно произошла), так и ложной (если
``разладка'' еще не появилась). Но в~любом случае (и~по достижении
уровня $A$ и уровня $B$), происходит возвращение в~нуль
с~возобновлением наблюдений. Полученный из процесса
$(Z_t)_{t\ge0}$ в~результате вышеописанной процедуры процесс будем
обозначать через $(\widehat{Z}_t)_{t\ge0}$.

Право же определения истинности или ложности поднятой ``тревоги''
передается ``другому ведомству''.

\textbf{2.} Чтобы сформулировать постановку задачи о~``разладке''
в варианте~\textbf{E}, обозначим
$\widehat{\mathbf{T}}_{\infty}(A,B)$ математическое ожидание
времени первого выхода введенного процесса
$(\widehat{Z}_t)_{t\ge0}$ c~$\widehat{Z}_0=0$ на верхний
уровень~$B$. Смысл этой величины
$\widehat{\mathbf{T}}_{\infty}(A,B)$ ясен~-- это среднее время до
первой ложной тревоги.

В силу многоэтапности рассматриваемой процедуры
на\-блю\-де\-ния/при\-ня\-тия решения и рекуррентности
(возобновляемости) процесса $(\widehat{Z}_t)_{t\ge0}$ величину
$\widehat{\mathbf{T}}_{\infty}(A,B)$ можно интерпретировать как
\textit{среднее время между двумя ложными тревогами}.

В предположении отсутствия разладок в~рассматриваемой процедуре
наблюдения над процессом $(\widehat{Z}_t)_{t\ge0}$ устанавливается
\textit{стационарный режим} в~том смысле, что существует (не
зависящий от~$x$) предел
\begin{equation}
\label{215}%
\lim_{t\to\infty}p_t(y,x)
\,\,(=\widehat{p}(y)),
\qquad A<y<B,
\end{equation}
где
\begin{equation}
\label{216}%
p_t(y,x)=\frac{\partial}{\partial y}\Pr_{\infty}
(\widehat{Z}_t\le y\mid\widehat{Z}_0=x).
\end{equation}
Естественно теперь \textit{величиной запаздывания} в~обнаружении
``разладки'', в~предположении, что она появляется на фоне
установившегося стационарного режима, назвать величину
\begin{equation}
\label{217}%
\widehat{\mathbf{R}}(A,B)
=\int_A^B\widehat{\mathbf{T}}_0(A,B;y)\widehat{p}(y)\,dy,
\end{equation}
где $\widehat{\mathbf{T}}_0(A,B;y)$~-- математическое ожидание
времени выхода процесса $(\widehat{Z}_t)_{t\ge0}$ на границу~$B$
в~предположении, что $\widehat{Z}_0=y$, $A<y<B$.

Пусть $T>0$~-- некоторая фиксированная константа.

\underline{Вариант $\mathbf{E}$} задачи о~``разладке'' (для
описанной ``вальдовской'' процедуры наблюдения) состоит
в~следующем: найти или оценить
\begin{equation}
\label{218}%
\widehat{\mathbf{R}}(T)=\inf\widehat{\mathbf{R}}(A,B),
\end{equation}
где $\inf$ берется по всем тем парам $(A,B)$, для которых
\begin{equation}
\label{219}%
\widehat{\mathbf{T}}_{\infty}(A,B)=T.
\end{equation}
(В дальнейшем ``Вариант $\mathbf{E}$'' будет рассмотрен в общем
случае и для других процедур наблюдения.)