\section{Обнаружение аномалий. Задачи о разладке}
\subsection{Мотивация}
Как известно, многие реальные процессы (трафик, доходность акций, атмосферное давление, ЭЭГ и так далее) можно описать многомерными временными рядами. 
В этих временных рядах обычно выделяют различные компоненты:
\begin{itemize}
	\item Тренды. 
	Например, когда временной ряд на каком-то участке линейно растёт.
	\item Циклы, то есть наблюдается периодичность временного ряда на каком-то участке.
	\item Корреляции.
\end{itemize}

Для описание, оценивания и создания каких-либо разумных выводов о наблюдаемых временных рядах была разработана теория случайных процессов. 
В ней были разработаны такие методы, как фильтрация, сегментация, шумоподавление, анализ трендов, различные виды анализов: корреляционный, дисперсионный, регрессионный, морфологический и так далее.

Одной из компонент временного ряда является так называемая \emph{разладка}, то есть изменение статистических свойств ряда. 
Вариантов разладок бывает много: например, разрывы, изломы, рост разброса и нарушения цикла. 
На практике это может соответствовать, например, поломке агрегата. 
Из этого и появилась \emph{задача о разладке}: как обнаружить возникающее изменение?

Теперь рассмотрим несколько примеров разладок:
\begin{itemize}
	\item Пузырь доткомов. % расписать подробнее
	\item Посещаемость интернет-ресурса. 
	Тут стоит различить два варианта: краткосрочные и долгосрочные разладки. 
	Первые могут возникнуть, например, из-за неполадок с серверами или же из-за того, что кто-то известный прорекламировал ресурс. 
	Долгосрочные же, например, вызываются некачественным контентом.
\end{itemize}

Вообще, задача о разладке "--- это весьма актуальная проблема, и она возникает в самых разных областях. 
Перечислим лишь несколько из них:
\begin{itemize}
	\item Обнаружение внедрений в компьютерные сети (атак, ведущих к изменению объема передаваемого трафика),
	\item Обнаружение аномалий в сетях передачи данных (видеопотоки в системах видеонаблюдения, сетевой трафик и так далее),
	\item Обнаружение и изоляция отказов узлов систем управления транспортными средствами,
	\item Мониторинг целостности системы геопозиционирования,
	\item Обнаружение изменений структуры породы при бурении скважин,
	\item Обнаружение начала рецессии или экономического роста,
	\item Обнаружение изменений волатильности индекса Dow Jones,
	\item Обнаружение сигнала при наблюдении подводных целей,
	\item Автоматическое обнаружение аномального человеческого поведения при видеонаблюдении,
	\item Автоматический контроль качества выпускаемой продукции,
	\item Мониторинг и анализ смертности и заболеваемости раком легких,
	\item Обнаружение возникновения эпидемий,
	\item Обнаружение аритмии (внезапных изменений ритма биения) сердца,
	\item Предсказание транзиторных ишемических атак (преходящих нарушений мозгового кровообращения),
	\item Диагностика задержки внутриутробного роста,
	\item Анализ несчастных случаев на угольных шахтах,
	\item Мониторинг уровня хлора в питьевой воде.
\end{itemize}

На самом деле первые работы по разладкам появились очень давно: Уолтер Шухарт написал свою статью про контроль качества выпускаемой продукции в 1931-м году. 
На данный же момент работ по разладкам огромное количество: поиск в системе индексации Google Scholar выдает, что на период с 2000 по 2017 год было написано статей с ключевыми словами
\begin{itemize}
	\item change point detection "--- 10 200 статей
	\item anomaly detection "--- 53 900 статей
	\item break detection "--- 3 980 статей
	\item обнаружение разладок, обнаружение аномалий, обнаружение изменений "--- 765 статей
\end{itemize}

\subsection{Математическая задача о разладке}

Пусть \(\{\xi_{k}\}_{k \geq 1}\)~--- последовательность независимых случайных величин таких, что в~моменты времени \(k = 1, 2, \ldots, \theta - 1\) они имеют распределение \(\Pr_{\infty}\) с~плотностью~\(f_{\infty}(x)\), а в~моменты \(k = \theta, \theta + 1, \ldots\) "--- распределение \(\Pr_{0}\) с~плотностью~\(f_{0}(x)\). 
Далее, над ними проводится наблюдение, что даёт последовательность чисел \(\{x_{k}\}_{k \geq 1}\). 

Параметр \(\theta\) считается заранее не известным и принимающим значения \(0, 1, \ldots, \infty\). 
Значения \(x_{\theta - 1}\) при \(\theta = 0\) и \(\theta = 1\) будем считать равными нулю. 
Такое искусственное введение этих моментов будет оправдано рассмотрениями переходов от дискретных схем к~соответствующим непрерывным (во времени) схемам.

Несложно понять, что случаи \(\theta = 0\) и \(\theta = 1\) отвечают наблюдениям с~плотностью~\(f_{0}(x)\). 
Если же \(\theta = \infty\), то все наблюдения имеют плотность распределения вероятностей~\(f_{\infty}(x)\).

Пусть \(p_{\theta}(x_{1}, \ldots, x_{n})\) "--- это плотность распределения случайных величин \(\xi_{1}, \ldots, \xi_{n}\) при условии, что <<переход>> выполняется в момент \(\theta \in [\,0, +\infty]\). 
Тогда для любого \(n \in \N\)
\begin{equation}\label{eq:anomaly-detection-density}
	p_{\theta}(x_{1}, \ldots, x_{n})
	=
	\begin{cases}
		f_{\infty}(x_{1}) \ldots f_{\infty}(x_{n}),& \theta = 0, 1, \\
		f_{\infty}(x_{1}) \ldots f_{\infty}(x_{\theta - 1}) f_{0}(x_{\theta}) \ldots f_{0}(x_{n}),& 1 < \theta \leq n, \\
		f_{0}(x_{1}) \ldots f_{0}(x_{n}),& \theta > n.
	\end{cases}
\end{equation}

Далее, введём следующую статистику:
\[
	\gamma_{n} 
	= \sup_{\theta \geq 0} \left(\frac{p_{\theta}(x_{1}, \ldots, x_{n})}{p_{\infty}(x_{1}, \ldots, x_{n})}\right).
\]

Заметим, что её можно переписать следующим образом:
\begin{align*}
	\gamma_{n} 
	&= \max\left(
		\frac{p_{0}(x_{1}, \ldots, x_{n})}{p_{\infty}(x_{1}, \ldots, x_{n})},
		\frac{p_{1}(x_{1}, \ldots, x_{n})}{p_{\infty}(x_{1}, \ldots, x_{n})}, 
		\ldots, 
		\frac{p_{n}(x_{1}, \ldots, x_{n})}{p_{\infty}(x_{1}, \ldots, x_{n})},
		\frac{p_{\infty}(x_{1}, \ldots, x_{n})}{p_{\infty}(x_{1}, \ldots, x_{n})}
	\right) \\
	&= \max\left(
		\frac{p_{0}(x_{1}, \ldots, x_{n})}{p_{\infty}(x_{1}, \ldots, x_{n})},
		\frac{p_{0}(x_{2}, \ldots, x_{n})}{p_{\infty}(x_{2}, \ldots, x_{n})}, 
		\ldots, 
		\frac{p_{0}(x_{n})}{p_{\infty}(x_{n})},
		1
	\right) \\
	&= \max\left(
		1, 
		\max_{1 \leq \theta \leq n}
			\prod_{k = \theta}^{n} \frac{f_{0}(x_{k})}{f_{\infty}(x_{k})}
	\right).
\end{align*}

Далее, введём ещё одну статистику, пользуясь обозначением \(\zeta_{k} = \ln f_{0}(x_{k}) - \ln f_{\infty}(x_{k})\):
\[
	T_{n} 
	= \ln \gamma_{n} 
	= \max\left(
		0, 
		\max_{1 \leq \theta \leq n}
			\sum_{k = \theta}^{n} \ln\frac{f_{0}(x_{k})}{f_{\infty}(x_{k})}
	\right)
	= \max\left(
		0, 
		\max_{1 \leq \theta \leq n}
			\sum_{k = \theta}^{n} \zeta_{k}
	\right).
\]

Далее, если ввести обозначение \(Z_{n} = \zeta_{1} + \ldots + \zeta_{n}\), то её можно переписать следующим образом:
\[
	T_{n} 
	= \max\left(
		0, 
		\max_{1 \leq \theta \leq n} [Z_{n} - Z_{\theta - 1}]
	\right)
	= \max\left(
		0, 
		Z_{n} - \min_{0 \leq \theta \leq n - 1} Z_{\theta}
	\right)
	= Z_{n} - \min_{0 \leq \theta \leq n} Z_{\theta}.
\]

Статистики \(T_{n}\) и \(\gamma_{n}\) играют в~задачах скорейшего обнаружения исключительно важную роль. 
Напомним, что в~задаче различения двух гипотез (\(\mathrm{H}_0\) и \(\mathrm{H}_{\infty}\)) ключевую роль играет отношение правдоподобия
\[
	L_{n} 
	= \frac{f_{0}(x_{1}) \ldots f_{0}(x_{n})}{f_{\infty}(x_{1}) \ldots f_{\infty}(x_{n})},
	\quad
	L_{0} 
	= 1.
\]

Заметим, что
\[
	\gamma_{n} 
	= \max\left(
		1, 
		\max_{1 \leq \theta \leq n}
			\prod_{k = \theta}^{n} \frac{f_{0}(x_{k})}{f_{\infty}(x_{k})}
	\right)
	= \max\left(
		\frac{L_{n}}{L_{n}}, 
		\max_{1 \leq \theta \leq n} \frac{L_{n}}{L_{\theta - 1}}
	\right)
	= \max_{0 \leq \theta \leq n} \frac{L_{n}}{L_{\theta}}.
\]

Для статистики \(T_{n}\) выполняется достаточно важное рекуррентное соотношение.
\begin{align*}
	T_{n} 
	&= Z_{n - 1} + \zeta_{n} - \min_{0 \leq \theta \leq n} Z_{\theta}
	= \zeta_{n} + \max_{0 \leq \theta \leq n}(Z_{n - 1} - Z_{\theta}) \\
	&= \zeta_{n} + \max(Z_{n - 1} - Z_{n}, \max_{0 \leq \theta \leq n - 1}(Z_{n - 1} - Z_{\theta})) \\
	&= \zeta_{n} + \max(Z_{n - 1} - Z_{n}, Z_{n} - \min_{0 \leq \theta \leq n - 1} Z_{\theta}) \\
	&= \zeta_{n} + \max(-\zeta_{n}, T_{n - 1}) = \max(0, T_{n - 1} + \zeta_{n}).
\end{align*}

Из этого свойства, в~частности, видно, что в~рассматриваемом случае независимых наблюдений статистики \((T_{n})_{n \geq 1}\), образуют марковскую цепь (по каждой из мер \(\Pr_{0}\) и \(\Pr_{\infty}\) в~пространстве последовательностей \(x = (x_1, x_2, \ldots)\)). 
Аналогичное верно и для статистик~\((L_{n})_{n \geq 1}\), так как \(L_{n} = L_{n - 1}e^{\zeta_{n}}\).

Теперь можно дать названия введённым статистикам.
\begin{itemize}
	\item Статистики \(\gamma = (\gamma_{n})_{n \geq 1}\) принято называть \emph{обобщенными отношениями правдоподобия}. 
	В~определенных постановках задач скорейшего обнаружения на этих статистиках основаны оптимальные или асимптотически оптимальные решения.
	
	\item Статистики \(T = (T_{n})_{n \geq 1}\) называют \emph{статистиками кумулятивных сумм} (или же CUSUM "--- от CUmulative SUMs). 
	Это название объясняется тем, что там, где \(T_{k} > 0\), \(k \leq n\), статистика \(T_{n}\) просто равна сумме \(\zeta_{k}\), то есть является кумулятивной суммой. 
	Если же \(T_{n - 1} + \zeta_{n} < 0\), то согласно рекуррентному соотношению \(T_{n}\) просто полагается равной нулю, а не \(T_{n - 1} + \zeta_{n}\).
\end{itemize}

К слову об истории. 
<<Первопроходец>> в теории решения задач о разладке, Уолтер Шухарт, предложил следующую статистику, которую называют \emph{контрольными картами Шухарта}. 
Её суть состоит в следующем: последовательность наблюдений \((x_{1}, x_{2}, \ldots)\) разбивают на группы (батчи) одинаковой длины \(n\), после чего для каждого батча строится логарифм отношения правдоподобия:
\[
	S_{m}^{n} = \ln\frac{p_{0}(x_{m}, x_{m + 1}, \ldots, x_{n})}{p_{\infty}(x_{m}, x_{m + 1}, \ldots, x_{n})}.
\]

Если все \(\xi_{k}\) независимы и одинаково распределены, то \(S_{m}^{n} = \zeta_{m} + \zeta_{m + 1} + \ldots + \zeta_{n}\).

Ещё вводится так называемая \emph{статистика Ширяева-Робертса} (SR-статистика):
\[
	R_{n} 
	= \sum_{k = 1}^{n} \frac{p_{\theta}(x_{1}, \ldots, x_{n})}{p_{\infty}(x_{1}, \ldots, x_{n})}.
\]

В случае независимых и одинаково распределённых случайных величин SR-статистику можно записать следующим образом:
\[
	R_{n} 
	= \sum_{\theta = 1}^{n} \prod_{k = \theta}^{n} \frac{f_{0}(x_{k})}{f_{\infty}(x_{k})}.
\]

Теперь вспомним, что
\[
	\prod_{k = \theta}^{n} \frac{f_{0}(x_{k})}{f_{\infty}(x_{k})} 
	= \frac{L_{n}}{L_{\theta - 1}} 
	\implies 
	R_{n} 
	= \sum_{\theta = 1}^{n} \frac{L_{n}}{L_{\theta - 1}} 
	= \sum_{\theta = 1}^{n} \exp\{Z_{n} - Z_{\theta - 1}\}.
\]

Для статистики \(R_{n}\) верно следующее рекуррентное соотношение (при \(R_{0} = 0\)):
\begin{align*}
	R_{n} 
	&= \sum_{\theta = 1}^{n} \exp\{Z_{n} - Z_{\theta - 1}\} 
	= \exp\{Z_{n} - Z_{n - 1}\} + \sum_{\theta = 1}^{n - 1} \exp\{Z_{n} - Z_{\theta - 1}\} \\
	&= e^{\zeta_{n}} + e^{\zeta_{n}}\sum_{\theta = 1}^{n - 1} \exp\{Z_{n - 1} - Z_{\theta - 1}\}
	= (1 + R_{n - 1})e^{\zeta_{n}}.
\end{align*}

Допустим, что мы рассматриваем случай с нормальным распределением, когда \(\Pr_{0}(x) = \mathcal{N}(\mu_{0}, \sigma^{2})\), а \(\Pr_{\infty}(x) = \mathcal{N}(\mu_{\infty}, \sigma^{2})\). 
В этом случае плотности равны
\[
	f_{0}(x) 
	= \frac{1}{\sqrt{2\pi\sigma^{2}}} \exp\left\{-\frac{(x - \mu_{0})^{2}}{2\sigma^{2}}\right\},
	\qquad
	f_{0}(x) 
	= \frac{1}{\sqrt{2\pi\sigma^{2}}} \exp\left\{-\frac{(x - \mu_{\infty})^{2}}{2\sigma^{2}}\right\}.
\]

Следовательно, \(\zeta_{n}\) равна
\[
	\zeta_{n} 
	= \ln\frac{f_{0}(x_{n})}{f_{\infty}(x_{n})}
	= \frac{(x - \mu_{\infty})^{2}}{2\sigma^{2}} - \frac{(x - \mu_{0})^{2}}{2\sigma^{2}}
	= \frac{\mu_{0} - \mu_{\infty}}{\sigma^{2}}\left(x_{n} - \frac{\mu_{0} + \mu_{\infty}}{2}\right).
\]

Теперь несложно выписать рекуррентные формулы для \(T_{n}\) и \(R_{n}\):
\begin{align*}
	T_{n} 
	&= \max\left(
		0,
		T_{n - 1} + \frac{\mu_{0} - \mu_{\infty}}{\sigma^{2}}\left(x_{n} - \frac{\mu_{0} + \mu_{\infty}}{2}\right)
	\right) \\
	R_{n} 
	&= (1 + R_{n - 1})\exp\left\{\frac{\mu_{0} - \mu_{\infty}}{\sigma^{2}}\left(x_{n} - \frac{\mu_{0} + \mu_{\infty}}{2}\right)\right\}.
\end{align*}

Теперь заменим измерение случайной величины \(x_{n}\) на саму случайную величину \(\xi_{n}\). 
Тогда можно посчитать матожидания \(\zeta_{n}\) по мерам \(\Pr_{0}\) и \(\Pr_{\infty}\):
\begin{align*}
	\E_{0}[\zeta_{n}] 
	&= \frac{\mu_{0} - \mu_{\infty}}{\sigma^{2}}\left(\mu_{0} - \frac{\mu_{0} + \mu_{\infty}}{2}\right)
	= \frac{(\mu_{0} - \mu_{\infty})^{2}}{\sigma^{2}}, \\
	\E_{\infty}[\zeta_{n}] 
	&= \frac{\mu_{0} - \mu_{\infty}}{\sigma^{2}}\left(\mu_{\infty} - \frac{\mu_{0} + \mu_{\infty}}{2}\right)
	= -\frac{(\mu_{0} - \mu_{\infty})^{2}}{\sigma^{2}}.
\end{align*}

Отсюда видно, что при \(n < \theta\) статистика \(T_{n}\) близка к нулю, однако при \(n \geq \theta\) она начинает (в~среднем) возрастать. 
Это рассуждение наглядно показывает, что статистики \(T_{n}\) и \(R_{n}\) позволяют <<ухватывать>> момент появления разладки, то есть момент \(\theta\).
Именно эти наблюдения послужили основанием применения этих статистик к~реальным данным.
Разумеется, для этой цели надо знать или уметь <<хорошо>> оценивать величины \(\zeta_{n}\), когда плотности \(f_{0}(x)\) и \(f_{\infty}(x)\) точно не известны.

Среди других методов, в~которых не требуется знание плотностей, укажем на ``метод экспоненциального сглаживания''. 
Он состоит в~следующем: по наблюдениям \((x_{1}, x_{2}, \ldots)\) строятся статистики \((Y_{n})_{n \geq 0}\) по следующему правилу:
\[
	Y_{n} = (1 - \beta)Y_{n-1} + \beta x_{n},
\]
где \(Y_{0} = 0\) и \(\beta \in (0, 1)\) "--- некоторый (подбираемый) параметр. 
Здесь несложно развернуть рекурсифную формулу, которая объясняет название метода:
\begin{align*}
	Y_{n} 
	&= (1 - \beta)Y_{n-1} + \beta x_{n}
	= (1 - \beta)^{2}Y_{n - 2} + \beta(1 - \beta)x_{n - 1} + \beta x_{n} \\
	&= \ldots
	= \beta\sum_{k = 1}^{n}(1 - \beta)^{n - k}x_{k}.
\end{align*}

В~случае различения двух гипотез мы рассмотрели две постановки задач. 
Одна (формулировка Неймана--Пирсона) носила, так сказать, ретроспективный характер, поскольку решение выносилось по заданным наблюдениям \((x_{1}, x_{2}, \ldots, x_{n})\). 
Вторая постановка (формулировка А.~Вальда) исходила из того, что наблюдения поступают последовательно, шаг за шагом, и решение принимается по поведению текущих данных. 

В задаче о разладке также принято различать две постановки "--- ретроспективная и последовательная.
В~первом случае наблюдения \((x_{1}, x_{2}, \ldots, x_{n})\) заданы \emph{полностью} и считается, что \(\theta \in \{1, 2, \ldots, n, \infty\}\).
Во втором случае наблюдения поступаюь последовательно и, вообще говоря, \(\theta \in \N\).
Введенные статистики кумулятивных сумм \(T_{n}\) и статистики Ширяева-Робертса \(R_{n}\) пригождаются в~обоих сформулированных случаях.

Стоит заметить, что введённые выше статистики предназначены для тех ситуаций, когда относительно параметра \(\theta\) не делается никаких априорных предположений. 
А как можно улучшить ситуацию, если такое предположение есть?
Рассмотрим на следующем примере.
Пусть \(\theta\) "--- это случайная величина, принимающая значения из \(\Z_{+}\) со следующими вероятностями:
\begin{align*}
	\Pr{\theta = 0} 
	&= \pi, \\
	\Pr{\theta = n \given \theta > 0}
	&= pq^{n - 1},
	\quad
	n \in \N,
\end{align*}
где \(\pi \in [0, 1]\), \(q \equiv 1 - p\).
Сразу же скажем, что \(\Pr(\theta = n) = (1 - \pi)pq^{n - 1}\) для любого \(n \in \N\).

Как и раньше, считаем, что случайные величины независимы и одинаково распределены с плотностями \(f_{\infty}(x)\) (до момента разладки) и \(f_{0}(x)\) (после). 
Тогда совместная плотность \(p_{\theta}(x_{1}, \ldots, x_{n})\) задаётся формулой \eqref{eq:anomaly-detection-density}.

Пусть \(p^{\pi}(x_{1}, \ldots, x_{n})\) "--- это плотность набора случайных величин \((\xi_{1}, \ldots, \xi_{n})\) в точках \((x_{1}, \ldots, x_{n})\) при условии, что \(\theta\) есть случайная величина с распределением, заданным выше. 
Тогда по формуле полной вероятности
\begin{align*}
	p^{\pi}(x_{1}, \ldots, x_{n}) 
	&= \sum_{k = 0}^{\infty} p_{k}(x_{1}, \ldots, x_{n})\Pr{\theta = k} \\
	&= (\pi + p(1 - \pi)) f_{0}(x_{1}) \ldots f_{0}(x_{n}) \\
	&\phantom{=} + (1 - \pi)p\sum_{k = 1}^{n} q^{k} f_{\infty}(x_{1}) \ldots f_{\infty}(x_{k - 1}) f_{0}(x_{k}) \ldots f_{0}(x_{n}) \\
	&\phantom{=} + (1 - \pi)q^{n} f_{\infty}(x_{1}) \ldots f_{\infty}(x_{n}).
\end{align*}

Далее, введём статистику \((\pi_{n})_{n \geq 1}\) по следующему правилу:
\[
	\pi_{n}
	= \Pr{\theta \leq n \given x_{1}, \ldots, x_{n}}.
\]
Она будет играть исключительно важную роль во многих постановках задач скорейшего обнаружения.

Можно ли считать её рекурсивно? 
Можно. 
Для этого введём следующее обозначение:
\[
	\phi_{n} 
	\equiv \frac{\pi_{n}}{1 - \pi_{n}}.
\]

Теперь докажем одну лемму, связанную с \(\phi_{n}\):
\begin{lemma}
	Для любого натурального \(n\)
	\[
		\phi_{n} 
		= (p + \phi_{n - 1})\frac{f_{0}(x_{n})}{q f_{\infty}(x_{n})}.
	\]
\end{lemma}
\begin{proof}
	Для начала вспомним формулу Байеса:
	\[
		\Pr{A \given B} = \frac{\Pr{A}}{\Pr{B}}\Pr{B \given A}.
	\]
	
	Отсюда получаем, что
	\[
		\Pr{\theta = k \given x_{1}, \ldots, x_{n}} = \frac{\Pr{\theta = k}}{p^{\pi}(x_{1}, \ldots, x_{n})}p_{k}(x_{1}, \ldots, x_{n}).
	\]
	
	Вопспользуемся этим и распишем \(\phi_{n}\):
	\begin{align*}
		\phi_{n}
		&= \frac{\Pr{\theta \leq n \given x_{1}, \ldots, x_{n}}}{\Pr{\theta > n \given x_{1}, \ldots, x_{n}}}
		= \sum_{k = 0}^{n} \frac{p_{k}(x_{1}, \ldots, x_{n})\Pr{\theta = k}}{p_{\infty}(x_{1}, \ldots, x_{n})\Pr{\theta > n}} \\
		&= \frac{p_{0}(x_{1}, \ldots, x_{n})\Pr{\theta = 0}}{p_{\infty}(x_{1}, \ldots, x_{n})\Pr{\theta > n}} 
		+ \sum_{k = 1}^{n} \frac{p_{k}(x_{1}, \ldots, x_{n})\Pr{\theta = k}}{p_{\infty}(x_{1}, \ldots, x_{n})\Pr{\theta > n}}.
	\end{align*}
	
	Теперь вспомним, что \(\Pr{\theta = 0} = \pi\) и \(\Pr{\theta = k} = (1 - \pi)pq^{k - 1}\) для любого \(k \in \N\). Тогда 
	\[
		\Pr{\theta > n} 
		= \sum_{k = n + 1}^{\infty} \Pr{\theta = k} 
		= (1 - \pi)pq^{n}\sum_{k = 0}^{\infty} q^{k} 
		= (1 - \pi)q^{n}.
	\]
	
	Далее, введём следующее обозначение:
	\[
		L_{m}^{n} 
		\equiv \frac{f_{0}(x_{m}) f_{0}(x_{m + 1}) \ldots f_{0}(x_{n})}{f_{\infty}(x_{m}) f_{\infty}(x_{m + 1}) \ldots f_{\infty}(x_{n})}.
	\]
	
	Пользуясь вышесказанным и \eqref{eq:anomaly-detection-density}, преобразуем \(\phi_{n}\):
	\[
		\phi_{n}
		= \frac{1}{q^{n}}\left(\frac{\pi}{1 - \pi}L_{1}^{n} + pL_{1}^{n} + pqL_{2}^{n} + \ldots + pq^{n - 1}L_{n}^{n}\right).
	\]
	
	Далее заметим, что
	\[
		\phi_{n}
		= \frac{f_{0}(x_{n})}{qf_{\infty}(x_{n})}\left(p + \frac{1}{q^{n - 1}}\left(\frac{\pi}{1 - \pi}L_{1}^{n - 1} + pL_{1}^{n - 1} + pqL_{2}^{n - 1} + \ldots + pq^{n - 2}L_{n - 1}^{n - 1}\right)\right).
	\]
	
	А это и есть желаемое:
	\[
		\phi_{n}
		= (p + \phi_{n - 1})\frac{f_{0}(x_{n})}{qf_{\infty}(x_{n})}. \qedhere
	\]
\end{proof}

Отсюда несложно получить рекурсивное выражение для \(\pi_{n + 1}\):
\[
	\pi_{n + 1} = \frac{f_{0}(x_{n + 1})(\pi_{n} + p(1 - \pi_{n}))}{f_{0}(x_{n + 1})(\pi_{n} + p(1 - \pi_{n})) + f_{\infty}(x_{n + 1})q(1 - \pi_{n})}.
\]

\begin{exercise}
	Оказывается, что получить рекуррентную формулу для \(\pi_{n + 1}\) можно напрямую, пользуясь формулой Байеса. Покажите это.
\end{exercise}

Пусть
\[
	\zeta_{n}(p)
	= \ln \frac{f_{0}(x_{n})}{(1 - p)f_{\infty}(x_{n})}.
\]

Следовательно,
\[
	\phi_{n}
	= (p + \phi_{n - 1})e^{\zeta_{n}(p)} 
	\implies 
	\frac{\phi_{n}}{p} 
	= \left(1 + \frac{\phi_{n}}{p}\right)e^{\zeta_{n}(p)}.
\]

Тем самым, статистика \(R_{n}(p) \equiv \phi_{n}/p\) достаточно похожа на статистику Ширяева-Робертса \(R_{n}\) (в том плане, что их рекуррентные соотношения похожи). 
Более того, при \(p \to 0\) \(\zeta_{n}(p) \to \zeta_{n}\) и \(R_{n}(p) \to R_{n}\).

Это показывает, что введенная выше ad-hoc статистика \(R_{n}\) возникает весьма естественным образом из статистики \(R_{n}(p)\) при \(p \to 0\). 
Стоит заметить, что
\[
	\E{\theta} = \frac{1 - \pi}{p}.
\]

Таким образом, предельный переход \(p \to 0\) означает, что делается предположение, что появление разладки следует ожидать нескоро.

\subsection{Основные постановки задачи скорейшего обнаружения для броуновского движения}

В статистике случайных процессов хорошо развита та ее часть, в~которой исходными предположениями является то, что
рассматриваемые процессы являются \emph{стационарными}.
В основе теории таких процессов лежат ковариационно-спектральные характеристики (то есть ковариационная функция, так называемая спектральная плотность и иже с ними).
Типичной задачей статистики таких процессов является задача определения корреляционной функции и среднего значения наблюдаемого процесса.

Сейчас же мы будем рассматривать сугубо \emph{нестационарные} процессы. 
Это обстоятельство приводит и к новым постановкам задач и к необходимости развития соответствующей теории их решения.
Описанный ниже материал будет посвящён моделям, основанным на броуновском движении, что объясняется и тем, что такие модели представляют практический интерес, и тем, что для них во многих случаях удается получить прозрачные и точные результаты.%
\footnote{Действительно, для броуновского движения задача различения двух гипотез относительно сноса может быть решена точно, но в общем случае получить что-то лучше приближенных результатов не так уж и просто.}

Пусть \(B = (B_{t})_{t \geq 0}\) "--- броуновское движение, заданное на некотором вероятностном пространстве \((\Omega, \F, \Pr)\). 
Построим по нему процесс \(X = (X_{t})_{t \geq 0}\) по следующему правилу:
\[
	X_{t} 
	= \mu\I_{\{t \geq \theta\}} + \sigma B_{t}
	\implies
	X_{t}
	= \begin{cases}
		\sigma B_{t}, & t < \theta, \\
		\mu + \sigma B_{t}, & t \geq \theta,
	\end{cases}
\]
где \(\sigma > 0\), \(\mu \neq 0\) и \(\theta \in [\,0, \infty]\).

Относительно природы момента \(\theta\), то есть того, является ли \(\theta\) случайной величиной или же просто константным параметром, можно будет делать вполне конкретные предположения. 
Сейчас же важно лишь то, что момент \(\theta\) "--- это тот момент, когда у наблюдаемого процесса \(X\) меняется снос у броуновского движения (с~\(0\) на~\(\mu\)). 
Момент $\theta$ называется моментом появления \emph{разладки}. 
При этом <<нормальный>> ход процесса~\(X\), характеризующийся тем, что \(X_{t} = \sigma B_{t}\), переходит
в~<<разлаженный>>: \(X_{t} = \mu + \sigma B_{t}\), где снос \(\mu \neq 0\).

Несложно понять, что означают крайние случаи \(\theta = 0\) и \(\theta = \infty\). 
Случай, когда \(\theta = 0\), соответствует тому, что с~самого начала (т.\,е. с~момента~\(t = 0\)) идет <<разлаженный>> процесс. 
Случай \(\theta = \infty\) соответствует тому, что <<разладка>> не появляется и вовсе, следовательно, у наблюдаемого процесса~\(X\) все время \(X_{t} = \sigma B_{t}\).

Пусть \(\Pr_{\theta} = \Law(X\,|\,\theta)\)~--- распределение вероятностей (по сути, семейство конечномерных функций распределения) процесса \(X = (X_{t})_{t \geq 0}\) при условии, что разладка происходит в момент времени \(\theta\). В частности, \(\Pr_{\infty}\) есть распределение вероятностей процесса \(X\) при условии, что разладка вообще не происходит.

Для дальнейшего повествования понадобится понятие \hyperref[def:markov-stopping-moment]{момента остановки}, или же марковского момента.
Наглядный смысл этого поняти состоит в том, что для каждого \(t \geq 0\) решение вопроса о~том, прекратить ли наблюдения или же продолжать их, зависит лишь от информации о~процессе \(X\), полученной на интервале времени \([\,0, t]\) и не зависит от <<будущего>>.
В данный момент марковские моменты удобно рассматривать, как моменты подачи сигнала тревоги о появлении разладки.

Рассмотрим два события: \(\{\tau < \theta\}\) и \(\{\tau \geq \theta\}\). 
Первое событие означает ложную тревогу (так как сигнал подаётся до момента разладки).
Во втором же событии нас обычно интересует то, насколько велико запаздывание \(\tau - \theta\) при правильной подаче сигнала.

Теперь сформулируем четыре варианта задачи о разладке.

\begin{enumerate}[label=(\textbf{\Alph*})]
	\item Пусть \(\theta\) "--- это случайная величина со следующим распределением:
	\begin{align*}
		\Pr{\theta = 0} 
		&= \pi \\
		\Pr{\theta > t \given \theta > 0} 
		&= e^{-\lambda t},
	\end{align*}
	где \(\pi \in [\,0, 1)\) и \(\lambda > 0\) "--- какие-то известные константы.
	
	Далее, зафиксируем какую-то константу \(\alpha > 0\) и построим для неё следующий класс:
	\[
		\mathcal{M}_{\alpha} 
		= \{\tau \colon \Pr{\tau < \theta} \leq \alpha\}.
	\]
	Другими словами, \(\mathcal{M}_{\alpha}\) "--- это класс тех моментов остановки \(\tau\) (относительно естественной фильтрации \(\mathbb{F}^{X}\)), для которых вероятность ложной тревоги не превосходит \(\alpha\). 
	
	Нужно найти такой момент остановки \(\tau_{\alpha}^{*} \in \mathcal{M}_{\alpha}\), для которого время срабатывания минимально:
	\[
		\E{\tau_{\alpha}^{*} - \theta \given \tau_{\alpha}^{*} \geq \theta\,} 
		= \inf_{\tau \in \mathcal{M}_{\alpha}} \E{\tau - \theta \given \tau \geq \theta\,}.
	\]
	
	Для решения этой \emph{условно-вариационной} задачи посмотрим на следующую байесовскую постановку задачи о разладке. Пусть
	\[
		\mathbf{A}(c) = \inf_{\tau \in \mathcal{M}} (\Pr{\tau < \theta} + c\E{(\tau - \theta)^{+}}),
	\]
	где \(\mathcal{M}\) "--- класс всех конечных моментов остановки, \(c > 0\) "--- некоторая константа, а \(\xi^{+} = \max(0, \xi)\). Тогда
	\begin{align*}
		\E{(\tau - \theta)^{+}} 
		&= \E{(\tau - \theta)\I_{\{\tau \geq \theta\}}}
		= \E{\tau - \theta \given \tau \geq \theta} \Pr{\tau \geq \theta}. 
	\end{align*}

	Момент \(\tau_{c}^{*}\) будем называть \emph{оптимальным}, если на нём достигается инфинум:
	\[
		\underbrace{
			\Pr{\tau_{c}^{*} < \theta}
		}_{\substack{\text{вероятность} \\ \text{ложного срабатывания}}}
		+ 
		\underbrace{
			c\E{(\tau_{c}^{*} - \theta)^{+}}
		}_{\substack{\text{(условное) среднее} \\ \text{время срабатывания}}}
		= \mathbf{A}(c).
	\]
\end{enumerate}

В трёх следующих вариантах "--- \(\mathbf{B}\), \(\mathbf{C}\) и \(\mathbf{D}\) "--- параметр \(\theta\) будем считать постоянным и принимающим значения в \([\,0, \infty]\).

\begin{enumerate}[label=(\textbf{\Alph*}),resume]
	\item Зафиксируем некоторое число \(T > 0\) и рассмотрим следующий класс:
	\[
		\mathcal{M}_{\tau} 
		= \{\tau \colon \E_{\infty}[\tau] \geq T\}.
	\]
	Этот класс содержит такие моменты остановки \(\tau\), для которых среднее время до ложной тревоги не меньше \(T\). 
	Далее, пусть
	\[
		\mathbf{B}(T) 
		= \inf_{\tau \in \mathcal{M}_{T}} \frac{1}{T}\int\limits_{0}^{\infty} \E_{\theta}[(\tau - \theta)^{+}] \diff \theta.
	\]
	
	В этом варинате момент остановки \(\tau_{T}^{*}\) будет оптимальным, если на нём достигается инфинум:
	\[
		\inf_{\tau \in \mathcal{M}_{T}} \frac{1}{T}\int\limits_{0}^{\infty} \E_{\theta}[(\tau_{T}^{*} - \theta)^{+}] \diff \theta
		= \mathbf{B}(T).
	\]
	
	\item Этот вариант ещё называют \emph{первой минимаксной задачей}. Пусть
	\[
		\mathbf{C}(T)
		= \inf_{\tau \in \mathcal{M}_{T}} \sup_{\theta \geq 0} \E_{\theta}[\tau - \theta\,|\,\tau \geq \theta].
	\]
	
	Будем говорить, что момент $\tau_{T}^{*} \in \mathcal{M}_{T}$ является
	оптимальным (если он существует) в~варианте~$\mathbf{C}$, если на нём достигается инфинум:
	\[
	\underbrace{
		\sup_{\theta \geq 0} 
		\underbrace{
			\E_{\theta}[\tau_{T}^{*} - \theta\,|\,\tau_{T}^{*} \geq \theta]
		}_{\substack{\text{(условное) среднее} \\ \text{время срабатывания}}}
	}_{\text{наихудшее по всем } \theta}
	= \mathbf{C}(T).
	\]
\end{enumerate}

Для введения варианта \(\mathbf{D}\) нужно ввести понятие вещественного супремума.
\begin{definition}
	Будем говорить, что \(a\) является \emph{вещественным супремумом} функции \(f\), если \(f(x) \leq a\) почти везде, то есть мера множества \(\{x \colon f(x) > a\}\) равна нулю. 
	
	\noindent\textbf{Обозначение}: \(a = \esssup f(x)\).
\end{definition}

\begin{enumerate}[label=(\textbf{\Alph*}),resume]
	\item Этот вариант называют \emph{второй минимаксной задачей}. Пусть
	\[
		\mathbf{D}(T)
		=\inf_{\tau \in \mathcal{M}_{T}} \sup_{\theta \geq 0} \esssup_{\omega}
		\E_{\theta}[(\tau - \theta)^{+}\,|\,\F_{\theta}^{X}](\omega),
	\]
	где \(\F_{\theta}^{X} = \sigma(X_{t} ,t \leq \theta)\).
	Момент \(\tau_{T}^{*} \in \mathcal{M}_{T}\) называется оптимальным (если он
	существует) в~варианте~$\mathbf{D}$, если
	\[
		\underbrace{
			\sup_{\theta \geq 0}
			\underbrace{
				\esssup_{\omega}
				\underbrace{
					\E_{\theta}[(\tau - \theta)^{+}\,|\,\F_{\theta}^{X}](\omega)
				}_{\text{ среднее время срабатывания}}
			}_{\text{наихудшее по всем траекториям}}
		}_{\text{наихудшее по всем } \theta}
		= \mathbf{D}(T).
	\]
\end{enumerate}

На самом деле есть ещё один вариант, который обычно называют вариантом \(\mathbf{E}\). 
Он кардинально отличается от остальных вариантов тем, что в них мы предполагаем, что после объявления сигнала тревоги наблюдение останавливается. 
На самом же деле в~реальной практике дело обстоит так, что после объявления тревоги системы обнаружения продолжают свое функционирование, ожидая, скажем, появления следующей разладки. 
Этот же вариант задачи о разладке носит \emph{многократный характер}.
При этом целесообразно считать, что сам процесс наблюдения начался <<давно>>, он мог прерываться ложными тревогами и <<разладка>> появляется на <<фоне установившегося режима наблюдения>>.