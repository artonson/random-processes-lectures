\section{Стохастические модели с дискретным временем}
Приступим к изучению \emph{временных рядов}, то есть случайных процессов с 
множеством индекосв \(T = \Z_{+}\). Пусть единица времени измерения есть сутки, 
а \(S = (S_{n})_{n \geq 0}\)~--- это какой-то финансовый индекс (например, 
раночная цена акции или же обменный курс валют). Практика показывает, что 
\(S_{n}\) ведёт себя весьма нерегулярно. Это привело Луи Башелье к идее 
использования аппарата теории вероятностей для изучения эмпирических феноменов, 
характеризующихся статистической неопределённостью, но при этом обладающих 
статистической устойчивостью частот.

Как обычно, будем считать, что все наблюдения проводятся на некотором 
вероятностном прострнастве \((\Omega, \F, \Pr)\). Однако время и динамика 
являются неотъемлимыми частями финансовой теории, в связи с чем целесообразно 
специфицировать вероятностное пространство, добавив 
\hyperref[def-filtration]{фильтрацию} \((\F_{n})_{n \geq 0}\). Интуитивно можно 
понимать, что \(\F_{n}\) есть доступная наблюдателю ``информация'' о рынке 
вплоть до момента времени \(n\). Получаемая четвёрка \((\Omega, \F, (\F_{n})_{n 
\geq 0}, \Pr)\) называется \emph{фильтрованным вероятностным пространством} или 
же \emph{стохастическим базисом}. Впрочем, во многих случаях целесообразно 
вводить не одну вероятностную меру, а целое семейство \(\mathcal{P} = \{\Pr\}\) 
(это связано с тем, что бывает трудно выбрать какую-то конкретную меру 
\(\Pr\)). Полученный набор объектов \((\Omega, \F, (\F_{n})_{n \geq 0}, 
\mathcal{P})\) можно назвать \emph{фильтрованным статистическим экспериментом}.
\label{price-time-series}
Разумно предположить, что \(S_{n}\) полностью задаётся тем, что произошло до 
момента времени \(n\) включительно. Формально говоря, \(S_{n}\) является 
\(\F_{n}\)-измеримой случайной величиной.\footnote{То есть для любого \(B \in 
\B(\R)\) \(S_{n}^{-1}(B) \in \F_{n}\).}

Теперь предположим, что для всех \(n \geq 0\) \(S_{n} > 0\). Как можно 
охарактеризовать \(S_{n}\)? Есть два способа.

Первый метод похож на метод сложных процентов (то есть проценты выплачиваются 
непрерывно). В нём \(S_{n} = S_{0}e^{H_{n}}\), где \(H_{n} = h_{0} + h_{1} + 
\ldots + h_{n}\), \(h_{0} = 0\), а \(h_{n}\) есть \(\F_{n}\)-измеримая 
случайная величина. Несложно 
понять, что
\[
	H_{n} = \ln\frac{S_{n}}{S_{0}}, \quad h_{n} = \ln\frac{S_{n}}{S_{n - 1}} = 
	\ln\left(1 + \frac{\Delta S_{n}}{S_{n - 1}}\right), \text{ где } \Delta 
	S_{n} = S_{n} - S_{n - 1}.
\]

Теперь введём следующие обозначения:
\[
	\hat{h}_{n} = \frac{\Delta S_{n}}{S_{n - 1}}, \quad \hat{H}_{n} = \sum_{k = 
	1}^{n} \hat{h}_{k}.
\]

Тогда 
\[
	S_{n} = S_{0} \prod_{k = 1}^{n}e^{h_{k}} = S_{0} \prod_{k = 1}^{n} (1 + 
	\hat{h}_{k}) = S_{0} \prod_{k = 1}^{n} (1 + \Delta\hat{H}_{k}) = 
	S_{0}e^{\hat{H}_{n}} \prod_{k = 1}^{n} (1 + \Delta\hat{H}_{k}) 
	e^{-\Delta\hat{H}_{k}}.
\]

Полученное разложение задаёт второй метод, который похож на метод простых 
процентов.

Теперь введём обозначение
\[
	\mathcal{E}(\hat{H})_{n} \equiv e^{\hat{H}_{n}} \prod_{k = 1}^{n} (1 + 
	\Delta\hat{H}_{k}) e^{-\Delta\hat{H}_{k}}.
\]

Получаемая случайная последовательность \(\mathcal{E}(\hat{H}) \equiv 
(\mathcal{E}(\hat{H})_{n})_{n \geq 0}\) называется \emph{экспонентой Долеан} 
или \emph{стохастической экспонентой}, порождённой случайной 
последовательностью \(\hat{H} = (\hat{H}_{n})_{n \geq 0}\). Несложно понять, 
что в первом методе используется обычная экспонента: \(S_{n} = 
S_{0}e^{H_{n}}\), а во втором~--- стохастическая: \(S_{n} = S_{0} 
\mathcal{E}(\hat{H})_{n}\).

Теперь покажем взаимосвязь между \(H_{n}\) и \(\hat{H}_{n}\). Заметим, что
\[
	\hat{H}_{n} = \sum_{k = 1}^{n} \hat{h}_{k} = \sum_{k = 1}^{n}(e^{h_{k}} - 
	1) = \sum_{k = 1}^{n}(e^{\Delta H_{k}} - 1).
\]

Тогда
\[
	\hat{H}_{n} = H_{n} + (\hat{H}_{n} - H_{n}) = H_{n} + \sum_{k = 
	1}^{n}(e^{\Delta H_{k}} - 1 - \Delta H_{k}).
\]

Также понятно, что
\[
	H_{n} = \sum_{k = 1}^{n} h_{k} = \sum_{k = 1}^{n} \ln(1 + \hat{h}_{k}) = 
	\sum_{k = 1}^{n} \ln(1 + \Delta \hat{H}_{k}).
\]

Теперь покажем, что для \(\mathcal{E}(\hat{H})\) верно следующее разностное 
уравнение, от которого и пошло название:
\[
	\Delta\mathcal{E}(\hat{H})_{n} = \mathcal{E}(\hat{H})_{n - 
	1}\Delta\hat{H}_{n}, \quad \mathcal{E}(\hat{H})_{0} = 1.
\]

Действительно,
\begin{align*}
	\Delta\mathcal{E}(\hat{H})_{n} &= \mathcal{E}(\hat{H})_{n} - 
	\mathcal{E}(\hat{H})_{n - 1} = e^{\hat{H}_{n}} \prod_{k = 1}^{n} (1 + 
	\Delta\hat{H}_{k}) e^{-\Delta\hat{H}_{k}} - e^{\hat{H}_{n - 1}} \prod_{k = 
	1}^{n - 1} (1 + \Delta\hat{H}_{k}) e^{-\Delta\hat{H}_{k}} = \\
	&= (e^{\Delta\hat{H}_{n}}(1 + \Delta\hat{H}_{n})e^{-\Delta\hat{H}_{n}} - 
	1)e^{\hat{H}_{n - 1}} \prod_{k = 1}^{n - 1} (1 + \Delta\hat{H}_{k}) 
	e^{-\Delta\hat{H}_{k}} = \\
	&= \Delta\hat{H}_{n}e^{\hat{H}_{n - 1}} \prod_{k = 1}^{n - 1} (1 + 
	\Delta\hat{H}_{k}) e^{-\Delta\hat{H}_{k}} = \mathcal{E}(\hat{H})_{n - 
	1}\Delta\hat{H}_{n}.
\end{align*}
\begin{remark}
	Несложно заметить, что при малых \(h_{k}\) \(\hat{h}_{k} \approx h_{k}\), 
	причём
	\[
		\hat{h}_{k} - h_{k} = \frac{1}{2}h_{k}^{2} + \frac{1}{6}h_{k}^{3} + 
		\ldots
	\]
\end{remark}

Впрочем, на данный момент ограничимся описанием распределения \(S = (S_{n})_{n 
\geq 0}\) и \(H = (H_{n})_{n \geq 0}\). С точки зрения классической теории 
вероятностей и продвинутой ``статистики нормального распределения'' было бы 
хорошо, если бы последовательность \(H = (H_{n})_{n \geq 0}\) была гауссовской. 
Если \(H_{n} = h_{1} + \ldots + h_{n}\), \(n \geq 1\), то распределение 
\(H_{n}\) полностью бы задавалось распределением последовательности \(h = 
(h_{n})_{n \geq 1}\). Однако она полностью задаётся двумя параметрами: 
матожиданием \(\mu_{n} = \E{h_{n}}\) и ковариациями \(\cov(h_{m}, h_{n})\).

Предположение о нормальности существенно упрощает решение многих задач, 
связанных с свойствами распределений. Например, 
\hyperref[normal-correlation-theorem]{теорема о нормальной корреляции} даёт 
формулу для вычисления условного математического ожидания \(\tilde{h}_{n 
+ 1} = \E{h_{n + 1} \given h_{1}, \ldots, h_{n}}\):
\[
	\tilde{h}_{n + 1} = \mu_{n + 1} + \sum_{k = 1}^{n}a_{k}(h_{k} - \mu_{k}),
\] 
где \(a_{k}\)~--- коэффициенты, задаваемые матрицей ковариаций. Оказывается, 
что \(\tilde{h}_{n + 1}\) является \emph{оптимальной} в среднеквадратичном 
смысле оценкой \(h_{n + 1}\) по \(h_{1}, \ldots, h_{n}\), то есть матожидание 
квадрата отклонения минимально.

Что будет в случае, если все \(h_{k}\) независимы? В таком случае матрица 
ковариаций диагональна и
\[
	\tilde{h}_{n + 1} = \E{h_{n + 1}} + \sum_{k = 1}^{n}\frac{\cov(h_{n + 1}, 
	h_{k})}{\D{h_{k}}}(h_{k} - \E{h_{k}}).
\]

Формула для ошибки оценивания будет иметь вид (проверьте!):
\[
	\Delta_{n + 1} = \E{(\tilde{h}_{n + 1} - h_{n + 1})^{2}} = \D{h_{n + 1}} - 
	\sum_{k = 1}^{n}\frac{\cov^{2}(h_{n + 1}, h_{i})}{\D{h_{i}}}
\]

Теперь вспомним один факт, связанный с нормальным распределением: с 
вероятностью около 90\% значение случайной величины \(\xi \sim \mathcal{N}(\mu, 
\sigma^{2})\) будет лежать в интервале \([\mu - 1,65\sigma, \mu + 
1,65\sigma]\). Тогда, пользуясь тем, что \(h_{n + 1} - \tilde{h}_{n + 1} 
\sim \mathcal{N}(0, \Delta_{n + 1})\), получаем, что
\[
	\Pr{|h_{n + 1} - \tilde{h}_{n + 1}| \leq 1,65\sqrt{\Delta_{n + 1}}} \approx 
	0,90.
\]

Отсюда получаем, что в 90\% случаев прогнозируемое значение \(\tilde{S}_{n + 
1}\) величины рыночной цены (по наблюдениям \(h_{1}, \ldots, h_{n}\)) лежит в 
интервале 
\[
	[S_{n}e^{\tilde{h}_{n + 1} - 1,65\sqrt{\Delta_{n + 1}}}, 
	S_{n}e^{\tilde{h}_{n + 1} + 1,65\sqrt{\Delta_{n + 1}}}].
\]

Впрочем, к гипотезе нормальности нужно относиться с осторожностью. Практика 
показывает, что
\begin{itemize}
	\item Число выборочных значений, не попадающих в интервал 
	\([\overline{h}_{n} - k\hat{\sigma}_{n}, \overline{h}_{n} + 
	k\hat{\sigma}_{n}]\), \(k = 1, 2, 3\), где 
	\begin{gather*}
		\overline{h}_{n} = \frac{1}{n}\sum_{k = 1}^{n} h_{k} \text{~--- 
		выборочное среднее,} \\
		\hat{\sigma}_{n} = \frac{1}{n - 1}\sum_{k = 1}^{n} (h_{k} - 
		\overline{h}_{n})^{2}\text{~--- выборочное стандартное отклонение,} 
	\end{gather*}
	значительно больше, чем это должно быть при гипотезе нормальности. Это 
	означает, что ``хвосты'' эмпирических распределений убывают значительно 
	медленнее, чем у гауссовского распределения (тяжёлые хвосты).
	\item Может оказаться так, что \emph{эксцесс}, или коэффициент вытянутости:
	\[
		\hat{k}_{n} = \frac{\hat{m}_{4}}{\hat{m}_{2}^{2}} - 3,
	\]
	где \(\hat{m}_{k}\) есть выборочный \(k\)-й момент, получается 
	положительным (хотя для нормального распределения он должен быть нулевым). 
	Это означает сильную вытянутость пика плотности распределения в окрестности 
	центральных значений.
\end{itemize}

Пожалуй, самым сильным предположением (относительно структуры распределения 
величин \(h = (h_{n})\)) является, помимо нормальности, предположение 
\emph{независимости и одинаковой распределённости} этих величин. В таком случае 
анализ цен легко проводится с помощью обычных методов теории вероятностей. 
Однако при таком предположении сразу же рушится надежда на то, что прошлые 
данные хоть как-то влияют на будущие.

Предположим, что в модели
\[
	S_{n} = S_{0}e^{H_{n}}, \quad H_{n} = h_{1} + \ldots + h_{n},
\]
случайные величины \(h_{n}\) имеют конечные абсолютные первые моменты: 
\(\E{|h_{n}|} < +\infty\). 

\emph{Разложение Дуба}, о котором пойдёт речь дальше, предполагает изучение 
последовательности \(H = (H_{n})\) в зависимости от свойств фильтрации 
\((F_{n})_{n \geq 0}\), то есть потока информаций, доступных наблюдателю. 
Положим \(\F_{0} = \{\emptyset, \Omega\}\).

Так как \(\E{|h_{n}|} < +\infty\), \(n \geq 1\), то определены условные 
математические ожидания \(\E{h_{n} \given \F_{n - 1}}\). Тогда
\[
	H_{n} = \sum_{k = 1}^{n} h_{k} = \sum_{k = 1}^{n} \E{h_{k} \given \F_{k - 
	1}} + \sum_{k = 1}^{n} (h_{k} - \E{h_{k} \given \F_{k - 1}}).
\]

Если ввести обозначения
\begin{align*}
	A_{n} &= \sum_{k = 1}^{n} \E{h_{k} \given \F_{k - 1}}, \\
	M_{n} &= \sum_{k = 1}^{n} (h_{k} - \E{h_{k} \given \F_{k - 1}}),
\end{align*}
то для \(H = (H_{n})_{n \geq 0}, H_{0} = 0\) справедливо разложение Дуба
\[
	H_{n} = A_{n} + M_{n}, \quad n \geq 0,
\]
где
\begin{itemize}
	\item \(A = (A_{n})_{n \geq 0}, A_{0} = 0\) является \emph{предсказуемой} 
	случайной последовательностью. Другими словами, для любого \(n \geq 1\) 
	\(A_{n}\) есть \(\F_{n - 1}\)-измеримой случайная величина.
	\item \(M = (M_{n})_{n \geq 0}, M_{0} = 0\) является \emph{мартингалом}, то 
	есть для любого \(n \geq 1\) \(\E{M_{n} \given \F_{n - 1}} = M_{n - 1}\), 
	причём \(M_{n}\) есть \(\F_{n}\)-измеримые величины и \(\E{|M_{n}|} < 
	\infty\).
\end{itemize}
\begin{remark}
	Предположим, что наряду с фильтрацией \((\F_{n})\) задана 
	\emph{подфильтрация} \((\mathcal{G}_{n})\), где \(\mathcal{G}_{n} \subseteq 
	\F_{n}\) и \(\mathcal{G}_{n} \subseteq \mathcal{G}_{n + 1}\). Аналогичным 
	образом можно написать разложение \(H = (H_{n})\) относительно потока 
	\((\mathcal{G}_{n})\):
	\[
		H_{n} = \sum_{k = 1}^{n} \E{h_{k} \given \mathcal{G}_{k - 1}} + \sum_{k 
		= 1}^{n} (h_{k} - \E{h_{k} \given \mathcal{G}_{k - 1}}).
	\]
	
	Последовательность \(A = (A_{n})\) с элементами
	\[
		A_{n} = \sum_{k = 1}^{n} \E{h_{k} \given \mathcal{G}_{k - 1}}
	\]
	будет \((\mathcal{G}_{n})\)-предсказуемой (то есть \(A_{n}\) являются
	\(\mathcal{G}_{n - 1}\)-измеримыми). Однако \(M = (M_{n})\), задаваемая по 
	правилу
	\[
		M_{n} = \sum_{k = 1}^{n} (h_{k} - \E{h_{k} \given \mathcal{G}_{k - 1}}).
	\]
	не является мартингалом относительно фильтрации \((\mathcal{G}_{n})\), так 
	как \(h_{k}\) являются измеримыми относительно сигма-алгебры \(\F_{k}\), 
	что не означает \(\mathcal{G}_{k}\)-измеримость.
\end{remark}

У этого разложения есть хорошее свойство: оно единственно. Действительно, пусть 
\(H_{n} = A'_{n} + M'_{n}\)~--- другое разложение с \((\F_{n})\)-предсказуемой 
последовательностью \(A' = (A'_{n}), A'_{0} = 0\) и мартингалом \(M' = (M'_{n}, 
\F_{n})\). Заметим, что
\[
	A'_{n + 1} - A'_{n} = H_{n + 1} - H_{n} - (M'_{n + 1} - M'_{n}) = (A_{n + 
	1} - A_{n}) + (M_{n + 1} - M_{n}) - (M'_{n + 1} - M'_{n}).
\]

Теперь возьмём от обеих частей условное матожидание \(\E{\cdot \given 
\F_{n}}\). Тогда мы получим, что
\[
	\E{A'_{n + 1} - A'_{n} \given \F_{n}} = \E{A_{n + 1} - A_{n} \given \F_{n}}.
\]
Так как \(A_{n + 1}\) и \(A_{n}\) являются \(\F_{n}\)-измеримыми, то \(A'_{n + 
1} - A'_{n} = A_{n + 1} - A_{n}\). Пользуясь тем, что \(A_{0} = A'_{0} = 0\), 
получаем желаемое.

Стоит заметить, что если в рассматриваемой модели \(\E{h_{k} \given \F_{k - 1}} 
= 0\) для всех \(k \geq 1\), то сама последовательность \(H = (H_{n})\) будет 
являться мартингалом.

Разложение Дуба не такое тривиальное, каким оно может показаться на первый 
взгляд.
\begin{example}
	Рассмотрим последовательность iid случайных величин \(\{\xi_{n}\}_{n \in 
	\N}\) таких, что
	\[
		\Pr{\xi_{n} = 1} = \Pr{\xi_{n}= -1} = \frac{1}{2}.
	\]
	
	Далее, пусть \(X_{n} = \xi_{1} + \ldots + \xi_{n}\). Другими словами, пусть 
	есть простейшее случайное блуждание. Разложение Дуба для \(H_{n} = 
	|X_{n}|\), \(n \geq 0\), \(|X_{0}| = 0\) будет устроено следующим образом:
	\[
		h_{n} = \Delta H_{n} = |X_{n}| - |X_{n - 1}| = |X_{n - 1} + \xi_{n}| - 
		|X_{n - 1}|.
	\]
	Далее, пользуясь свойствами условного математического ожидания и 
	независимостью \(\xi_{n}\) и \(X_{n - 1}\), получаем, что
	\begin{align*}
		\Delta M_{n} &= h_{n} - \E{h_{n} \given \F_{n - 1}} = |X_{n - 1} + 
		\xi_{n}| - |X_{n - 1}| - \E{|X_{n - 1} + \xi_{n}| - |X_{n - 1}| \given 
		\F_{n - 1}} = \\
		&= |X_{n - 1} + \xi_{n}| - \E{|X_{n - 1} + \xi_{n}| \given X_{n - 1}} = 
		(\sgn{X_{n - 1}})\xi_{n}.
	\end{align*}
	
	Отсюда получаем, что мартингал в разложении Дуба имеет вид
	\[
		M_{n} = \sum_{k = 1}^{n} (\sgn{X_{k - 1}})\xi_{k} = \sum_{k = 1}^{n} 
		(\sgn{X_{k - 1}})\Delta X_{k}.
	\]
	
	Далее,
	\[
		\E{h_{n} \given \F_{n - 1}} = \E{|X_{n - 1} + \xi_{n}| - |X_{n - 1}| 
		\given \F_{n - 1}} = \E{|X_{n - 1} + \xi_{n}| \given \F_{n - 1}} - 
		|X_{n - 1}|.
	\]
	
	Несложно заметить, что если \(X_{n - 1} \neq 0\), то это условное 
	матожидание обращается в ноль. Если же \(X_{n -1} = 0\), то оно равно 
	единице. Тогда 
	\[
		\sum_{k = 1}^{n} \E{h_{k} \given \F_{k - 1}} = \#\{1 \leq k \leq n: 
		X_{k - 1} = 0\}.
	\]
	
	Пусть \(L_{n}(0) = \#\{0 \leq k \leq n - 1: X_{k} = 0\}\)~--- число нулей 
	последовательности \((X_{k})_{0 \leq k \leq n - 1}\). Тогда по разложению 
	Дуба
	\[
		|X_{n}| = \sum_{k = 1}^{n} (\sgn{X_{k - 1}})\Delta X_{k} + L_{n}(0).
	\]
	
	Теперь воспользуемся тем, что у мартингала матожидание постоянно и равно 
	нулю. Тогда
	\[
		\E{L_{n}(0)} = \E{|X_{n}|}.
	\]
	
	Согласно ЦПТ \(X_{n}/\sqrt{n} \sim \mathcal{N}(0, 1)\). Следовательно,
	\[
		\E{|X_{n}|} \sim \sqrt{\frac{2n}{\pi}} \implies \E{L_{n}(0)} \sim 
		\sqrt{\frac{2n}{\pi}}.
	\]
	
	Полученная формула~--- это известный результат о среднем числе нулей в 
	симметричном случайном блуждании Бернулли.
\end{example}
\section{Гауссовские и условно-гауссовские модели}
На абсолютно \emph{эффективных рынках} наилучшим прогнозом будущего уровня цены 
финансового актива является текущая цена этого актива. Поэтому понятие 
мартингала стало одним из основных при исследовании динамики эволюции цен как 
стохастических последовательностей или процессов с определёнными свойствами их 
распределений. Однако при проведении конкретных расчетов одного лишь знания 
``мартингальности распределений'' слишком мало~--- нужна более ``тонкая'' 
структура этих распределений, что приводит к необходимости детального 
рассмотрения самых разнообразных вероятностно-статистических моделей с целью 
выявления тех из них, свойства распределений которых лучше всего согласуются со 
свойствами эмпирических распределений, построенных по статистическим данным.

Предположение гауссовости распределений величин \(h_{1}, \ldots, h_{n}\), 
конечно, выглядит привлекательным и с точки зрения теоретического анализа, и с 
точки зрения ``статистики нормального распределения''. Но это предположение не 
всегда соответсвтует истинной картине поведения цен. Но какую альтернативу 
можно привести? Для этого вспомним разложение Дуба. Как известно, оно 
определяется с привлечением условных матожиданий вида \(\E{h_{n} \given \F_{n - 
1}}\). Тогда было бы разумно предположить, что не безусловные, а 
\emph{условные} распределения являются гауссовскими:
\[
	\Law(h_{n} \mid \F_{n - 1}) = \mathcal{N}(\mu_{n}, \sigma_{n}^{2})
\]
с некоторыми \(\F_{n - 1}\)-измеримыми величинами \(\mu_{n} = \mu_{n}(\omega)\) 
и \(\sigma_{n}^{2} = \sigma_{n}^{2}(\omega)\).

Оказывается, что \(\E{h_{n} \given \F_{n - 1}} = \mu_{n}\) и \(\D{h_{n} \given 
\F_{n - 1}} = \sigma_{n}^{2}\) (это следует из регулярности условного 
распределения~--- за доказательством обращайтесь к первому тому Ширяева). Тем 
самым видем смысл этих параметров~--- условное среднее и условная дисперсия 
распределения \(\Law(h_{n} \mid \F_{n - 1})\).

Само же распределение \(\Law(h_{n})\) является \emph{взвесью} условных 
гауссовских распределений \(\Law(h_{n} \mid \F_{n - 1})\) с усреднением по 
распределению величин \(\mu_{n}\) и \(\sigma_{n}^{2}\). % Почему?

Обычно наряду с \(h = (h_{n})\) вводится ``стандартная'' условно-гауссовская 
последовательность \(\epsilon = (\epsilon_{n})_{n \geq 1}\) 
\(\F_{n}\)-измеримых случайных величин таких, что
\[
	\Law(\epsilon_{n} \mid \F_{n - 1}) = \mathcal{N}(0, 1), \text{ где } \F_{0} 
	= \{\emptyset, \Omega\}.
\]

Оказывается, что это будет последовательность \emph{независимых} случайных 
величин с стандартным нормальным распределением, так как
\[
	\Law(\epsilon_{n} \mid \epsilon_{1}, \ldots, \epsilon_{n - 1}) = 
	\mathcal{N}(0, 1).
\]

Если \(\omega_{n}^{2} \neq 0\) поточечно для всех \(n \geq 1\), то величины 
\(\epsilon_{n}\), задаваемые по правилу \(\epsilon_{n} \equiv (h_{n} - 
\mu_{n})/\omega_{n}\), будут задавать стандартную гауссовскую 
последовательность. Тогда можно считать, что рассматриваемые 
условно-гауссовские последовательности представимы в виде
\[
	h_{n} = \mu_{n} + \sigma_{n}\epsilon_{n},
\]
где \(\epsilon = (\epsilon_{n})\)~--- последовательность независимых 
\(\F_{n}\)-измеримых случайных величин с распределением \(\mathcal{N}(0, 1)\).

Понятно, что более подробное изучение свойств последовательности \(h = 
(h_{n})\) зависит от структуры \(\mu_{n}\) и \(\sigma_{n}^{2}\). Именно это и 
делается в представляемых ниже моделях.

В теории временных рядов есть целый арсенал разнообразных \emph{линейных} 
моделей, среди которых в первую очередь нужно назвать следующие:
\begin{itemize}
	\item Модель \emph{скользящего среднего} порядка \(q\) \(\mathrm{MA}(q)\),
	\item Модель \emph{авторегрессии} порядка \(p\) \(\mathrm{AR}(p)\),
	\item  Модель \emph{авторегрессии и скользящего среднего} порядка \((p, 
	q)\) \(\mathrm{ARMA}(p, q)\).
\end{itemize}
Эти модели широко исследуются в теории временных рядов, особенно в 
предположении \emph{стационарности}. Вообще, для чего вводятся линейные модели? 
Они весьма просты, но при этом ими можно неплохо приближать весьма широкий 
класс стационарных последовательностей.

Вот только не все временные ``эконометрические'' ряды являются стационарными. 
Анализ показывает, что часто в данных вырисовываются три составляющие:
\begin{itemize}
	\item Медленно меняющийся (например, ``инфляционный'') тренд \(x\),
	\item Периодические или же апериодические циклы \(y\),
	\item Нерегулярная, флуктуирующая (``стохастическая'' или ``хаотическая'') 
	компонента \(z\).
\end{itemize}
В наблюдаемые данные \(h\) они могут входить весьма разнообразными способами. 
Образно это можно представить так: \(h = x * y * z\), где вместо \(*\) могут 
выступать сложение \(+\), умножение \(\times\) и так далее.

Ниже мы рассмотрим некоторые \emph{линейные} (а затем и нелинейные) модели, 
преследуя цель дать представление об их структуре, особенностях, свойствах, 
применяемых в анализе данных. 

Не стоит забывать, что конечной целью анализа статистических данных является 
\emph{прогнозирование} дальнейшнего поведения. Качество этого прогнозирования 
зависит от удачного выбора модели, точности оценивания определяющих её 
параметров и качества экстраполяционного оценивания.

Во всех рассматриваемых далее моделях будем считать, что задана некоторая 
``базисная'' последовательность \(\epsilon = (\epsilon_{n})\), которую в теории 
временных рядов обычно считают \emph{белым шумом} и идентифицируют с источником 
случайности, определяющим стохастический характер исследуемых 
вероятностно-статистических объектов. При этом (в ``\(L^{2}\)-теории'') 
говорят, что последовательность является \emph{белым шумом в широком смысле}. 
\begin{definition}
	Последовательность \(\epsilon = (\epsilon_{n})\) называется белым шумом в 
	широком смысле, если 
	\[
		\forall m, n \in \Z, m \neq n : \E{\epsilon_{n}} = 0, 
		\E{\epsilon_{n}^{2}} 
		< \infty, \E{\epsilon_{m}\epsilon_{n}} = 0.
	\]
	
	Другими словами, белый шум в широком смысле~--- это квадратично 
	интегрируемая последовательность некоррелированных случайных величин с 
	нулевыми средними.
\end{definition}
Ещё вводят белый шум \emph{в узком смысле}, который обычно называют просто 
\emph{белым (гауссовским) шумом}. 
\begin{definition}
	Белый шум~--- это гауссовская последовательность, являющаяся белым шумом в 
	широком смысле.
\end{definition}
Другими словами, это последовательность независимых случайных величин с 
нормальными распределениями \(\mathcal{N}(0, \sigma_{n}^{2})\). Далее будем 
считать, что \(\sigma_{n}^{2} \equiv 1\). В таком случае обычно говорят, что 
\(\epsilon\) есть стандартная гауссовская последовательность.

\subsection{Модель скользящего среднего \(\mathrm{MA}(q)\)}

В модели скользящего среднего порядка \(q\), описывающей эволюцию 
последовательности \(h = (h_{n})\), предполагается следующие способ 
формирования значений \(h_{n}\) по белому шуму в широком смысле \(\epsilon = 
(\epsilon_{n})\):
\[
	h_{n} = (\mu + b_{1}\epsilon_{n - 1} + \ldots + b_{q}\epsilon_{n - q}) + 
	b_{0}\epsilon_{n},
\]
где параметр \(q\) определяет порядок зависомости от ``прошлого'', а 
\(\epsilon_{n}\) играет роль величин, ``обновляющих'' информацию, содержащуюся 
в \(\F_{n - 1} = \sigma(\epsilon_{n - 1}, \epsilon_{n - 2}, \ldots)\).

Далее, для компактности вводят \emph{лаговый оператор} \(L\), действующий по 
правилу \(Lx_{n} = x_{n - 1}\). Так как \(L(Lx_{n}) = x_{n - 2}\), то разумно 
ввести обозначение
\[
	L^{2}x_{n} \equiv L(Lx_{n}) = x_{n - 2},
\]
и, в общем случае, \(L^{k}x_{n} = x_{n - k}\).

Отметим следующие свойства лагового оператора:
\begin{align*}
	L(cx_{n}) &= cLx_{n}, \\
	L(x_{n} + y_{n}) &= Lx_{n} + Ly_{n}, \\
	(c_{1}L + c_{2}L^{2})x_{n} &= c_{1}Lx_{n} + c_{2}L^{2}x_{n} = c_{1}x_{n - 
	1} + c_{2}x_{n - 2}, \\
	(1 - \lambda_{1}L)(1 - \lambda_{2}L)x_{n} &= x_{n} - (\lambda_{1} + 
	\lambda_{2})x_{n - 1} + (\lambda_{1}\lambda_{2})x_{n - 2}.
\end{align*}

Пользуясь этими свойствами, модели \(\mathrm{MA}(q)\) можно придать следующую 
форму: \(h_{n} = \mu + \beta(L)\epsilon_{n}\), где \(\beta(L) = b_{0} + b_{1}L 
+ \ldots + b_{q}L^{q}\).

Теперь положим \(q = 1\). В таком случае
\[
	h_{n} = \mu + b_{0}\epsilon_{n} + b_{1}\epsilon_{n - 1}.
\]

Несложно проверить, что
\begin{align*}
	\E{h_{n}} = \mu,& \D{h_{n}} = b_{0}^{2} + b_{1}^{2} \\
	\cov(h_{n + 1}, h_{n}) = b_{0}b_{1}, & \cov(h_{n + k}, h_{n}) = 0, k > 1.
\end{align*}

Это означает, что \(h = (h_{n})\)~--- это последовательность с коррелированными 
соседними значениями (\(h_{n}\) и \(h_{n + 1}\)), причём корреляция значений 
\(h_{n + k}\) и \(h_{n}\) при \(k > 1\) равна нулю.

Из соотношений сверху следует, что у элементов последовательности \(h = 
(h_{n})\) матожидание, дисперсия и ковариация не зависят от \(n\) (впрочем, это 
определяется предположением стандартности последовательности \(\epsilon\) и 
тем, что \(b_{k}\) не зависят от \(n\)). Отсюда следует, что последовательность 
\(h = (h_{n})\) является стационарной в широком смысле (просто по определению). 
Если же добавить то, что \(\epsilon\) является гауссовской, то и \(h\) тоже 
будет гауссовской. Это означает, что все её параметры полностью задаются 
средним, дисперсией и ковариацией. Но тогда \(h\) будет стационарной и в узком 
смысле, так как для произвольных \(n\), \(k\) и \(i_{1}, \ldots, i_{n}\)
\[
	\Law(h_{i_{1}}, \ldots, h_{i_{n}}) = \Law(h_{i_{1} + k}, \ldots, h_{i_{n} + 
	k})
\]

Теперь покажем одно интересное свойство модели \(\mathrm{MA}(1)\). Пусть 
\((h_{1}, \ldots, h_{n})\)~--- некоторая реализация, полученная в результате 
наблюдений величин \(h_{k}\) в моменты времени \(k = 1, \ldots, n\). Далее, 
пусть \(\overline{h}_{n} = (\sum_{k = 1}^{n} h_{k})/n\)~--- это временное 
среднее. Со статистической точки зрения обращение к ``статистике'' 
\(\overline{h}_{n}\) представляет тот интерес, что \(\overline{h}_{n}\) 
является естественным кандидатом для оценивания среднего \(\mu\). 

Оказывается, что для стационарной в широком смысле последовательности \(h_{n}\) 
есть хороший критерий эргодичности, который похож на 
\hyperref[slutsky-criterion]{условие Слуцкого} (на самом деле это оно и есть):
\begin{theorem}
	Пусть \(h = (h_{n})\)~--- стационарная в широком смысле последовательность, 
	а \(R(k) = \cov(h_{n + k}, h_{n})\). Тогда
	\[
		\lim\limits_{n \to \infty} \E{(\overline{h}_{n} - \mu)^{2}} = 0 \iff 
		\lim\limits_{n \to \infty} \frac{1}{n}\sum_{k = 1}^{n} R(k) = 0.
	\]
\end{theorem}
\begin{proof}
	Без ограничения общности скажем, что \(\mu = \E{h_{n}} = 0\).
	
	Пусть \(\E{(\overline{h}_{n} - \mu)^{2}} \to 0\). Тогда по неравенству 
	Коши-Буняковского:
	\[
		\left|\frac{1}{n}\sum_{k = 1}^{n} R(k)\right|^{2} = 
		\left|\E{\frac{h_{0}}{n}\sum_{k = 1}^{n}h_{k}}\right|^{2} \leq 
		\E{h_{0}^{2}}\E{\left|\frac{1}{n}\sum_{k = 1}^{n}h_{k}\right|^{2}} 
		\xrightarrow[n \to \infty]{} 0.
	\]
	
	Теперь докажем в другую сторону. Заметим, что
	\[
		\E{\left|\frac{1}{n}\sum_{k = 1}^{n}h_{k}\right|^{2}} = 
		\frac{1}{n^{2}}\E{\sum_{k = 1}^{n} h_{k}^{2} + 2\sum_{i = 1}^{n}\sum_{j 
		= 1}^{i -1} h_{i}h_{j}} = \frac{2}{n^{2}}\sum_{i = 1}^{n}\sum_{j = 
		0}^{i - 1} R(j) - \frac{1}{n}R(0)
	\]
	
	Зафиксируем произвольное \(\delta > 0\) и найдём \(n(\delta)\) такое, что 
	для любого \(l \geq n(\delta)\) выполнено, что
	\[
		\left|\frac{1}{l}\sum_{k = 0}^{l}h_{k}\right| \leq \delta.
	\]
	
	Тогда для \(n \geq n(\delta)\) имеем
	\begin{align*}
		\left|\frac{1}{n^{2}}\sum_{i = 1}^{n}\sum_{j = 0}^{i - 1} R(j)\right| 
		&= \left|\frac{1}{n^{2}}\sum_{i = 1}^{n(\delta)}\sum_{j = 0}^{i - 1} 
		R(j) + \frac{1}{n^{2}}\sum_{i = n(\delta) + 1}^{n}\sum_{j = 0}^{i - 1} 
		R(j)\right| \leq \\
		&\leq \left|\frac{1}{n^{2}}\sum_{i = 1}^{n(\delta)}\sum_{j = 0}^{i - 1} 
		R(j)\right| + \left|\frac{1}{n^{2}}\sum_{i = n(\delta) + 1}^{n}i\cdot 
		\frac{1}{i}\sum_{j = 0}^{i - 1} R(j)\right| \leq \\
		&\leq \frac{1}{n^{2}}\left|\sum_{i = 1}^{n(\delta)}\sum_{j = 0}^{i - 1} 
		R(j)\right| + \delta
	\end{align*}
	
	Теперь вспомним, что \(R(0) = const < \infty\). Тогда
	\[
		\varlimsup\limits_{n \to \infty} \E{\left|\frac{1}{n}\sum_{k = 
		1}^{n}h_{k}\right|^{2}} \leq \delta.
	\]
	
	Устремляя \(\delta\) к нулю, получаем желаемое.
\end{proof}

Тем самым мы получили весьма полезное свойство: \(\mathrm{MA}(1)\) эргодична в 
среднеквадратичном, то есть среднее по времени стремится в среднеквадратичном 
смысле к среднему по ансамблю \(\mu\).

Теперь вспомним про корреляционную функцию. Для \(\mathrm{MA}(1)\) она будет 
иметь вид
\[
	r(k) = \frac{\cov(h_{n + k}, h_{n})}{\sqrt{\D{h_{n + k}}\D{h_{n}}}} = 
	\frac{R(k)}{R(0)} = 
	\begin{cases}
	1, & k = 0 \\
	\frac{b_{0}b_{1}}{b_{0}^{2} + b_{1}^{2}}, & k = 1 \\
	0, & k > 1
	\end{cases}
\]

Вернёмся к общему случаю \(\mathrm{MA}(q)\). В качестве упражнения оставим 
вывод следующих тождеств:
\begin{gather*}
	\E{h_{n}} = \mu, \quad \D{h_{n}} = \sum_{k = 0}^{q} b_{k}^{2}, \\
	R(k) = \begin{cases}
	\sum_{i = 0}^{q - k} b_{i}b_{k + i},& k \leq q \\
	0,& k > q
	\end{cases}
\end{gather*}

Пользуясь этими формулами, моделью \(\mathrm{MA}(q)\) можно пытаться 
моделировать поведение последовательностей \(h = (h_{n})\), у которых 
корреляция величин \(h_{n}\) и \(h_{n + k}\), где \(k > q\), нулевая. Но как 
это делать? Общий принцип подгонки следующий:
\begin{itemize}
	\item Для начала, по выборке \((h_{1}, \ldots, h_{n})\) строятся некоторые 
	эмпирические характеристики: например,
	\begin{gather}
		\overline{h}_{n} = \frac{1}{n}\sum_{k = 1}^{n} h_{k}\text{~--- 
		выборочное среднее} \\
		\hat{\sigma}_{n}^{2} = \frac{1}{n - 1}\sum_{k = 1}^{n} (h_{k} - 
		\overline{h}_{n})\text{~--- выборочная дисперсия} \\
		r_{n}(k) = \frac{1}{n\hat{\sigma}_{n}^{2}}\sum_{i = k + 1}^{n}(h_{i} - 
		\overline{h}_{n})(h_{i - k} - \overline{h}_{n})\text{~--- выборочное 
		среднее}
	\end{gather}
	\item Далее, используя выражения для теоретических характеристик, 
	производится варьирование параметров с целью подгонки теоретических 
	значений под эмпирические.
	\item В конце провдодится \emph{оценка кажества} подгонки, основываясь на 
	знании эмпирических характеристик и их отклонений от теоретических 
	распределений.
\end{itemize}

Ранее мы смотрели на модели с конечным \(q\). Было бы разумно ввести обобщение, 
которое позволяет и бесконечное \(q\): \(\mathrm{MA}(\infty)\). Устроено оно 
так:
\[
	h_{n} = \mu + \sum_{j = 0}^{\infty} b_{j}\epsilon_{n - j}.
\]

Но, конечно, должны быть какие-то условия (как минимум, на сходимость). Если 
потребовать сходимость ряда \(\sum b_{j}^{2}\), то ряд в формуле для \(h_{n}\) 
будет сходиться в среднеквадратичном смысле.

Для этой модели
\[
	\E{h_{n}} = \mu, \quad \D{h_{n}} = \sum_{k = 0}^{\infty} b_{k}^{2}, \quad 
	R(k) = \sum_{i = 0}^{\infty} b_{k + i}b_{i}.
\]

В теории стационарных случайных процессов принято говорить, что \(h_{n}\) есть 
``результат реакции физически осуществимого фильтра с импульсной переходной 
функцией \(b = (b_{n})\), когда на вход подается последовательность \(\epsilon 
= (\epsilon_{n})\)''.

Оказывается, что в определённом смысле ``регулярная'' стационарная (в широком 
смысле) последовательность может быть представлена моделью 
\(\mathrm{MA}(\infty)\). Результат, связанный с этим фактом, называется 
\emph{разложением Вольда} стационарных последовательностей на <<сингулярную>> и 
<<регулярную>> части. За подробностями обращайтесь ко второму тому Ширяева.

\subsection{Авторегрессионная модель \(\mathrm{AR}(p)\)}
Поехали дальше. Авторегрессионая модель порядка \(p\) определяется следующим 
образом:
\[
	h_{n} = \mu_{n} + \sigma\epsilon_{n}, \quad \mu_{n} = a_{0} + a_{1}h_{n - 
	1} + \ldots + a_{p}h_{n - p}.
\]

Можно сказать, что модель \(\mathrm{AR}(p)\) подчиняется \emph{разностному 
уравнению} порядка \(p\):
\[
	h_{n} = a_{0} + a_{1}h_{n - 1} + \ldots + a_{p}h_{n - p} + 
	\sigma\epsilon_{n}.
\]

Если совпользоваться лаговым оператором, то это уравнение можно записать в виде:
\[
	(1 - a_{1}L - \ldots - a_{p}L^{p})h_{n} = a_{0} + \sigma\epsilon_{n}.
\]

Тогда, введя пару обознаений, уравнение приобретает вид
\[
	\alpha(L)h_{n} = \omega_{n}, \quad \omega_{n} = a_{0} + \sigma\epsilon_{n}, 
	\quad \alpha(L) = 1 - a_{1}L - \ldots - a_{p}L^{p}
\]

В отличие от модели со скользящий средним, в этой модели нужно задавать 
\emph{начальные} условия \((h_{1 - p}, h_{2 - p}, \ldots, h_{0})\). Обычно их 
обнуляют, хотя можно считать их случайными и не зависящими от \(\epsilon\). В 
эргодических случаях асимптотическое поведение последовательности при \(n \to 
\infty\) не зависит от начальных условий, и в этом смысле их конкретизация не 
столь существенна.

Рассмотрим модель \(\mathrm{AR}(1)\) поподробнее. Выглядит она следующим 
образом:
\[
	h_{n} = a_{0} + a_{1}h_{n - 1} + \sigma\epsilon_{n}.
\]

Этоа модель выделяется из общего класса \(\mathrm{AR}(p)\) моделей тем, что из 
<<прошлых>> величин \(h_{n - 1}, \ldots, h_{n - p}\), \(h_{n}\) зависит только 
от ближайшего (по времени) значения \(h_{n - 1}\). Если добавить к этой модели 
независимость последовательности \(\epsilon = (\epsilon_{n})\) и независимость 
\(h_{0}\) от неё, то получится \emph{конструктивный} пример марковской цепи.

Несложно получить, что
\[
	h_{n} = a_{0}(1 + a_{1} + a_{1}^{2} + \ldots + a_{1}^{n - 1}) + 
	a_{1}^{n}h_{0} + \sigma(\epsilon_{n} + a_{1}\epsilon_{n - 1}+ \ldots + 
	a_{1}^{n - 1}\epsilon_{1}).
\]

Отсюда видно, что свойства последовательности сильно зависят от \(a_{1}\). 
Учитывая формулу, есть смысл различать три случая: \(|a_{1}| < 1\), \(|a_{1}| = 
1\) и \(|a_{1}| > 1\), причём второй случай является <<пограничным>>. Смысл 
этого станет понятен позднее.

Из разложения понятно, что % нет, не понятно - надо расписать
\begin{align*}
	\E{h_{n}} &= a_{0}(1 + a_{1} + \ldots + a_{1}^{n - 1}) + a_{1}^{n}\E{h_{0}} 
	= \frac{a_{0}(1 - a_{1}^{n})}{1 - a_{1}} + a_{1}^{n}\E{h_{0}} \\
	\D{h_{n}} &= a_{1}^{2n}\D{h_{0}} + \sigma^{2}(1 + a_{1}^{2} + \ldots + 
	a_{1}^{2(n - 1)}) = a_{1}^{2n}\D{h_{0}} + \frac{\sigma^{2}(1 - 
	a_{1}^{2n})}{1 - a_{1}^{2}}\\
	\cov(h_{n}, h_{n - k}) &= a_{1}^{2n - k}\D{h_{0}} + \sigma^{2}a_{1}^{k}(1 + 
	a_{2} + \ldots + a_{1}^{2(n - k - 1)}) \\
	&= a_{1}^{2n - k}\D{h_{0}} + \frac{\sigma^{2}a_{1}^{k}(1 - a_{1}^{2(n - 
	k)})}{1 - a_{1}^{2}} \\
\end{align*}

Если \(|a_{1}| < 1\) и \(\E{h_{0}} < \infty]\), \(\D{h_{0}} < \infty\), то при 
\(n \to \infty\) последовательность <<стационаризируется>>:
\[
	\E{h_{n}} \to \frac{a_{0}}{1 - a_{1}}, \quad \D{h_{n}} \to 
	\frac{\sigma^{2}}{1 - a_{1}^{2}}, \quad \cov(h_{n}, h_{n - k}) \to 
	\frac{\sigma^{2}a_{1}^{k}}{1 - a_{1}^{2}}
\]

Теперь заметим, что если начальное распределение для \(h_{0}\) является 
нормальным:
\[
	h_{0} \sim \mathcal{N}\left(\frac{a_{0}}{1 - a_{1}}, \frac{\sigma^{2}}{1 - 
	a_{1}^{2}}\right),
\]
то последовательность \(h = (h_{n})\) является стационарной в узком смысле 
гауссовской последовательностью. Заметим, что для такой последовательности 
корреляция равна
\[
	r(k) = \frac{\cov(h_{n - k}, h_{n})}{\sqrt{\D{h_{n - k}}\D{h_{n}}}} = 
	a_{1}^{k}.
\] 

Зафиксируем \(n\). В таком случае
\[
	h_{n} = a_{0}(1 + a_{1} + a_{1}^{2} + \ldots + a_{1}^{n - 1}) + 
	a_{1}^{n}h_{0} + \sigma(\epsilon_{n} + a_{1}\epsilon_{n - 1}+ \ldots + 
	a_{1}^{n - 1}\epsilon_{1}).
\]

Если \(h_{0}\) есть константа, то, введя обозначение \(\mu = a_{0}(1 + a_{1} + 
a_{1}^{2} + \ldots + a_{1}^{n - 1}) + a_{1}^{n}h_{0}\), получится модель 
\(\mathrm{MA}(n - 1)\). В этом смысле иногда несколько вольно говорят, что 
<<модель \(\mathrm{AR}(1)\) может рассматриваться как модель 
\(\mathrm{MA}(\infty)\)>>.

Теперь скажем, что \(|a_{1}| = 1\). Тогда
\[
	h_{n} = a_{0}n + h_{0} + \sigma(\epsilon_{1} + \ldots + \epsilon_{n}).
\]

Если ввести обозначение \(\omega_{n} = a_{0} + \sigma\epsilon_{n}\), то модель 
будет иметь вид
\[
	h_{n} = h_{0} + \omega_{1} + \omega_{2} + \ldots + \omega_{n}.
\]

Это есть ни что иное, как случайное блуждание. Заметим, что
\[
	\E{h_{n}} = a_{0}n + \E{h_{0}}, \quad \D{h_{n}} = \sigma^{2}n 
	\xrightarrow[n \to \infty]{} \infty.
\]

Случай с \(|a_{1}| > 1\) называют \emph{взрывающимся}, так как и среднее 
значение, и дисперсия растут с ростом \(n\), причём экспоненциально быстро.

Теперь посмотрим на модель \(\mathrm{AR}(2)\):
\[
	h_{n} = a_{0} + a_{1}h_{n - 1} + a_{2}h_{n - 2} + \sigma\epsilon_{n} \iff 
	(1 - a_{1}L - a_{2}L^{2})h_{n} = a_{0} + \sigma\epsilon_{n}.
\]

Если \(a_{2} = 0\), то мы возвращаемся к модели \(\mathrm{AR}(1)\). Введя 
обозначение \(\omega_{n} = a_{0} + \sigma\epsilon_{n}\), получаем, что
\[
	(1 - a_{1}L)h_{n} = \omega_{n}.
\]

Вопрос: можно ли как-то <<обратить>> это равенство и сразу считать \(h_{n}\) 
только по \(\omega = (\omega_{n})\), не обращаясь к предыдущим значениям? 
Воспользуемся свойствами оператора \(L\) и заметим, что
\[
	(1 + a_{1}L + a_{1}^{2}L^{2} + \ldots + a_{1}^{k}L^{k})(1 - a_{1}L) = 1 - 
	a_{1}^{k + 1}L^{k + 1}.
\]

Тогда
\[
	h_{n} = (1 + a_{1}L + a_{1}^{2}L^{2} + \ldots + a_{1}^{k}L^{k})\omega_{n} + 
	a_{1}^{k + 1}L^{k + 1}h_{n}.
\]

Если положить \(k = n - 1\), то получим разложение, которое получали ранее:
\begin{align*}
	h_{n} &= (1 + a_{1}L + a_{1}^{2}L^{2} + \ldots + a_{1}^{n - 1}L^{n - 
	1})\omega_{n} + a_{1}^{n}h_{0} = \\
	&= (a_{0} + \sigma\epsilon_{n}) + a_{1}(a_{0} + \sigma\epsilon_{n - 1}) + 
	\ldots + a_{1}^{n - 1}(a_{0} + \sigma\epsilon_{1}) + a_{1}^{n}h_{0} = \\
	&= a_{0}(1 + a_{1} + \ldots + a_{1}^{n - 1}) + a_{1}^{n}h_{0} + 
	\sigma(\epsilon_{n} + a_{1}\epsilon_{n - 1} + \ldots + a_{1}^{n - 
	1}\epsilon_{1}).
\end{align*}

Если \(|a_{1}| < 1\) и \(n\) достаточно велико, то неформально можно сказать, 
что
\[
	h_{n} \approx (1 + a_{1}L + a_{1}^{2}L^{2} + \ldots + a_{1}^{n - 1}L^{n - 
	1})\omega_{n} = (1 + a_{1}L + a_{1}^{2}L^{2} + \ldots + a_{1}^{n - 1}L^{n - 
	1})(1 - a_{1}L)h_{n}.
\]

Тем самым мы получаем, что <<обратный>> оператор \((1 - a_{1}L)^{-1}\) разумно 
определить следующим образом:
\[
	(1 - a_{1}L)^{-1} \equiv \sum_{k = 0}^{\infty} a_{1}^{k}L^{k} = 1 + a_{1}L 
	+ a_{1}^{2}L^{2} + \ldots + a_{1}^{n}L^{n} + \ldots
\]

Оказывается, что если получаемый ряд для \(h_{n}\) сходится в 
среднеквадратичном смысле, то это разложение единственно.

Пользуясь этим рассуждением, можно найти похожее представление для 
\(\mathrm{AR}(2)\). Так как
\[
	(1 - \lambda_{1}L)(1 - \lambda_{2}L) = 1 - (\lambda_{1} + \lambda_{2})L + 
	\lambda_{1}\lambda_{2}L^{2},
\]
то, определяя \(\lambda_{1}\) и \(\lambda_{2}\) из системы
\[
	\begin{cases}
	\lambda_{1} + \lambda_{2} = a_{1} \\
	\lambda_{1}\lambda_{2} = -a_{2}
	\end{cases}
\]
получим, что
\[
	1 - a_{1}L - a_{2}L^{2} = (1 - \lambda_{1}L)(1 - \lambda_{2}L).
\]

Тогда
\[
	(1 - \lambda_{1}L)(1 - \lambda_{2})h_{n} = \omega_{n} \implies h_{n} = (1 - 
	\lambda_{1}L)^{-1}(1 - \lambda_{2}L)^{-1}\omega_{n}
\]

Воспользуемся методом неопределённых коэффициентов:
\[
	\frac{1}{(1 - \lambda_{1}L)(1 - \lambda_{2}L)} = \frac{A}{1 - \lambda_{1}L} 
	+ \frac{B}{1 - \lambda_{2}L} = \frac{A - A\lambda_{2}L + B - 
	B\lambda_{1}L}{(1 - \lambda_{1}L)(1 - \lambda_{2}L)}
\]

Тогда
\[
	\begin{cases}
	A + B = 1 \\
	A\lambda_{2} + B\lambda_{1} = 0
	\end{cases}
	\implies
	A = \frac{\lambda_{1}}{\lambda_{1} - \lambda_{2}}, B = 
	-\frac{\lambda_{2}}{\lambda_{1} - \lambda_{2}}
\]

Отсюда получаем, что
\[
	h_{n} = \frac{\lambda_{1}}{\lambda_{1} - \lambda_{2}}(1 - 
	\lambda_{1}L)^{-1}\omega_{n} - \frac{\lambda_{2}}{\lambda_{1} - 
	\lambda_{2}}(1 - \lambda_{2}L)^{-1}\omega_{n}.
\]

Предположим, что все \(\lambda_{i}\) такие, что \(|\lambda_{i}| < 1\). Тогда 
\[
	h_{n} = \sum_{k = 0}^{\infty} (c_{1}\lambda_{1}^{k} + 
	c_{2}\lambda_{2}^{k})\omega_{k}, \text{ где } c_{1} = 
	\frac{\lambda_{1}}{\lambda_{1} - \lambda_{2}}, \quad c_{2} = 
	-\frac{\lambda_{2}}{\lambda_{1} - \lambda_{2}}.
\]

Это рассуждение обобщается и на модель \(\mathrm{AR}(p)\). Для неё
\[
	(1 - a_{1}L - a_{2}L^{2} - \ldots - a_{p}L^{p})h_{n} = \omega_{n}.
\]

Опять же, разложим его на множители:
\[
	1 - a_{1}L - a_{2}L^{2} - \ldots - a_{p}L^{p} = (1 - \lambda_{1}L)\ldots(1 
	- \lambda_{p}L)
\]

Если все \(|\lambda_{i}| < 1\), то получится стационарное решение, которое 
будет единственным среди решений с конечным вторым моментом:
\[
	h_{n} = (1 - \lambda_{1}L)^{-1}\ldots(1 - \lambda_{p}L)^{-1}\omega_{n}.
\]

Теперь сведём это в виду ряда. Для этого снова воспользуемся методом 
неопределённых коэффициентов:
\[
	\frac{1}{(1 - \lambda_{1}z)\ldots(1 - \lambda_{p}z)} = \frac{c_{1}}{1 - 
	\lambda_{1}z} + \ldots + \frac{c_{p}}{1 - \lambda_{p}z}.
\]

Умножая на \((1 - \lambda_{1}z)\ldots(1 - \lambda_{p}z)\), получаем уравнение, 
которое должно выполняться для всех \(z\):
\[
	1 = \sum_{k = 1}^{p}c_{k}\prod_{i \neq k}(1 - \lambda_{i}z).
\]

Пподставляя значения \(z = 0, \lambda_{k}^{-1}\), \(k \in \{1, 2, \ldots, 
p\}\), получаем, что \(c_{1} + \ldots + c_{p} = 1\) и
\[
	c_{k} = \frac{\lambda_{k}^{p - 1}}{\prod_{i \neq k}(\lambda_{k} - 
	\lambda_{i})}.
\]

Следовательно, решение имеет вид
\[
	h_{n} = \sum_{k = 0}^{\infty}(c_{1}\lambda_{1}^{k} + \ldots + 
	c_{p}\lambda_{p}^{k})\omega_{n - k}.
\]

Это разложение помогает считать различные характеристики последовательности \(h 
= (h_{n})\): например, моменты \(\E{h_{n}^{k}}\), ковариации, условные 
математические ожидания и так далее.

Что мы можем сказать про некоторые характеристики последовательности \(h = 
(h_{n})\), если она стационарна (в широком смысле)? Для этого воспользуемся 
определением модели. Тогда, если \(\mu \equiv \E{h_{k}}\), то
\[
	\mu = a_{0} + a_{1}\mu + \ldots + a_{p}\mu \implies \mu = \frac{a_{0}}{1 - 
	(a_{1} + \ldots + a_{p})}.
\]

Ковариация \(R(k) = \cov(h_{n + k}, h_{n})\)  при \(k > 0\) же удовлетворяет 
следующему уравнению:
\begin{align*}
	R(k) &= \cov(h_{n + k}, h_{n}) = \cov(a_{0} + a_{1}h_{n + k - 1} + \ldots + 
	a_{p}h_{n + k - p} + \sigma\epsilon_{n + k}, h_{n}) = \\
	&= a_{1}R(k - 1) + a_{2}R(k - 2) + \ldots + a_{p}R(k - p).
\end{align*}

Если \(k = 0\), то уравнение имеет вид
\[
	R(0) = a_{1}R(1) + \ldots + a_{p}R(p) + \sigma^{2}.
\]

Оказывается, что аналогичные самые уравнения верны и для корреляций \(r(k)\). 
Их принято называть \emph{уравнениями Юла-Уолкера}.

\subsection{Модель авторегрессии и скользящего среднего \(\mathrm{ARMA}(p, q)\) 
и интегральная модель \(\mathrm{ARIMA}(p, d, q)\)}

Модель \(\mathrm{ARMA}(p, q)\) совмещает в себе возможности и модели 
скользящего среднего, и авторегрессионной модели. Это отображено и в названии: 
\(\mathrm{ARMA} = \mathrm{AR} + \mathrm{MA}\). Перейдём к определению.

\begin{definition}
	Будем называть последовательность \(h = (h_{n})\) 
	\emph{\(\mathrm{ARMA}(p, q)\)-моделью}, если
	\[
		h_{n} = \mu_{n} + \sigma\epsilon_{n}, \text{ где } \mu_{n} = (a_{0} + 
		a_{1}h_{n - 1} + \ldots + a_{p}h_{n - p}) + (b_{1}\epsilon_{n - 
		1} + \ldots + b_{q}\epsilon_{n - q}).
	\]
\end{definition}

Без ограничения общности можно полагать, что \(\sigma = 1\). Тогда
\[
	h_{n} - a_{1}h_{n - 1} - \ldots - a_{p}h_{n - p} = a_{0} + \epsilon_{n} + 
	b_{1}\epsilon_{n - 1} + \ldots + b_{q}\epsilon_{n - q}
\]

Введём два оператора:
\[
	\alpha(L) = 1 - a_{1}L - \ldots - a_{p}L^{p}, \quad \beta(L) = 1 + b_{1}L + 
	\ldots + b_{q}L^{q}.
\]

Тогда модель можно записать следующим образом:
\[
	\alpha(L)h_{n} = a_{0} + \beta(L)\epsilon_{n} \iff h_{n} = \frac{a_{0}}{1 - 
	(a_{1} + \ldots + a_{p})} + \frac{\beta(L)}{\alpha(L)}\epsilon_{n}.
\]

Опять же, поднимем вопрос о существовании стационарного решения этого 
уравнения. Из предыдущих рассуждений и вида уравнения понятно, что 
стационарность задаётся авторегрессионной компонентой модели \(\mathrm{ARMA}(p, 
q)\). Следовательно, если все корни уравнения \(1 - a_{1}z - \ldots - 
a_{p}z^{p} = 0\) меньше единицы по модулю, то стационарное решение существует и 
единственно (в классе \(L^{2}\)). Для него
\[
	\E{h_{n}} = \frac{a_{0}}{1 - (a_{1} + \ldots + a_{p})}.
\]

Со ковариацией же ситуация немного другая. Если \(k > q\), то выполнено 
уравнение Юла-Уолкера:
\begin{align*}
	R(k) &= \cov(a_{0} + a_{1}h_{n + k - 1} + \ldots + 
	a_{p}h_{n + k - p} + \epsilon_{n + k} + b_{1}\epsilon_{n + k - 1} + \ldots 
	+ b_{q}\epsilon_{n + k - q}, h_{n}) = \\
	&= a_{1}R(k - 1) + \ldots + a_{p}R(k - p).
\end{align*}
Если же \(k \leq q\), то уже нужно учитывать корреляционную зависимость между 
\(h_{n}\) и \(\epsilon_{n - l}\), \(l \geq 0\).

Рассмотрим модель \(\mathrm{ARMA}(1, 1)\):
\[
	h_{n} - a_{1}h_{n - 1} = a_{0} + \epsilon_{n} + b_{1}\epsilon_{n - 1}.
\]

Предположим, что \(|a_{1}| < 1\). Тогда
\begin{align*}
	h_{n} &= \frac{a_{0}}{1 - a_{1}} + \frac{1 + b_{1}L}{1 - 
	a_{1}L}\epsilon_{n} = \frac{a_{0}}{1 - a_{1}} + \bigg(\sum_{k = 0}^{\infty} 
	a_{1}^{k}L^{k}\bigg)(1 + b_{1}L)\epsilon_{n} = \\
	&= \frac{a_{0}}{1 - a_{1}} + \sum_{k = 0}^{\infty} 
	a_{1}^{k}L^{k}\epsilon_{n} + b_{1}\sum_{k = 0}^{\infty} a_{1}^{k}L^{k + 
	1}\epsilon_{n} = \frac{a_{0}}{1 - a_{1}} + \epsilon_{n} + (a_{1} + 
	b_{1})\sum_{k = 1}^{\infty} a_{1}^{k - 1}\epsilon_{n - k}
\end{align*}

Что мы можем сказать про ковариацию? На самом деле много. Пользуясь этим 
разложением, получаем, что при \(k > 1\) \(R(k) = a_{1}R(k - 1)\) и
\[
	R(1) = \cov(h_{n + 1}, h_{n}) = \cov(a_{0} + a_{1}h_{n} + \epsilon_{n + 1} 
	+ b_{1}\epsilon_{n}, h_{n}) = a_{1}R(0) + b_{1}
\]

Осталось решить:
\begin{align*}
	R(0) &= \D{h_{n}} = (a_{1} + b_{1})^{2}\sum_{k = 0}^{\infty}a_{1}^{2k} + 1 
	= \frac{a_{1}^{2} + 2a_{1}b_{1} + b_{1}^{2}}{1 - a_{1}^{2}} + 1 = \frac{1 + 
	2a_{1}b_{1} + b_{1}^{2}}{1 - a_{1}^{2}},\\
	R(1) &= \frac{a_{1} + 2a_{1}^{2}b_{1} + a_{1}b_{1}^{2}}{1 - a_{1}^{2}} + 
	b_{1} = \frac{a_{1} + b_{1} + a_{1}^{2}b_{1} + a_{1}b_{1}^{2}}{1 - 
	a_{1}^{2}} = \frac{(a_{1} + b_{1})(1 + a_{1}b_{1})}{1 - a_{1}^{2}} \\
	R(k) &= \frac{(a_{1} + b_{1})(1 + a_{1}b_{1})}{1 - a_{1}^{2}}a_{1}^{k - 1}, 
	\quad k > 0.
\end{align*}

Следовательно, корреляция равна
\[
	r(k) = \frac{(a_{1} + b_{1})(1 + a_{1}b_{1})}{1 + 2a_{1}b_{1} + 
	b_{1}^{2}}a_{1}^{k - 1}, \quad k > 0.
\]

Модели \(\mathrm{ARMA}(p, q)\) достаточно хорошо изучены и успешно применяются 
при описании стационарных временных рядов. Однако стационарность есть не 
всегда. Но, переходя от временного ряда \(x = (x_{n})\) к ряду разностей \(h = 
(h_{n})\), где \(h_{n} = \Delta x_{n} \equiv x_{n} - x_{n - 1}\), или же 
разностей более высокого порядка: \(h_{n} = \Delta^{d} x_{n} = (1 - 
L)^{d}x_{n}\), получается получить стационарность. Именно из этих соображений и 
появилась модель \(\mathrm{ARIMA}(p, d, q)\). 
\begin{definition}
	Будем говорить, что последоватеьность \(x = (x_{n})\) образует 
	\(\mathrm{ARIMA}(p, d, q)\)-модель, если последовательность \(\Delta^{d} x 
	= (\Delta^{d} x)\) образует \(\mathrm{ARMA}(p, q)\)-модель.
\end{definition}

Неформально это можно записать так:
\[
	\Delta^{d}\mathrm{ARIMA}(p, d, q) = \mathrm{ARMA}(p, q).
\]

Проясним смысл модели на примере \(\mathrm{ARIMA}(0, 1, 1)\). Она устроена 
следующим образом: \(h_{n} = \Delta x_{n}\), где \(h = (h_{n})\) является 
моделью \(\mathrm{ARMA}(0, 1) = \mathrm{MA}(1)\):
\[
	\Delta x_{n} = \mu + (b_{0} + b_{1}L)\epsilon.
\]

Если ввести оператор <<интегрирования>> \(S\) по правилу \(S \equiv 
\Delta^{-1}\), или, что равносильно,
\[
	S = (1 - L)^{-1} = 1 + L + L^{2} + \ldots
\]
то формально можно записать, что \(x_{n} = (Sh)_{n}\), где \(h_{n} = \mu + 
b_{0}\epsilon_{n} + b_{1}\epsilon_{n - 1}\).

Следовательно, \(x = (x_{n})\) можно рассматривать, как результат 
<<интегрирования>> последовательности \(h = (h_{n})\), подчиняющейся модели 
\(\mathrm{MA}(1)\). Это объясняет происхождение названия: \(\mathrm{ARIMA} = 
\mathrm{AR} + \mathrm{I} + \mathrm{MA}\), где I происходит от слова 
<<Integrated>>.

\subsection{Нелинейные модели: \(\mathrm{ARCH}\) и \(\mathrm{GARCH}\)}

Казалось бы, линейные модели всем хороши: имеют широкое применение на практике 
и устроены очень просто. Однако всё не так радужно. На практике в данных могут 
возникать самые разные феномены, которые линейная модель описать не может. 
Например, если мы смотрим на цены, то могут возникать: кластеризация, 
катастрофическое изменение, тяжёлые хвосты распределений величин \(h = 
(h_{n})\) (см. \hyperref[price-time-series]{выше}), наличие <<долгой памяти>> у 
цен и её свойств и так далее. Для того, чтобы как-то описать их, обращаются к 
\emph{нелинейным} моделям. Таких моделей много, а особенностей в данных ещё 
больше, поэтому перед исследователями возникает далеко не самая тривиальная 
задача подбора <<подходящей>> модели.

Как и раньше, пусть \((\Omega, \F, \Pr)\)~--- исходное вероятностное 
пространство, а \(\epsilon = (\epsilon_{n})\)~--- последовательность iid 
случайных величин с стандартным нормальным распределением (они будут 
моделировать <<случайность>> в рассматриваемых далее моделях). Далее, введём 
фильтрацию \((\F_{n})_{n \geq 0}\) по правилу: \(\F_{0} = \{\emptyset, 
\Omega\}\), \(\F_{n} = \sigma(\epsilon_{1}, \ldots, \epsilon_{n})\).

Модель \(\mathrm{ARCH}(p)\) была введена Робертом Энглем следующим образом:
\begin{definition}
	Будем называть последовательность случайных величин \(h = (h_{n})\) 
	\(\mathrm{ARCH}(p)\)-моделью, если
	\[
		h_{n} = \sigma_{n}\epsilon_{n}, \text{ где } \sigma_{n}^{2} = 
		\alpha_{0} + \sum_{k = 1}^{p}\alpha_{i}h_{n - k}^{2},
	\]
	\(\alpha_{0} > 0\), \(\alpha_{i} \geq 0\), \(h_{0} = h_{0}(\omega)\)~--- 
	случайная величина, не зависящая от \(\epsilon = (\epsilon_{n})_{n \geq 
	1}\).
\end{definition}
Обычно \(h_{0}\) полагают либо константой, либо случайной величиной, для 
которой второй момент выбирается из соображений <<стационарности>> значений 
\(\E{h_{n}^{2}}\).

Из формулы для \(\sigma^{2}_{n}\) видна явная зависимость от \(h_{n - 1}^{2}, 
\ldots, h_{n - p}^{2}\). При этом ясно, что большие (малые) значения \(h_{n - 
k}^{2}\) приводят к большим (малым) значениям \(\sigma_{n}^{2}\). Возникновение 
же большого значения \(h_{n}^{2}\) при условии, что \(h_{n - 1}^{2}, \ldots, 
h_{n - p}^{2}\) были мылыми, происходит из-за возникновения большого значения 
\(\epsilon_{n}\). Это объясняет то, почему нелинейные модели могут помочь в 
описании событий наподобие кластерности, то есть группирования значений в пачки 
<<больших>> и пачки <<маленьких>> значений.

\(\mathrm{ARCH}\)расшифровывается, как \emph{авторегрессионная модель условной 
неоднородности}, или AutoRegressive Conditional Heteroskedastic. Смысл каждого 
слова весьма понятен: авторегрессивная = прямая зависимость от своих предыдущих 
значений, условная = задаётся условное распределение \(\Law(h_{n} \mid \F_{n - 
1})\), неоднородность = \(\sigma_{n}^{2}\) ведёт себя весьма неоднородно.

Теперь рассмотрим модель \(\mathrm{ARCH}(1)\). Для неё
\[
	h_{n} = \sigma_{n}\epsilon_{n}, \text{ где } \sigma_{n}^{2} = a_{0} + 
	\alpha_{1}h_{n - 1}^{2}.
\]

Несложно понять, что выполнены следующие условия:
\[
	\E{h_{n}} = 0, \quad \E{h^{2}} = \alpha_{0} + \alpha_{1}\E{h_{n - 1}^{2}}, 
	\quad \E{h^{2} \given \F_{n - 1}} = \sigma_{n}^{2} = a_{0} + 
	\alpha_{1}h_{n - 1}^{2}.
\]

Если предположить, что \(\alpha_{1} \in (0, 1)\), то рекуррентное соотношение 
на матожидание квадрата будет иметь единственное <<стационарное>> решение: 
\(\E{h_{n - 1}^{2}} = \alpha_{0}/(1 - \alpha_{1})\). Если взять \(h_{0}^{2} = 
\alpha_{0}/(1 - \alpha_{1})\), то матожидание квадрата будет постоянно.

Далее, посчитаем четвёртый момент, пользуясь независимостью \(\sigma_{n}\) и 
\(\epsilon_{n}\) и тем, что \(\E{\epsilon_{n}^{4}} = 3\) (проверьте!):
\begin{align*}
	\E{h_{n}^{4}} &= \E{\sigma_{n}^{4}}\E{\epsilon_{n}^{4}} = 3\E{(\alpha_{0} + 
	\alpha_{1}h_{n - 1}^{2})^{2}} = \\
	&= 3(\alpha_{0}^{2} + 2\alpha_{0}\alpha_{1}\E{h_{n - 1}^{2}} + 
	\alpha_{1}^{2}\E{h_{n - 1}^{4}}) = \\
	&= \frac{3\alpha_{0}^{2}(1 + \alpha_{1})}{1 - \alpha_{1}} + 
	3\alpha_{1}^{2}\E{h_{n - 1}^{4}}.
\end{align*}

Если предположить, что \(\alpha_{1} \in (0, 1)\) и \(3\alpha_{1}^{2} < 1\), то 
можно найти <<стационарное решение>>:
\[
	\E{h_{n}^{4}} = \frac{3\alpha_{0}^{2}(1 + \alpha_{1})}{(1 - \alpha_{1})(1 - 
	3\alpha_{1}^{2})}.
\]

Из полученных формул несложно получить, что стационарное значение коэффициента 
эксцесса равно
\[
	K \equiv \frac{\E{h_{n}^{4}}}{(\E{h_{n}^{2}})^{2}} - 3 = \frac{3(1 - 
	\alpha_{1}^{2})}{1 - 3\alpha_{1}^{2}} - 3 = \frac{6\alpha_{1}^{2}}{1 - 
	3\alpha_{1}^{2}}.
\]

Его положительность говорит о том, что плотность <<установившегося>> 
распределения \(h = (h_{n})\) в окрестности среднего значения <<вытянута>> 
вверх. Напомним, что для нормального распределения эксцесс равен нулю.

Теперь заметим, что наша модель задаёт \emph{мартингал-разность}, то есть 
\(\E{h_{n} \given \F_{n - 1}} = 0\). Отсюда следует, что для любого \(k < n\)
\[
	\E{h_{n}h_{k}} = \E{\E{h_{n}h_{k} \given \F_{n - 1}}} = \E{h_{k}\E{h_{n} 
	\given \F_{n - 1}}} = 0.
\]

Это означает \emph{ортогональность} значений: \(\cov(h_{n}, h_{m}) = 0\) при 
\(n \neq m\). Но из ортогональности этих значений не следует независимость, так 
как совместное распределение \(\Law(h_{m}, h_{n})\) не является гауссовским при 
\(\alpha_{1} > 0\) (почему?). Но если они не независимы, то между ними есть 
какая-то зависимость. Как её исследовать? Для этого посмотрим на корреляционную 
зависимость \emph{квадратов} \(h_{n}^{2}\) и \(h_{m}^{2}\) в <<стационарном>> 
случае. Посчитаем дисперсию и ковариацию для соседних значений:
\begin{align*}
	\D{h_{n}^{2}} &= \E{h_{n}^{4}} - (\E{h_{n}^{2}})^{2} = 
	\frac{3\alpha_{0}^{2}(1 + \alpha_{1})}{(1 - \alpha_{1})(1 - 
	3\alpha_{1}^{2})} - \frac{\alpha_{0}^{2}}{(1 - \alpha_{1})^{2}} = \\
	&= \left(\frac{\alpha_{0}}{1 - \alpha_{1}}\right)^{2}\left(\frac{3 - 
	3\alpha_{1}^{2}}{1 - 3\alpha_{1}^{2}} - 1\right) = \frac{2}{1 - 
	3\alpha_{1}^{2}}\left(\frac{\alpha_{0}}{1 - \alpha_{1}}\right)^{2}. \\
	\E{h_{n}^{2}h_{n - 1}^{2}} &= \E{(\alpha_{0} + \alpha_{1}h_{n - 
	1}^{2})\epsilon_{n}^{2}h_{n - 1}^{2}} = \E{\alpha_{0}h_{n - 1}^{2}} + 
	\E{\alpha_{1}h_{n - 1}^{4}} = \\
	&= \frac{\alpha_{0}^{2}}{1 - \alpha_{1}} + \frac{2\alpha_{1}}{1 - 
	3\alpha_{1}^{2}}\left(\frac{\alpha_{0}}{1 - \alpha_{1}}\right)^{2} = 
	\frac{\alpha_{0}^{2}}{1 + \alpha_{1}}\left(1 - \frac{2\alpha_{1}}{(1 - 
	\alpha_{1})(1 - 3\alpha_{1}^{2})}\right) = \\
	&= \frac{\alpha_{0}^{2}}{1 - \alpha_{1}}\left(\frac{1 - \alpha_{1} - 
	3\alpha_{1}^{2} + 3\alpha_{1}^{2} + 2\alpha_{1}}{(1 - 3\alpha_{1}^{2})(1 - 
	\alpha_{1})}\right) =  \frac{\alpha_{0}^{2}}{1 - \alpha_{1}}\frac{1 + 
	3\alpha_{1}}{1 - 3\alpha_{1}^{2}}, \\
	\cov(h_{n}^{2}, h_{n - 1}^{2}) &= \E{h_{n}^{2}h_{n - 1}^{2}} - 
	\E{h_{n}^{2}}\E{h_{n - 1}^{2}} = \frac{1 + 3\alpha_{1}}{1 - 
	3\alpha_{1}^{2}}\frac{\alpha_{0}^{2}}{1 - \alpha_{1}} - 
	\frac{\alpha_{0}^{2}}{(1 - \alpha_{1})^{2}} = \\
	&= \left(\frac{\alpha_{0}}{1 - \alpha_{1}}\right)^{2}\left(\frac{(1 - 
	\alpha_{1})(1 + 3\alpha_{1})}{1 - 3\alpha_{1}^{2}} - 1\right) = 
	\frac{2\alpha_{1}}{1 - 3\alpha_{1}^{2}}\left(\frac{\alpha_{0}}{1 - 
	\alpha_{1}}\right)^{2}, \\
	r(1) &\equiv \frac{\cov(h_{n}^{2}, h_{n - 1}^{2})}{\sqrt{\D{h_{n}^{2}} 
	\D{h_{n - 1}^{2}}}} = \alpha_{1}.
\end{align*}

Далее посчитаем корреляцию в общем случае. Заметим, что для \(k < n\)
\begin{align*}
	\E{h_{n}^{2}h_{n - k}^{2}} &= \E{\E{h_{n}^{2}h_{n - k}^{2} \given \F_{n - 
	1}}} = \E{h_{n - k}^{2}\E{h_{n}^{2} \given \F_{n - 1}}} = \\
	&= \E{h_{n - k}^{2}\E{\sigma_{n}^{2}\epsilon_{n}^{2} \given \F_{n - 1}}} =  
	\E{h_{n - k}^{2}\sigma_{n}^{2}\E{\epsilon_{n}^{2} \given \F_{n - 1}}} = \\
	&= \E{h_{n - k}^{2}(\alpha_{0} + \alpha_{1}h_{n - 1}^{2})} = 
	\alpha_{0}\E{h_{n - k}^{2}} + \alpha_{1}\E{h_{n - 1}^{2}h_{n - k}^{2}}.
\end{align*}

Тогда в стационарном случае это равенство можно преобразовать следующим образом:
\[
	\E{h_{n}^{2}h_{n - k}^{2}} - \left(\frac{\alpha_{0}}{1 - 
	\alpha_{1}}\right)^{2} = \frac{\alpha_{0}^{2}}{1 - 
	\alpha_{1}} - (1 - \alpha_{1})\left(\frac{\alpha_{0}}{1 - 
	\alpha_{1}}\right)^{2} + \alpha_{1}\left(\E{h_{n - 1}^{2}h_{n - k}^{2}} - 
	\left(\frac{\alpha_{0}}{1 - \alpha_{1}}\right)^{2}\right).
\]

Отсюда следует, что
\[
	r(k) = \alpha_{1}r(k - 1) \implies r(k) = \alpha_{1}^{k}.
\]

В названии модели \(\mathrm{ARCH}(p)\) фигурирует слово <<авторегрессионая>>. 
Оказывается, что модель \(\mathrm{ARCH}(p)\) сводится к 
\(\mathrm{AR}(p)\)-модели. Действительно, пусть \(\nu_{n} = h_{n}^{2} - 
\sigma_{n}^{2}\). Если \(\E{h_{n}^{2}} < \infty\), то \(\E{\nu_{n} \given \F_{n 
- 1}} = 0\) и \(\nu = (\nu_{n})\) образует мартингал-разность относительно 
\((\F_{n})_{n \geq 0}\). Далее, введём обозначение \(x_{n} = h_{n}^{2}\). Тогда
\[
	x_{n} = \alpha_{0} + \alpha_{1}x_{n - 1} + \ldots + \alpha_{p}x_{n - p} 
	+ \nu_{n}.
\]

Успех условно-гауссовской модели \(\mathrm{ARCH}(p)\), давшей объяснение многим 
феноменам в поведении финансовых индексов, породил целую кучу различных её 
обобщений, преследующих цель <<ухватить>>, дать описание ряда других эффектов. 
Исторически первое обобщение было введено Тимом Боллерслевом в 1986-м году: так 
называемая \emph{обобщённая \(\mathrm{ARCH}\)-модель}, характеризуемая двумя 
параметрами \((p, q)\). Её принято обозначать \(\mathrm{GARCH}(p, q)\). 
Определяется она следующим образом:
\[
	h_{n} = \sigma_{n}\epsilon_{n}, \text{ где } \sigma_{n}^{2} = \alpha_{0} + 
	\sum_{i = 1}^{p}\alpha_{i}h_{n - i}^{2} + \sum_{j = 1}^{q} 
	\beta_{j}\sigma_{n - j}^{2}.
\]

Основное преимущество \(\mathrm{GARCH}(p, q)\)-моделей перед их 
прародительницей, \(\mathrm{ARCH}(p)\)-моделью, состоит в подборе параметров 
модели. На практике периодически может оказаться так, что при подгонке 
статистических данных моделями \(\mathrm{ARCH}(p)\) параметр \(p\) становится 
слишком большим (что усложняет анализ модели), в то время как при подгонке 
\(\mathrm{GARCH}(p, q)\)-моделями можно ограничиваться небольшими значениями 
\(p\) и \(q\) (экспериментальный факт!).

Как и всегда, подробнее рассмотрим частный случай: \(\mathrm{GARCH}(1, 1)\). 
Для него
\[
	h_{n} = \sigma_{n}\epsilon_{n}, \text{ где } \sigma_{n}^{2} = \alpha_{0} + 
	\alpha_{1}h_{n - 1}^{2} + \beta_{1}\sigma_{n - 1}^{2}.
\]

Отсюда ясно, что
\[
	\E{h_{n}} = 0, \quad \E{h_{n}^{2}} = \E{\sigma_{n}^{2}} = \alpha_{0} + 
	\alpha_{1}\E{h_{n - 1}^{2}} + \beta_{1}\E{\sigma_{n - 1}^{2}} = \alpha_{0} 
	+ (\alpha_{1} + \beta_{1})\E{h_{n - 1}^{2}}.
\]

Если \(\alpha_{1} + \beta_{1} < 1\), то существует <<стационарное>> значение 
\(\E{h_{n}^{2}}\), равное
\[
	\E{h_{n}^{2}} = \frac{\alpha_{0}}{1 - \alpha_{1} - \beta_{1}}.
\]

Далее, посчитаем четвёртый момент.
\begin{align*}
	\E{h_{n}^{4}} &= \E{\sigma_{n}^{4}}\E{\epsilon_{n}^{4}} = 3\E{(\alpha_{0} 
	+ \alpha_{1}h_{n - 1}^{2} + \beta_{1}\sigma_{n - 1}^{2})^{2}} = \\
	&= 3(\alpha_{0}^{2} + \alpha_{1}^{2}\E{h_{n - 1}^{4}} + 
	\beta_{1}^{2}\E{\sigma_{1}^{4}} + 2\alpha_{0}\alpha_{1}\E{h_{n - 1}^{2}} + 
	2\alpha_{0}\beta_{1}\E{\sigma_{n - 1}^{2}} + \\
	&+ 2\alpha_{1}\beta_{1}\E{h_{n - 1}^{2}\sigma_{n - 1}^{2}}) = 
	3\alpha_{0}^{2} + 3\alpha_{1}^{2}\E{h_{n - 1}^{4}} + \beta_{1}^{2}\E{h_{n - 
	1}^{4}} + \\
	&+ 6\alpha_{0}\alpha_{1}\E{h_{n - 1}^{2}} + 6\alpha_{0}\beta_{1}\E{h_{n - 
	1}^{2}} + 2\alpha_{1}\beta_{1}\E{h_{n - 1}^{4}}.
\end{align*}

Если предположить, что \(3\alpha_{1}^{2} + \beta_{1}^{2} + 2\alpha_{1}\beta_{1} 
< 1\), то можно найти <<стационарное>> решение:
\begin{gather*}
	(1 - 3\alpha_{1}^{2} - \beta_{1}^{2} - 2\alpha_{1}\beta_{1})\E{h_{n}^{4}} = 
	3\alpha_{0}^{2}\left(1 + \frac{2(\alpha_{1} + \beta_{1})}{1 - \alpha_{1} - 
		\beta_{1}}\right), \\
	\E{h_{n}^{4}} = \frac{3\alpha_{0}^{2}(1 + \alpha_{1} + \beta_{1})}{(1 - 
	\alpha_{1} - \beta_{1})(1 - 3\alpha_{1}^{2} - \beta_{1}^{2} - 
	2\alpha_{1}\beta_{1})}.
\end{gather*}

Отсюа можно получить коэффициент эксцесса:
\[
	K = \frac{3(1 - (\alpha_{1} + \beta_{1})^{2})}{1 - 3\alpha_{1}^{2} - 
	\beta_{1}^{2} - 2\alpha_{1}\beta_{1}} - 3 = \frac{6\alpha_{1}^{2}}{1 - 
	3\alpha_{1}^{2} - \beta_{1}^{2} - 2\alpha_{1}\beta_{1}}.
\]

Теперь посчитаем корреляционную функцию для этой модели. Заметим, что
\begin{align*}
	\E{h_{n}^{2}h_{n - 1}^{2}} &= \E{(\alpha_{0} + \alpha_{1}h_{n - 1}^{2} + 
	\beta_{1}\sigma_{n - 1}^{2})\epsilon_{n}^{2}h_{n - 1}^{2}} = \\
	&= \alpha_{0}\E{h_{n - 1}^{2}} + (\alpha_{1} + \beta_{1}/3)\E{h_{n - 
	1}^{4}} = \\
	&= \frac{\alpha_{0}^{2}}{1 - \alpha_{1} - \beta_{1}} + 
	\frac{\alpha_{0}^{2}(3\alpha_{1} + \beta_{1})(1 + \alpha_{1} + 
	\beta_{1})}{(1 - \alpha_{1} - \beta_{1})(1 - 3\alpha_{1}^{2} - 
	\beta_{1}^{2} - 2\alpha_{1}\beta_{1})} = \\
	&= \frac{\alpha_{0}^{2}}{1 - \alpha_{1} - \beta_{1}}\left(1 + 
	\frac{(3\alpha_{1} + \beta_{1})(1 + \alpha_{1} + \beta_{1})}{1 - 
	3\alpha_{1}^{2} - \beta_{1}^{2} - 2\alpha_{1}\beta_{1}}\right) \\
	\cov(h_{n}^{2}, h_{n - 1}^{2}) &= \E{h_{n}^{2}h_{n - 1}^{2}} - 
	\E{h_{n}^{2}}\E{h_{n - 1}^{2}} = \\
	&= \frac{\alpha_{0}^{2}}{1 - \alpha_{1} - \beta_{1}}\left(1 + 
	\frac{(3\alpha_{1} + \beta_{1})(1 + \alpha_{1} + \beta_{1})}{1 - 
	3\alpha_{1}^{2} - \beta_{1}^{2} - 2\alpha_{1}\beta_{1}}\right) - 
	\frac{\alpha_{0}^{2}}{(1 - \alpha_{1} - \beta_{1})^{2}} = \\
	&= \frac{\alpha_{0}^{2}}{(1 - \alpha_{1} - \beta_{1})^{2}}\left(1 - 
	\alpha_{1} - \beta_{1} + \frac{(3\alpha_{1} + \beta_{1})(1 - (\alpha_{1} + 
	\beta_{1})^{2})}{1 - 3\alpha_{1}^{2} - \beta_{1}^{2} - 
	2\alpha_{1}\beta_{1}} - 1\right) = \\
	&= \frac{\alpha_{0}^{2}}{(1 - \alpha_{1} - 
	\beta_{1})^{2}}\left(\frac{(3\alpha_{1} + \beta_{1})(1 - \alpha_{1}^{2} - 
	\beta_{1}^{2} - 2\alpha_{1}\beta_{1})}{1 - 3\alpha_{1}^{2} - \beta_{1}^{2} 
	- 2\alpha_{1}\beta_{1}} - (\alpha_{1} + \beta_{1})\right) = \\
	&= \left(\frac{\alpha_{0}}{1 - \alpha_{1} - 
	\beta_{1}}\right)^{2}\frac{2\alpha_{1}(1 - \alpha_{1}\beta_{1} - 
	\beta_{1}^{2})}{1 - 3\alpha_{1}^{2} - \beta_{1}^{2} - 2\alpha_{1}\beta_{1}}
\end{align*} 
\begin{align*}
	\D{h_{n}^{2}} &= \E{h_{n}^{4}} - (\E{h_{n}^{2}})^{2} = 
	\frac{3\alpha_{0}^{2}(1 + \alpha_{1} + \beta_{1})}{(1 - \alpha_{1} - 
	\beta_{1})(1 - 3\alpha_{1}^{2} - \beta_{1}^{2} - 2\alpha_{1}\beta_{1})} - 
	\frac{\alpha_{0}^{2}}{(1 - \alpha_{1} - \beta_{1})^{2}} = \\
	&= \frac{\alpha_{0}^{2}}{(1 - \alpha_{1} - \beta_{1})^{2}}\left(\frac{3(1 - 
	\alpha_{1}^{2} - \beta_{1}^{2} - 2\alpha_{1}\beta_{1})}{1 - 3\alpha_{1}^{2} 
	- \beta_{1}^{2} - 2\alpha_{1}\beta_{1}} - 1\right) = \\
	&= \left(\frac{\alpha_{0}}{1 - \alpha_{1} - \beta_{1}}\right)^{2}\frac{2(1 
	- 2\alpha_{1}\beta_{1} - \beta_{1}^{2})}{1 - 3\alpha_{1}^{2} - 
	\beta_{1}^{2} - 2\alpha_{1}\beta_{1}}
\end{align*}

Отсюда сразу же получаем, что
\[
	\rho(1) \equiv \frac{\cov(h_{n}^{2}, h_{n - 
	1}^{2})}{\sqrt{\D{h_{n}^{2}}\D{h_{n - 1}^{2}}}} = \frac{\alpha_{1}(1 - 
	\alpha_{1}\beta_{1} - \beta_{1}^{2})}{1 - 2\alpha_{1}\beta_{1} - 
	\beta_{1}^{2}}.
\]

Далее заметим, что
\begin{align*}
	\E{h_{n}^{2}h_{n - k}^{2}} &= \E{\E{h_{n}^{2}h_{n - k}^{2} \given \F_{n - 
	1}}} = \E{h_{n - k}^{2}\E{h_{n}^{2} \given \F_{n - 1}}} = \\
	&= \E{h_{n - k}^{2}\E{\sigma_{n}^{2}\epsilon_{n}^{2} \given \F_{n - 1}}} =  
	\E{h_{n - k}^{2}\sigma_{n}^{2}\E{\epsilon_{n}^{2} \given \F_{n - 1}}} = \\
	&= \E{h_{n - k}^{2}(\alpha_{0} + \alpha_{1}h_{n - 1}^{2} + 
	\beta_{1}\sigma_{n - 1}^{2})} = \\
	&= \alpha_{0}\E{h_{n - k}^{2}} + (\alpha_{1} + \beta_{1})\E{h_{n - 
	1}^{2}h_{n - k}^{2}}
\end{align*}

Следовательно, в стационарном случае это можно перезаписать следующим образом:
\begin{multline*}
	\E{h_{n}^{2}h_{n - k}^{2}} - \left(\frac{\alpha_{0}}{1 - \alpha_{1} - 
	\beta_{1}}\right)^{2} = \frac{\alpha_{0}^{2}}{1 - \alpha_{1} - \beta_{1}} + 
	(\alpha_{1} + \beta_{1} - 1)\left(\frac{\alpha_{0}}{1 - \alpha_{1} - 
	\beta_{1}}\right)^{2} + \\
	+ (\alpha_{1} + \beta_{1})\left(\E{h_{n - 1}^{2}h_{n - k}^{2}} - 
	\left(\frac{\alpha_{0}}{1 - \alpha_{1} - \beta_{1}}\right)^{2}\right)
\end{multline*}

Тогда \(\cov(h_{n}^{2}, h_{n - k}^{2}) = (\alpha_{1} + \beta_{1})\cov(h_{n - 
1}^{2}, h_{n - k}^{2})\) и
\[
	\rho(k) \equiv \frac{\cov(h_{n}^{2}, h_{n - k}^{2})} 
	{\sqrt{\D{h_{n}^{2}}\D{h_{n - k}^{2}}}} = \frac{\alpha_{1}(1 - 
	\alpha_{1}\beta_{1} - \beta_{1}^{2})}{1 - 2\alpha_{1}\beta_{1} - 
	\beta_{1}^{2}}(\alpha_{1} + \beta_{1})^{k - 1}.
\]