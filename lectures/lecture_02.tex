\subsection{Лирическое отступление: гауссовские векторы}
Сделаю небольшое лирическое отступление и вспомним гауссовские векторы. Для 
этого вспомним, что такое \emph{характеристическая функция} случайной величины 
и вектора.
\begin{definition}
	Пусть \(\xi\)~--- случайная величина с плотностью \(p_{\xi}\). Тогда её 
	характеристической функцией называется функция \(\phi_{\xi} : \R \mapsto 
	\C\), определяемая следующим образом:
	\[
	\phi_{\xi}(t) = \E{e^{it\xi}} = \int\limits_{-\infty}^{+\infty} 
	e^{itx}p_{\xi}(x)\diff x.
	\]
\end{definition}
\begin{definition}
	Пусть \(\bm{\xi} = (\xi_{1}, \dots, \xi_{n})\)~--- случайный вектор с 
	совместной плотностью \(p_{\bm{\xi}}\). Тогда её характеристической 
	функцией называется функция \(\phi_{\bm{\xi}} : \R^{n} \mapsto \C\), 
	определяемая следующим образом: \(\phi_{\bm{\xi}}(\mathbf{t}) = 
	\E{e^{i\langle\bm{\xi}, \mathbf{t}\rangle}}\), где \(\langle\cdot, 
	\cdot\rangle\)~--- скалярное произведение в \(\R^{n}\).
\end{definition}

По сути, характеристическая функция~--- это преобразование Фурье функции 
распределения.

Далее, из курса теории вероятности известно, что если \(\xi \sim 
\mathcal{N}(\mu, \sigma^{2})\), то
\[
\phi_{\xi}(t) = \exp\left\{i \mu t - \frac{1}{2}\sigma^{2}t^{2}\right\}.
\]

Поэтому гауссовский вектор вводят следующим образом:
\begin{definition}
	Случайный вектор \(\bm{\xi} = (\xi_{1}, \dots, \xi_{n})\) подчиняется 
	\emph{многомерному нормальному распределению}, если его характеристическая 
	функция равна
	\[
	\phi_{\bm{\xi}}(\mathbf{t}) = \exp\left\{i\langle\bm{\mu}, 
	\mathbf{t}\rangle - \frac{1}{2}\langle\mathbf{\Sigma t}, 
	\mathbf{t}\rangle\right\},
	\]
	где \(\bm{\mu} \in \R^{n}\)~--- некоторый фиксированный вектор, а 
	\(\mathbf{\Sigma}\)~--- некоторая симметрическая и неотрицательно 
	определённая матрица. В таком случае пишут, что \(\xi \sim 
	\mathcal{N}(\bm{\mu}, \mathbf{\Sigma})\).
\end{definition}

Это определение не очень удобно. Докажем одну теорему, которая даст несколько 
более удобное определение.
\begin{theorem}
	Пусть \(\bm{\xi} = (\xi_{1}, \dots, \xi_{n})\)~--- случайный вектор. Он 
	будет гауссовским тогда и только тогда, когда для любого неслучайного 
	вектора \(\bm{\lambda} \in \R^{n}\) \(\langle\bm{\lambda}, 
	\bm{\xi}\rangle\) имеет нормальное распределение.
\end{theorem}
\begin{proof}
	Пусть \(\bm{\xi} \sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})\). Тогда посмотрим 
	на характеристическую функцию \(\langle\bm{\lambda}, \bm{\xi}\rangle\). 
	Заметим, что она равна
	\[
		\phi_{\langle\bm{\lambda}, \bm{\xi}\rangle}(t) = 
		\E{e^{it\langle\bm{\lambda}, \bm{\xi}\rangle}} = \E{e^{i\langle 
		t\bm{\lambda}, \bm{\xi}\rangle}} = \phi_{\bm{\xi}}(t\bm{\lambda}) = 
		\exp\left\{it\langle\bm{\mu}, \bm{\lambda}\rangle - 
		\frac{t^{2}}{2}\langle\mathbf{\Sigma}\bm{\lambda}, 
		\bm{\lambda}\rangle\right\}.
	\]
	
	Это означает, что \(\langle\bm{\lambda}, \bm{\xi}\rangle \sim 
	\mathcal{N}(\langle\bm{\mu}, \bm{\lambda}\rangle, \langle 
	\mathbf{\Sigma}\bm{\lambda}, \bm{\lambda}\rangle)\).
	
	Теперь предположим, что \(\langle\bm{\lambda}, \bm{\xi}\rangle\) имеет 
	нормальное распределение для любого \(\bm{\lambda}\). Тогда посмотрим на 
	характеристическую функцию \(\bm{\xi}\):
	\[
		\phi_{\bm{\xi}}(\bm{\lambda}) = \E{e^{i\langle\bm{\lambda}, 
		\bm{\xi}\rangle}} = \phi_{\langle \bm{\lambda}, \bm{\xi}\rangle}(1) = 
		\exp\left\{i\E{\langle\bm{\lambda}, \bm{\xi}\rangle} - 
		\frac{1}{2}\D{\langle\bm{\lambda},\bm{\xi}\rangle}\right\}.
	\]
	
	Теперь заметим, что
	\begin{gather}
		\E{\langle\bm{\lambda}, \bm{\xi}\rangle} = \E{\sum_{k = 
		1}^{n}\lambda_{k}\xi_{k}} = \sum_{k = 1}^{n}\lambda_{k}\E{\xi_{k}} = 
		\langle\bm{\lambda}, \E{\bm{\xi}}\rangle, \\
		\D{\langle\bm{\lambda}, \bm{\xi}\rangle} = \cov\left(\sum_{k = 
		1}^{n}\lambda_{k}\xi_{k}, \sum_{k = 1}^{n}\lambda_{k}\xi_{k}\right) = 
		\sum_{i, j = 1}^{n} \cov(\xi_{i}, \xi_{j})\lambda_{i}\lambda_{j} = 
		\langle\D{\bm{\xi}}\bm{\lambda}, \bm{\lambda}\rangle.
	\end{gather}

	Отсюда получаем, что \(\bm{\xi} \sim \mathcal{N}(\E{\bm{\xi}}, 
	\D{\bm{\xi}})\).
\end{proof}
Теперь выпишем следствия из этой теоремы.
\begin{consequence}[Смысл параметров]
	Если случайный вектор \(\bm{\xi} \sim \mathcal{N}(\bm{\mu}, 
	\mathbf{\Sigma})\), то \(\bm{\mu} = \E{\xi}\), \(\mathbf{\Sigma} = 
	\D{\xi}\).
\end{consequence}
\begin{consequence}[Линейные преобразования]
	Любое линейное преобразование гауссовского вектора тоже является 
	гауссовским вектором.
\end{consequence}

Теперь вспомним про плотности. У гауссовских векторов она есть не всегда.

\begin{theorem}[о плотности гауссовских векторов]
	Пусть \(\bm{\xi} \sim \mathcal{N}(\bm{\mu}, \mathbf{\Sigma})\)~--- 
	\(n\)-мерный гауссовский вектор. Тогда, если \(\mathbf{\Sigma}\) 
	положительно определена, то существует плотность 
	\(p_{\bm{\xi}}(\mathbf{t})\) и она равна
	\[
	p_{\bm{\xi}}(\mathbf{t}) = \frac{1}{(2\pi)^{n/2} 
	\sqrt{\det{\mathbf{\Sigma}}}} \exp\left\{-\frac{1}{2} 
	\left\langle\mathbf{\Sigma}^{-1}(\mathbf{t}
	- \bm{\mu}), (\mathbf{t} - \bm{\mu})\right\rangle\right\}.
	\]
\end{theorem}
\begin{problem}\label{bivariate-normal-density}
	Пусть \(\mathbf{X} = (\xi, \eta)\)~--- гауссовский вектор, для 
	которого:\footnote{Несложно понять, что \(\rho\) есть коэффициент 
	корреляции между \(\xi\) и \(\eta\).}
	\[
		\E{\mathbf{X}} = \begin{pmatrix}\mu_{1} \\ \mu_{2}\end{pmatrix}, \quad
		\D{\mathbf{X}} =
		\begin{pmatrix}
		\sigma_{1}^{2} & \rho\sigma_{1}\sigma_{2} \\
		\rho\sigma_{1}\sigma_{2} & \sigma_{2}^{2}
		\end{pmatrix}, \quad |\rho| < 1.
	\]
	
	Докажите, что плотность случайного вектора \(\mathbf{X}\) равна
	\begin{multline*}
		p_{\mathbf{X}}(x_{1}, x_{2}) = \frac{1}{2\pi\sigma_{1}\sigma_{2}\sqrt{1 
		- \rho^{2}}}\exp\bigg\{-\frac{1}{2(1 - \rho^{2})} 
		\bigg(\bigg(\frac{x_{1} - \mu_{1}}{\sigma_{1}}\bigg)^{2} + \\ +
		\bigg(\frac{x_{2} - \mu_{2}}{\sigma_{2}}\bigg)^{2} - 2\rho\frac{x_{1} - 
		\mu_{1}}{\sigma_{1}}\frac{x_{2} - \mu_{2}}{\sigma_{2}}\bigg)\bigg\}
	\end{multline*}
\end{problem}

Порой хочется сказать, что любой вектор, состоящий из нормальных случайных 
величин, является гауссовским. Но это неверно.

\begin{example}
	Пусть \(\xi_{1}\) и \(\xi_{2}\)~--- это независимые стандартные нормальные 
	случайные величины. Построим случайный вектор \((X_{1}, X_{2})\) следующим 
	образом:
	\[
		(X_{1}, X_{2}) = \begin{cases}
		(\xi_{1}, |\xi_{2}|),& \xi_{1} \geq 0 \\
		(\xi_{1}, -|\xi_{2}|),& \xi_{1} < 0
		\end{cases}
	\] 
	
	Данный случайный вектор не будет гауссовским (почему?).
\end{example}

У гауссовских векторов есть одно уникальное свойство, связанное с 
некоррелированностью.
\begin{theorem}
	Пусть \(\bm{\xi}\)~--- гауссовский вектор. Тогда его компоненты независимы 
	тогда и только тогда, когда они некоррелированны.
\end{theorem}

Вопрос: допустим, что у нас есть вектор, состоящий из некоррелированных 
нормальных случайных величин. Можно ли сказать, что он гауссовский? 
Оказывается, что нет.
\begin{example}
	Пусть \(X \sim \mathcal{N}(0, 1)\) и \(c \geq 0\). Построим по ней новую 
	случайную величину \(Y\) следующим образом:
	\[
		Y = \begin{cases}
		X,& |X| \leq c \\
		-X,& |X| > c
		\end{cases}
	\]
	
	Оказывается, что \(Y \sim \mathcal{N}(0, 1)\). Тогда \(\cov(X, Y) = 
	\E{XY}\), что, в свою очередь, равно \(\E{X^{2}\I\{|X| \leq c\}} - 
	\E{X^{2}\I\{|X| > c\}}\). Что мы можем сказать про ковариацию?
	\begin{itemize}
		\item Она является непрерывной функцией от \(c\).
		\item Если \(c = 0\), то \(\cov(X, Y) = -1\), а если \(c = +\infty\), 
		то \(\cov(X, Y) = 1\).
	\end{itemize}

	В таком случае можно сказать, что есть \(c\) такая, что \(\cov(X, Y) = 0\). 
	Зафиксируем её. Можно ли сказать, что \((X, Y)\)~--- это гауссовский 
	вектор? Увы, но нет. Если бы это было так, то \(X\) и \(Y\) были бы 
	независимы. Но \(\Pr{X > c, Y > c} = 0 \neq \Pr{X > c}\Pr{Y > c}\).
\end{example}

Теперь расскажем два факта, которые могут понадобиться в дальнейшем и связаны с 
нормальным распределением.
\begin{example}
	Пусть \((\xi, \eta)\)~--- случайный вектор с совместной плотностью
	\[
		p_{\xi, \eta}(x, y) = C\exp\{-(1 + x^{2})(1 + y^{2})\}, \text{ где } C
		= \iint\limits_{\R^{2}} \exp\{-(1 + x^{2})(1 + y^{2})\}\diff x \diff y
	\]
	
	Попробуем найти условную плотность \(p_{\xi \mid \eta}(x \mid y)\). Сразу 
	же заметим, что процесс аналогичен для \(p_{\eta \mid \xi}(y \mid x)\). Для 
	этого найдём плотность \(\eta\):
	\begin{multline*}
		p_{\eta}(y) = \int\limits_{-\infty}^{+\infty} Ce^{-(1 + x^{2})(1 + 
		y^{2})}\diff x = Ce^{-(1 + y^{2})}\int\limits_{-\infty}^{+\infty} 
		e^{-x^{2}(1 + y^{2})}\diff x = \\ = \frac{Ce^{-(1 + y^{2})}} 
		{\sqrt{y^{2} + 1}}\int\limits_{-\infty}^{+\infty} e^{-u^{2}}\diff u = 
		\frac{C\sqrt{\pi}e^{-(1 + y^{2})}}{\sqrt{y^{2} + 1}}.
	\end{multline*}
	
	Теперь несложно посчитать условную плотность:
	\[
		p_{\xi \mid \eta}(x \mid y) = \frac{g(x, y)}{p_{\eta}(y)} = 
		\frac{\sqrt{y^{2} + 1}}{\sqrt{\pi}}e^{-x^{2}(1 + y^{2})} = 
		\frac{1}{\sqrt{2\pi\sigma^{2}_{y}}}e^{-\frac{x^{2}}{2\sigma^{2}_{y}}}, 
		\text{ где } \sigma^{2}_{y} = \frac{1}{2 + 2y^{2}}.
	\]
\end{example}

Из математической статистики вам известны такие понятия, как оценки. Следующая 
теорема в некоторой степени связана с ними, но это станет ясней позднее, когда 
дело дойдёт до фильтров Калмана.
\begin{theorem}[о нормальной корреляции, одномерный случай]
	Пусть \((\xi, \eta)\)~--- двумерный гауссовский вектор. Тогда
	\begin{gather}
		\E{\eta \given \xi} = \E{\eta} + \frac{\cov(\xi, \eta)}{\D{\xi}}(\xi - 
		\E{\xi}), \\
		\Delta = \E{(\eta - \E{\eta \given \xi})^{2}} = \D{\eta} - 
		\frac{\cov^{2}(\xi, \eta)}{\D{\xi}}.
	\end{gather}
\end{theorem}
\begin{proof}
	Докажем эту теорему в лоб. Для этого посчитаем условную плотность \(p_{\eta 
	\mid \xi}(y \mid x)\). Формула для совместной плотности была выведена в 
	\hyperref[bivariate-normal-density]{задаче \ref*{bivariate-normal-density}}.
	Пользуясь теми же обозначениями, получим (проверьте!), что
	\[
		p_{\eta \mid \xi}(y \mid x) = \frac{1}{\sqrt{2\pi\sigma_{2}^{2}(1 - 
		\rho^{2})}}\exp\left\{-\frac{1}{2\sigma_{2}^{2}(1 - \rho^{2})}\left(y - 
		\mu_{2} - \rho\frac{\sigma_{2}}{\sigma_{1}}(x - 
		\mu_{1})\right)^{2}\right\}.
	\]
	
	Из этого можно сделать следующий вывод:
	\[
		(\eta \mid \xi = x) \sim \mathcal{N}\left(\mu_{2} + 
		\rho\frac{\sigma_{2}}{\sigma_{1}}(x - \mu_{1}), \sigma_{2}^{2}(1 - 
		\rho^{2})\right) \implies \E{\eta \given \xi} = \mu_{2} + 
		\rho\frac{\sigma_{2}}{\sigma_{1}}(\xi - \mu_{1}).
	\]
	
	Заменяя, получим первую часть утверждения. Для доказательства второго 
	утверждения подставим полученный результат и раскроем скобки как квадрат 
	разности:
	\begin{multline*}
		\Delta = \E{\left(\eta - \E{\eta} - \frac{\cov(\xi, \eta)}{\D{\xi}}(\xi 
			- \E{\xi})\right)^{2}} = \E{(\eta - \E{\eta})^{2}} - \\ -
			2\frac{\cov(\xi, \eta)}{\D{\xi}}\E{(\eta - \E{\eta})(\xi - 
			\E{\xi})} + \frac{\cov^{2}(\xi, \eta)}{\D^{2}[\xi]}\E{(\xi - 
			\E{\xi})^{2}} = \D{\eta} - \frac{\cov^{2}(\xi, \eta)}{\D{\xi}}. 
			\qedhere
	\end{multline*}
\end{proof}

У данной теоремы есть многомерное обобщение.
\begin{theorem}[о нормальной корреляции, общий 
случай]\label{normal-correlation-theorem}
	Пусть \(\mathbf{X} \sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})\)~--- 
	\(n\)-мерный гауссовский вектор. Сделаем следующее разбиение:
	\[
		\mathbf{X} = \begin{pmatrix}
		\bm{\xi} \\ \bm{\eta}
		\end{pmatrix} \text{ с размерами }
		\begin{pmatrix}
		q \times 1 \\
		(n - q) \times 1
		\end{pmatrix}
	\]
	
	Соответственным образом вводятся разбиения матожидания и матрицы ковариаций:
	\begin{gather}
		\bm{\mu} = \begin{pmatrix}
		\bm{\mu}_{1} \\ \bm{\mu}_{2}
		\end{pmatrix} \text{ с размерами }
		\begin{pmatrix}
		q \times 1 \\
		(n - q) \times 1
		\end{pmatrix} \\
		\bm{\Sigma} = \begin{pmatrix}
		\bm{\Sigma}_{11} & \bm{\Sigma}_{12} \\
		\bm{\Sigma}_{21} & \bm{\Sigma}_{22}
		\end{pmatrix} \text{ с размерами }
		\begin{pmatrix}
		q \times q & q \times (n - q) \\
		(n - q) \times q & (n - q) \times (n - q)
		\end{pmatrix}
	\end{gather}
	
	Тогда верны следующие формулы:
	\begin{gather}
		\E{\bm{\xi} \given \bm{\eta}} = \bm{\mu}_{1} + 
		\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1}(\bm{\eta} - \bm{\mu}_{2}), \\
		\Delta = \bm{\Sigma}_{11} - 
		\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1}\bm{\Sigma}_{21}.
	\end{gather}
\end{theorem}
\begin{proof}
	Для начала найдём линейную комбинацию \(\bm{\delta} = \bm{\xi} + 
	\mathbf{A}\bm{\eta}\) такую, что она некоррелирована (а следовательно, и 
	независима) с \(\bm{\eta}\). Для этого распишем корреляцию:
	\[
		\cov(\bm{\delta}, \bm{\eta}) = \cov(\bm{\xi}, \bm{\eta}) + 
		\cov(\mathbf{A}\bm{\eta}, \bm{\eta}) = \cov(\bm{\xi}, \bm{\eta}) + 
		\mathbf{A}\D{\bm{\eta}} = \bm{\Sigma}_{12} + \mathbf{A}\bm{\Sigma}_{22} 
		\implies \mathbf{A} = -\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1}.
	\]
	
	Отсюда получаем, что
	\begin{multline*}
		\E{\bm{\xi} \given \bm{\eta}} = \E{\bm{\delta} - \mathbf{A}\bm{\eta} 
		\given \bm{\eta}} = \E{\bm{\delta} \given \bm{\eta}} - 
		\E{\mathbf{A}\bm{\eta} \given \bm{\eta}} = \E{\bm{\delta}} - 
		\mathbf{A}\bm{\eta} = \bm{\mu}_{1} + \mathbf{A}(\bm{\mu}_{2} - 
		\bm{\eta}) = \\ = \bm{\mu}_{1} + 
		\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1}(\bm{\eta} - \bm{\mu}_{2}).
	\end{multline*}
	
	Теперь посмотрим на то, как себя ведёт  \(\Delta\). Как известно, по 
	формуле 
	полной вероятности:
	\[
		\Delta = \E{(\bm{\xi} - \E{\bm{\xi} \given \bm{\eta}})^{2}} = 
		\E{\E{(\bm{\xi} - \E{\bm{\xi} \given \bm{\eta}})^{2} \given \bm{\eta}}} 
		= \E{\D{\bm{\xi} \given \bm{\eta}}}.
	\]
	
	Теперь посмотрим на условную дисперсию:
	\[
		\D{\bm{\xi} \given \bm{\eta}} = \D{\bm{\delta} - \mathbf{A}\bm{\eta} 
		\given \bm{\eta}} = \D{\bm{\delta} \given \bm{\eta}} + 
		\D{\mathbf{A}\bm{\eta} \given \bm{\eta}} - \cov(\bm{\delta}, 
		\mathbf{A}\bm{\eta} \mid \bm{\eta}) - \cov(\mathbf{A}\bm{\eta}, 
		\bm{\delta} \mid \bm{\eta}).
	\]
	
	Как известно, матожидание вектора, умноженного на матрицу слева/справа, 
	равно матожиданию вектора, умноженного на эту же матрицу слева/справа, а 
	условная дисперсия случайной величины, не зависящей от условия, равна 
	обычной дисперсии. Тогда это равно
	\[
		\D{\bm{\delta} \given \bm{\eta}} + \mathbf{A}\D{\bm{\eta} \given 
		\bm{\eta}}\mathbf{A}^{\intercal} - \cov(\bm{\delta}, \bm{\eta} \mid 
		\bm{\eta})\mathbf{A}^{\intercal} - \mathbf{A}\cov(\bm{\eta}, 
		\bm{\delta} \mid \bm{\eta}) = \D{\bm{\delta} \given \bm{\eta}} = 
		\D{\bm{\delta}}.
	\]
	
	Осталось посчитать эту дисперсию. Для этого распишем дисперсию суммы, 
	пользуясь свойствами матриц \(\bm{\Sigma}_{ij}\):
	\begin{align*}
		\D{\bm{\delta}} &= \D{\bm{\xi} + \mathbf{A}\bm{\eta}} = \D{\bm{\xi}} + 
		\D{\mathbf{A}\bm{\eta}} + \cov(\bm{\xi}, \mathbf{A}\bm{\eta}) + 
		\cov(\mathbf{A}\bm{\eta}, \bm{\xi}) = \\
		&= \D{\bm{\xi}} + \mathbf{A}\D{\bm{\eta}}\mathbf{A}^{\intercal} + 
		\cov(\bm{\xi}, \bm{\eta})\mathbf{A}^{\intercal} + 
		\mathbf{A}\cov(\bm{\eta}, \bm{\xi}) = \\
		&= \bm{\Sigma}_{11} + (-\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1}) 
		\bm{\Sigma}_{22}(-\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1})^{\intercal} + 
		\bm{\Sigma}_{12}(-\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1})^{\intercal} + 
		(-\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1})\bm{\Sigma}_{21} = \\
		&= \bm{\Sigma}_{11} + \bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1}
		\bm{\Sigma}_{22} \bm{\Sigma}_{22}^{-1}\bm{\Sigma}_{21} - 
		\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1}\bm{\Sigma}_{21} - 
		\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1}\bm{\Sigma}_{21} = \\
		&= \bm{\Sigma}_{11} - 
		\bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1}\bm{\Sigma}_{21}
	\end{align*}
	Так как матожидание константы есть сама константа, то \(\Delta = 
	\bm{\Sigma}_{11} - \bm{\Sigma}_{12}\bm{\Sigma}_{22}^{-1}\bm{\Sigma}_{21}\).
\end{proof}

\subsection{Примеры случайных процессов}
Наше небольшое введение закончилось. Теперь можно посмотреть на несколько 
основных примеров случайных процессов.
\subsubsection{Гауссовский и винеровский процессы}
Многие процессы, которые попадаются на практике, обладают так называемыми 
независимыми приращениями. Что это значит?
\begin{definition}
	Пусть \(X = (X_{t})_{t \geq 0}\)~--- некоторый случайный процесс. Будем 
	говорить, что \(X\) есть \emph{процесс с независимыми приращениями}, если 
	для любых \(t_{0}, t_{1}, \dots, t_{n} \in T\) таких, что \(0 = t_{0} < 
	t_{1} < \ldots < t_{n}\) случайные величины \(X_{t_{0}}\), \(X_{t_{1}} - 
	X_{t_{0}}, \dots, X_{t_{n}} - X_{t_{n - 1}}\) независимы в 
	совокупности.
\end{definition}

В 1827 году Роберт Броун открыл движение пыльцевых зёрен в жидкости. Исследуя 
пыльцу под микроскопом, он установил, что в растительном соке плавающие 
пыльцевые зёрна двигаются совершенно хаотически зигзагообразно во все стороны. 
В дальнейшем это хаотическое движение назвали \emph{броуновским}. Для его 
математического описания испольузется так называемый \emph{винеровский 
процесс}. Как он вводится?
\begin{definition}
	Случайный процесс \(B = (B_{t})_{t \geq 0}\) называется винеровским, если 
	для него выполнены следующие условия:
	\begin{enumerate}
		\item \(B_{0} = 0\) почти наверное.
		\item \(B\)~--- процесс с независимыми приращеними.
		\item \(B_{t} - B_{s} \sim \mathcal{N}(0, t - s)\) для любых 
		\(0 \leq s < t < +\infty\).
		\item \(B\) имеет непрерывные почти наверное траектории, то есть с 
		вероятностью 1 \(B_{t}\) непрерывна, как функция от \(t\).
	\end{enumerate}
\end{definition}

Обычно наряду с винеровскими процессами вводят \emph{гауссовские процессы}.
\begin{definition}
	Случайный процесс \(X = (X_{t})_{t \in T}\) называется гауссовским, если 
	все его конечномерные функции распределения являются гауссовскими, то есть 
	задают гауссовский вектор.
\end{definition}
\begin{property}
	Гауссовский процесс однозначно определяется своим математическим ожиданием 
	и ковариационной функцией.
\end{property}
\begin{example}
	Оказывается, что винеровский процесс является гауссовским. Действительно, 
	возьмём произвольные \(t_{0}, t_{1}, \dots, t_{n}\) таким образом, что \(0 
	= t_{0} < t_{1} < \dots < t_{n}\). Как известно, у винеровского процесса 
	независимые приращения. Следовательно, \(B_{t_{0}}\), \(B_{t_{1}} - 
	B_{t_{0}}, \dots, B_{t_{n}} - B_{t_{n - 1}}\) независимы в 
	совокупности и образуют гауссовский вектор. Теперь поймём, какие 
	распределения имеют \(B_{t_{0}}, B_{t_{1}}, \dots, B_{t_{n}}\). Для этого 
	заметим, что
	\[
	\begin{pmatrix}
	B_{t_{0}} \\ B_{t_{1}} \\ B_{t_{2}} \\ \vdots \\ B_{t_{n}}
	\end{pmatrix}
	=
	\begin{pmatrix}
	1 & 0 & 0 & \ldots & 0 \\
	1 & 1 & 0 & \ldots & 0 \\
	1 & 1 & 1 & \ldots & 0 \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	1 & 1 & 1 & \ldots & 1
	\end{pmatrix}
	\begin{pmatrix}
	B_{t_{0}} \\ B_{t_{1}} - B_{t_{0}} \\ B_{t_{2}} - B_{t_{1}} \\ \vdots \\ 
	B_{t_{n}} - B_{t_{n - 1}}
	\end{pmatrix}
	\]

	Следовательно, \(B_{t_{i}} \sim \mathcal{N}(0, t_{i})\) и они образуют 
	гауссовский вектор.
\end{example}
Из этого сразу же получаем, что \(\E{B_{t}} = 0\). Теперь покажем, что 
\(R_{B}(t_{1}, t_{2}) = \min(t_{1}, t_{2})\). Без ограничения общности будем 
считать, что \(t_{1} < t_{2}\). Тогда
\[
	R_{B}(t_{1}, t_{2}) = \E{B_{t_{1}}B_{t_{2}}} = \E{B_{t_{1}}((B_{t_{2}} - 
	B_{t_{1}}) + B_{t_{1}})} = \E{B_{t_{1}}(B_{t_{2}} - 
	B_{t_{1}})} + \E{B_{t_{1}}^{2}}.
\]

Так как \(B_{t_{1}}\) и \(B_{t_{2}} - B_{t_{1}}\) независимы, то 
\(\E{B_{t_{1}}(B_{t_{2}} - B_{t_{1}})} = \E{B_{t_{1}}}\E{B_{t_{2}} - 
B_{t_{1}}} = 0\). Тем самым мы получаем, что \(R_{B}(t_{1}, t_{2}) = t_{1}\).

\subsubsection{Процесс Орнштейна-Уленбека}\label{ornstein-uhlenbeck-process}
Этот процесс пошёл из теории стохастических дифференциальных уравнений. Пусть 
\(B = (B_{t})_{t \geq 0}\)~--- винеровский процесс. Построим по нему новый 
процесс \(X = (X_{t})_{t \geq 0}\) по следующему правилу: \(X_{t} = 
e^{-t}B_{e^{2t}}\). Полученный процесс называется \emph{процессом 
Орнштейна-Уленбека}. Каковы его свойства?

\begin{property}
	\(\E{X_{t}} = 0\), \(R_{X}(t, s) = e^{-|t - s|}\).
\end{property}
\begin{proof}
	Первая часть очевидна: \(\E{X_{t}} = \E{e^{-t}B_{e^{2t}}} = 0\). Теперь 
	рассмотрим ковариационную функцию:
	\[
		R_{X}(t, s) = \E{X_{t}X_{s}} = e^{-(s + t)}\E{B_{e^{2t}}B_{e^{2s}}} = 
		e^{2\min(t, s) - (s + t)} = e^{-|t - s|}. \qedhere
	\]
\end{proof}
\begin{property}
	Процесс Орнштейна-Уленбека является гауссовским.
\end{property}
\begin{proof}
	Без ограничения общности зафиксируем числа \(t_{1} < t_{2} < \ldots < 
	t_{n}\). Рассмотрим случайный вектор \((X_{t_{1}}, \ldots, X_{t_{n}})\). 
	Как он устроен? Распишем последний член:
	\begin{multline*}
		X_{t_{n}} = e^{-t_{n}}B_{e^{2t_{n}}} = e^{-t_{n}}(B_{e^{2t_{n}}} - 
		B_{e^{2t_{n - 1}}}) + e^{-t_{n}}B_{e^{2t_{n - 1}}} = \ldots = \\ = 
		e^{-t_{n}}\sum_{k = 1}^{n - 1} (B_{e^{2t_{k + 1}}} - 
		B_{e^{2t_{k}}}) + e^{-t_{n}}B_{e^{2t_{1}}}
	\end{multline*}
	
	Далее, нам известно, что \(B_{e^{2t_{1}}}\), \(B_{e^{2t_{2}}} - 
	B_{e^{2t_{1}}}, \ldots, B_{e^{2t_{n}}} - B_{e^{2t_{n - 1}}}\) независимы в 
	совокупности и имеют следующие распределения:
	\[
		B_{e^{2t_{1}}} \sim \mathcal{N}(0, e^{2t_{1}}), \quad B_{e^{2t_{k}}} - 
		B_{e^{2t_{k - 1}}} \sim \mathcal{N}(0, e^{2t_{k}} - e^{2t_{k - 1}}).
	\]
	
	Осталось заметить, что
	\[
	\begin{pmatrix}
	X_{t_{1}} \\ X_{t_{2}} \\ X_{t_{3}} \\ \vdots \\ X_{t_{n}}
	\end{pmatrix}
	=
	\begin{pmatrix}
	e^{-t_{1}} & 0 & 0 & \ldots & 0 \\
	e^{-t_{2}} & e^{-t_{2}} & 0 & \ldots & 0 \\
	e^{-t_{3}} & e^{-t_{3}} & e^{-t_{3}} & \ldots & 0 \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	e^{-t_{n}} & e^{-t_{n}} & e^{-t_{n}} & \ldots & e^{-t_{n}}
	\end{pmatrix}
	\begin{pmatrix}
	B_{e^{2t_{1}}} \\ B_{e^{2t_{2}}} - B_{e^{2t_{1}}} \\ B_{e^{2t_{3}}} - 
	B_{e^{2t_{2}}} \\ \vdots \\ B_{e^{2t_{n}}} - B_{e^{2t_{n - 1}}}
	\end{pmatrix}
	\]
	
	Отсюда получаем, что \((X_{t_{1}}, \dots, X_{t_{n}})\)~--- действительно 
	гауссовский вектор.
\end{proof}
Теперь посчитаем совместную плотность такого вектора. Для этого выпишем 
совместноую плотность \(\bm{\xi} = (B_{e^{2t_{1}}}, B_{e^{2t_{2}}} - 
B_{e^{2t_{1}}}, \ldots, B_{e^{2t_{n}}} - B_{e^{2t_{n - 1}}})\). Так как 
компоненты независимы, то она равна произведению плотностей каждой компоненты:
\[
	p_{\bm{\xi}}(x_{1}, \dots, x_{n}) = \frac{1}{\sqrt{2\pi}e^{t_{1}}} 
	\exp\left\{-\frac{x_{1}^{2}}{2e^{2t_{1}}}\right\}\prod_{k = 2}^{n} 
	\frac{1}{\sqrt{2\pi(e^{2t_{k}} - e^{2t_{k - 1}})}} 
	\exp\left\{-\frac{x_{k}^{2}}{2(e^{2t_{k}} - e^{2t_{k - 1}})}\right\}
\]

Теперь выразим компоненты вектора \(\bm{\xi}\) через компоненты вектора 
\(\bm{\eta} = (X_{1}, X_{2}, \ldots, X_{n})\):
\[
	\begin{cases}
	B_{e^{2t_{1}}} = e^{t_{1}}X_{1} \\
	B_{e^{2t_{2}}} = e^{t_{2}}X_{2} \\
	\dots \\
	B_{e^{2t_{n}}} = e^{t_{n}}X_{n}
	\end{cases}
	\implies 
	\begin{cases}
	B_{e^{2t_{1}}} = e^{t_{1}}X_{1} \\
	B_{e^{2t_{2}}} - B_{e^{2t_{1}}} = e^{t_{2}}X_{2} - e^{t_{1}}X_{1} \\
	\dots \\
	B_{e^{2t_{n}}} - B_{e^{2t_{n - 1}}} = e^{t_{n}}X_{n} - e^{t_{n - 1}}X_{n - 
	1}
	\end{cases}
\]

В матричном виде это можно записать следующим образом:
\[
	\begin{pmatrix}
	B_{e^{2t_{1}}} \\ B_{e^{2t_{2}}} - B_{e^{2t_{1}}} \\ B_{e^{2t_{3}}} - 
	B_{e^{2t_{2}}} \\ \vdots \\ B_{e^{2t_{n}}} - B_{e^{2t_{n - 1}}}
	\end{pmatrix}
	=
	\begin{pmatrix}
	e^{t_{1}} & 0 & 0 & \ldots & 0 \\
	-e^{t_{1}} & e^{t_{2}} & 0 & \ldots & 0 \\
	0 & -e^{t_{2}} & e^{t_{3}} & \ldots & 0 \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	0 & 0 & 0 & \ldots & e^{t_{n}}
	\end{pmatrix}
	\begin{pmatrix}
	X_{t_{1}} \\ X_{t_{2}} \\ X_{t_{3}} \\ \vdots \\ X_{t_{n}}
	\end{pmatrix}
\]

Несложно понять, что матрица перехода служит матрицей Якоби и её определитель 
равен \(e^{t_{1} + \dots + t_{n}}\). Следовательно, 
\[
	p_{\bm{\eta}}(x_{1}, \dots, x_{n}) = e^{t_{1} + \dots + 
	t_{n}}p_{\bm{\xi}}(e^{t_{1}}x_{1}, e^{t_{2}}x_{2} - e^{t_{1}}x_{1}, \dots, 
	e^{t_{n}}x_{n} - e^{t_{n - 1}}x_{n - 1}).
\]

Подставляя, получаем, что
\begin{align*}
	p_{\bm{\eta}}(x_{1}, \dots, x_{n}) &=\frac{e^{t_{1}}}{\sqrt{2\pi}e^{t_{1}}} 
	\exp\left\{-\frac{x_{1}^{2}e^{2t_{1}}}{2e^{2t_{1}}}\right\}\prod_{k = 
	2}^{n} \frac{e^{t_{k}}}{\sqrt{2\pi(e^{2t_{k}} - e^{2t_{k - 1}})}} 
	\exp\left\{-\frac{(e^{t_{k}}x_{k} - e^{t_{k - 1}}x_{k - 1})^{2}} 
	{2(e^{2t_{k}} - e^{2t_{k - 1}})}\right\} = \\
	&= \left(2\pi\prod_{k = 2}^{n}\left(1 - e^{2(t_{k - 1} - 
	t_{k})}\right)\right)^{-\frac{1}{2}}\exp\left\{-\frac{x_{1}^{2}}{2} - 
	\frac{1}{2}\sum_{k = 2}^{n}\frac{(x_{k} - x_{k - 1}e^{t_{k - 1} - 
	t_{k}})^{2}}{1 - e^{2(t_{k - 1} - t_{k})}}\right\}.
\end{align*}

\subsubsection{Пуассновский процесс}
Перейдём к одному из самых простых для исследования процессов~--- к 
\emph{пуассоновскому потоку}.
\begin{definition}\label{poisson-process-def}
	Случайный процесс \(X = (X_{t})_{t \geq 0}\) называется пуассоновским 
	потоком с \emph{интенсивностью} \(\lambda\), если он удовлетворяет трём 
	условиям:
	\begin{enumerate}
		\item \(X_{0} = 0\) почти наверное.
		\item \(X\)~--- процесс с независимыми приращениями.
		\item Для всех \(0 \leq s < t < +\infty\) \(X_{t} - X_{s} \sim 
		\mathrm{Pois}(\lambda(t - s))\), то есть для любого \(n \in \Z_{+}\)
		\[
			\Pr{X_{t} - X_{s} = n} = \frac{(\lambda(t - 
			s))^{n}}{n!}e^{\lambda(t - s)}.
		\]
	\end{enumerate}
\end{definition}

Теперь построим пример такого процесса. Для этого вспомним процесс 
восстановления, описанный в \hyperref[counting-process]{примере 
\ref*{counting-process}}.

Пусть для любого натурального \(n\) \(T_{n}\)~--- это iid случайные величины с 
распределением \(\mathrm{Exp}(\lambda)\), то есть их плотность равна \(p(z) = 
\lambda e^{-\lambda z}\I\{z \geq 0\}\). Дальше, \(S_{n} = T_{1} + \ldots + 
T_{n}\), а для любого \(t \geq 0\)
\[
	N_{t} = \sum_{n = 1}^{\infty} \I\{S_{n} \leq t\} = \#\{n \in \N : S_{n} < 
	t\}.
\]

Полученный случайный процесс обладает некоторыми интересными свойствами.
\begin{property}
	Для любого \(t > 0\) \(N_{t} \sim \mathrm{Pois}(\lambda t)\).
\end{property}
\begin{proof}
	Понятно, что \(N_{t}\) принимает значения в \(\Z_{+}\). Следовательно, 
	достаточно показать, что вероятности принять нужное значение будут именно 
	такими, какими они должны быть.
	
	Пусть \(n = 0\). Тогда
	\[
		\Pr{N_{t} = 0} = \Pr{T_{1} > t} = \int\limits_{t}^{\infty} \lambda 
		e^{-\lambda x}\diff x = e^{-\lambda t} \int\limits_{t}^{\infty} \lambda 
		e^{-\lambda(x - t)}\diff x = e^{-\lambda t} = 
		\frac{(\lambda t)^{0}}{0!}e^{-\lambda t}.
	\]
	
	Теперь посмотрим на вероятность события \(N_{t} = n\). Она равна
	\[
		\Pr{N_{t} = n} = \Pr{N_{t} \geq n} - \Pr{N_{t} \geq n + 1} = \Pr{S_{n} 
		\leq t} - \Pr{S_{n + 1} \leq t}.
	\]
	
	Для того, чтобы посчитать полученные вероятности, вспомним один факт: сумма 
	\(n\) iid случайных величин с распределением \(\mathrm{Exp}(\lambda)\) 
	имеет гамма-распределение \(\Gamma(1/\lambda, n)\) с плотностью
	\[
		p_{n}(x) = \frac{\lambda^{n}x^{n - 1}e^{-\lambda x}}{\Gamma(n)} = 
		\frac{\lambda^{n}x^{n - 1}e^{-\lambda x}}{(n - 1)!}.
	\]
	
	Тогда эта вероятность равна
	\[
		\Pr{N_{t} = n} = \int\limits_{0}^{t} \left(\frac{\lambda^{n}x^{n - 
		1}e^{-\lambda x}}{(n - 1)!} - \frac{\lambda^{n + 1}x^{n}e^{-\lambda 
		x}}{n!}\right)\diff x = \int\limits_{0}^{t} 
		\left(\frac{\lambda^{n}x^{n}e^{-\lambda x}}{n!}\right)'\diff x = 
		\frac{(\lambda t)^{n}}{n!}e^{-\lambda t}. \qedhere
	\]
\end{proof}
\begin{consequence}
	\(\E{N_{t}} = \lambda t\), \(\D{N_{t}} = \lambda t\).
\end{consequence}

\begin{property}
	\(N = (N_{t})_{t \geq 0}\)~--- это пуассоновский поток с интенсивностью 
	\(\lambda\).
\end{property}
\begin{proof}
	Для начала покажем, что \(N_{0} = 0\) почти наверное. Действительно, если 
	\(N_{0} \neq 0\), то \(T_{1} = 0\), что происходит с 
	нулевой вероятностью. Тем самым \(N_{0} = 0\) почти наверное.
	
	Теперь докажем, что \((N_{t})_{t \geq 0}\) удовлетворяет двум последним 
	свойствам. Для этого заметим, что
	\[
	\begin{pmatrix}
	S_{1} \\ S_{2} \\ S_{3} \\ \vdots \\ S_{n} 
	\end{pmatrix}
	=
	\begin{pmatrix}
	1 & 0 & 0 & \ldots & 0 \\
	1 & 1 & 0 & \ldots & 0 \\
	1 & 1 & 1 & \ldots & 0 \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	1 & 1 & 1 & \ldots & 1 \\
	\end{pmatrix}
	\begin{pmatrix}
	T_{1} \\ T_{2} \\ T_{3} \\ \vdots \\ T_{n} 
	\end{pmatrix}
	\]
	
	Так как у матрицы единичный определитель и замена линейна, то
	\[
	p_{S_{1}, \dots, S_{n}}(x_{1}, x_{2}, \dots, x_{n}) = p_{T_{1}, \dots, 
		T_{n}}(x_{1}, x_{2} - x_{1}, \dots, x_{n} - x_{n - 1})
	\]
	
	Пользуясь независимостью \(T_{n}\), получаем, что
	\[
	p_{S_{1}, \dots, S_{n}}(x_{1}, x_{2}, \dots, x_{n}) = \lambda 
	e^{-\lambda x_{1}}\prod_{k = 2}^{n}\lambda e^{-\lambda(x_{k} - x_{k - 
			1})}\I\{x_{k} - x_{k - 1} \geq 0\}
	\]
	
	После преобразования получаем, что
	\[
	p_{S_{1}, \dots, S_{n}}(x_{1}, x_{2}, \dots, x_{n}) = 
	\lambda^{n}e^{-\lambda x_{n}}\I\{x_{n} \geq x_{n - 1} \geq \ldots \geq 
	x_{1} \geq 0\}
	\]
	
	Дальше, зафиксируем какие-либо числа \(0 \leq t_{1} < t_{2} < \dots < 
	t_{n}\), \(k_{1} \leq k_{2} \leq \dots \leq k_{n}\) и посмотрим на 
	следующую вероятность:
	\[
	\Pr{N_{t_{1}} = k_{1}, N_{t_{2}} - N_{t_{1}} = k_{2} - k_{1}, \dots, 
		N_{t_{n}} - N_{t_{n - 1}} = k_{n} -  k_{n - 1}}
	\]
	
	Поймём, как связать это с \(S_{n}\). Для этого поймём, как устроено первое 
	условие. Оно означает, что \(S_{1}, \dots, S_{k_{1}} \leq t_{1}\), а 
	\(S_{k_{1} + 1} > t_{1}\). Аналогично, получаем, что эта вероятность равна
	\[
	\Pr{S_{1}, \dots, S_{k_{1}} \in (0, t_{1}], S_{k_{1} + 1}, \dots, 
		S_{k_{2}} \in (t_{1}, t_{2}], \ldots, S_{k_{n - 1} + 1}, \dots, 
		S_{k_{n}} \in (t_{n - 1}, t_{n}], S_{k_{n} + 1} > t_{n}}
	\]
	
	Пользуясь плотностью случайного вектора из \(S_{k}\), запишем это в виде 
	интеграла:
	\[
	\idotsint\limits_{\mathclap{\substack{0 < x_{1}, \ldots, x_{k_{1}} \leq 
				t_{1} \\ t_{1} < x_{k_{1} + 1}, \ldots, x_{k_{2}} \leq t_{2} \\ 
				\dots 
				\\ t_{n - 1} < x_{k_{n - 1} + 1}, \ldots, x_{k_{n}} \leq t_{n} 
				\\ 
				x_{k_{n} + 1} > t_{n}}}} \lambda^{k_{n} + 1}e^{-\lambda 
				x_{k_{n} + 
			1}}\I\{x_{k_{n} + 1} \geq x_{k_{n}} \geq \ldots \geq x_{1} \geq 
	0\}\diff x_{1} \ldots \diff x_{k_{n}} \diff x_{k_{n} + 1}
	\]
	
	Разобъём его в произведение интегралов, положив \(k_{0} = t_{0} = 0\):
	\[
	\lambda^{k_{n}}\int\limits_{t_{n}}^{\infty} \lambda e^{-\lambda 
		x_{k_{n} + 1}}\diff x_{k_{n} + 1}\prod_{j = 1}^{n} 
	\int\limits_{\substack{t_{j - 1} < x_{k_{j - 1} + 
				1} \leq \ldots \leq x_{k_{j}} \leq t_{j}}} \diff x_{k_{j - 1} + 
				1} 
	\ldots \diff x_{k_{j}}
	\]
	
	Интегралы в произведении берутся достаточно просто: это объём симплекса. 
	Рассуждая по аналогии с трёхмерным случаем, получаем, что интеграл равен
	\[
	\lambda^{k_{n}}e^{-\lambda t_{n}}\prod_{j = 1}^{n} \frac{(t_{j} - t_{j 
			- 1})^{k_{j} - k_{j - 1}}}{(k_{j} - k_{j - 1})!}
	\]
	
	Теперь сделаем подгон: заметим, что
	\[
	k_{n} = k_{n} - k_{0} = \sum_{j = 1}^{n} (k_{j} - k_{j - 1}), \quad 
	t_{n} = t_{n} - t_{0} = \sum_{j = 1}^{n} (t_{j} - t_{j - 1})
	\]
	
	Тогда интеграл равен
	\[
	\prod_{j = 1}^{n} \frac{(\lambda(t_{j} - t_{j - 1}))^{k_{j} - 
			k_{j - 1}}}{(k_{j} - k_{j - 1})!}e^{-\lambda(t_{j} - t_{j - 1})}.
	\]
	
	Какая красота. Отсюда мы сразу получаем оба свойства. Тем самым процесс 
	восстановления для экспоненциального распределения является пуассоновским 
	потоком с интенсивностью \(\lambda\).
\end{proof}

Следующее свойство связано с понятием стационарных приращений.
\begin{definition}
	Случайный процесс \(X = (X_{t})_{t \geq 0}\) имеет \emph{стационарные 
	приращения}, если для любых \(0 \leq t_{0} < t_{1} < t_{2} < \ldots < 
	t_{n} < +\infty\) и \(\forall h \geq 0\) 
	\[
		(X_{t_{1} + h} - X_{t_{0} + h}, \dots, X_{t_{n} + h} - X_{t_{n - 1} + 
		h}) \eqdist (X_{t_{1}} - X_{t_{0}}, \dots, X_{t_{n}} - X_{t_{n - 1}}).
	\]
\end{definition}
\begin{property}
	Пуассоновский поток имеет стационарные приращения.
\end{property}
\begin{proof}
	Следует из того, что \(X_{a + h} - X_{b + h} \sim \mathrm{Pois}(a - b) \sim 
	X_{a} - X_{b}\) и приращения независимы.
\end{proof}

\begin{property}
	Пусть \(N^{1}, \ldots, N^{k}\)~--- независимые пуассоновские потоки с 
	интенсивностями \(\lambda_{1}, \ldots, \lambda_{k}\). Тогда случайный 
	процесс \(N_{t} = N_{t}^{1} + \ldots + N_{t}^{k}\)~--- это пуассоновский 
	процесс с интенсивностью \(\lambda = \lambda_{1} + \ldots + \lambda_{k}\).
\end{property}

\subsection{Стационарность случайных процессов}
На практике часто попадаются процессы, которые неизменны во времени. Их принято 
называть \emph{стационарными}. Это понятие было введено и для случайных 
процессов, хоть и не в одной ипостаси.

Буквальный перевод вышесказанного на математический язык даёт 
\emph{стационарность в узком смысле}.
\begin{definition}
	Случайный процесс \(X = (X_{t})_{t \in T}\) называется сильно стационарным 
	(strong sense stationary, SSS, стационарным в узком смысле), если для 
	любого натурального \(n\), любых индексов \(t_{1}, t_{2}, \ldots, t_{n} \in 
	T\) и любого сдвига \(h \geq 0\)
	\[
		F_{t_{1} + h, \ldots, t_{n} + h}(x_{1}, \dots, x_{n}) = F_{t_{1}, 
		\ldots, t_{n}}(x_{1}, \dots, x_{n}).
	\]
\end{definition}
\begin{example}
	Последовательность iid случайных величин~--- это стационарная в узком 
	смысле случайная последовательность.
\end{example}
\begin{example}
	Винеровский процесс не является стационарным в узком смысле. Действительно, 
	пусть \(n = 1\) и \(h > 0\). Тогда \(B_{t} \sim \mathcal{N}(0, t)\), а 
	\(B_{t + h} \sim \mathcal{N}(0, t + h)\) и \(F_{t + h}(x) \neq F_{t}(x)\).
\end{example}
У стационарных в узком смысле процессов есть одно полезное свойство. Но у него 
есть требование~--- процесс должен быть второго порядка. Что это значит?
\begin{definition}
	Случайный процесс \((X_{t})_{t \in T}\) называется \emph{процессом второго 
	порядка}, если \(\E{X_{t}^{2}}\) конечно для всех \(t\) (то есть это 
	ограниченная функция от \(t\)).
\end{definition}
\begin{property}
	Если \((X_{t})_{t \in T}\)~--- стационарный в узком смысле процесс второго 
	порядка, то
	\begin{enumerate}
		\item \(m(t) = \mu = const\), \(D(t) = \sigma^{2} = const\).
		\item Для любых \(t_{1}, t_{2}, h \in T\) \(R_{X}(t_{1}, t_{2}) = 
		R_{X}(t_{1} + h, t_{2} + h)\).
	\end{enumerate}
\end{property}
Данное свойство очевидным образом следует из определения стационарного в узком 
смысле процесса и того, что матожидание, дисперсия и ковариация конечны. 
Последнее свойство позволяет свести ковариационную функцию к одному аргументу: 
\(R_{X}(t_{1}, t_{2}) = R_{X}(0, t_{2} - t_{1}) \equiv R_{X}(t_{2} - t_{1})\).

Вообще говоря, выполнение этих трёх свойств~--- это тоже в некоторой степени 
стационарность. Только в широком смысле.
\begin{definition}
	Случайный процесс \(X = (X_{t})_{t \in T}\) называется слабо стационарным 
	(стационарным в широком смысле, wide sense stationary, WSS, ковариационно 
	стационарным, стационарным второго порядка),\footnote{Название 
	ковариационной стационарности появилось из-за того, что ковариационная 
	функция зависит только от разности индексов. Стационарность второго порядка 
	же означает постоянство второго момента.} если выполнены следующие
	условия:
	\begin{enumerate}
		\item \(m(t) = \mu = const\), \(D(t) = \sigma^{2} = const\).
		\item Для любых \(t_{1}, t_{2}, h \in T\) \(R_{X}(t_{1}, t_{2}) = 
		R_{X}(t_{1} + h, t_{2} + h)\).
	\end{enumerate}
\end{definition}

Обычно сильная и слабая стационарности идут вместе (если это не так, то что-то 
пошло не туда). Например, если семейство конечномерных функций распределения 
полностью задаётся матожиданием и ковариационной функцией, то сильная 
стационарность равносильна слабой стационарности.

У ковариационной функции стационарного в широком смысле случайного процесса 
есть несколько свойств:
\begin{enumerate}
	\item Она неотрицательна в нуле: \(R_{X}(0) = R_{X}(t, t) = D(t) = 
	\sigma^{2} \geq 0\).
	\item Она чётна: \(R_{X}(-\tau) = R_{X}(0, -\tau) = R_{X}(-\tau, 0) = 
	R_{X}(0, \tau) = R_{X}(\tau)\).
	\item Она ограничена по модулю дисперсией. Действительно, по неравенству 
	Коши-Буня\-ковского-Шварца \(|R_{X}(\tau)| = |R_{X}(t, t + 
	\tau)| = |\cov(X_{t}, X_{t + \tau})| \leq \sqrt{\D{X_{t}}\D{X_{t + \tau}}} 
	= \sigma^{2}\).
	\item Аналог неотрицательной определённости: для любого натурального \(n\), 
	любого неслучайного вектора \((z_{1}, \ldots, z_{n})\) и любого набора 
	индексов \(t_{1}, \ldots, t_{n}\)
	\[
		\sum_{i, j = 1}^{n} R_{X}(t_{i} - t_{j})z_{i}z_{j} \geq 0.
	\]
	\item Если \(R_{X}(\tau)\) непрерывна в нуле, то она непрерывна для любого 
	\(\tau\).
\end{enumerate}
\begin{leftbar}
\begin{small}\noindent
\begin{proof}
	Пусть \(\xi_{t} = X_{t} - \E{X_{t}}\). Пользуясь этим обозначением, 
	распишем разность ковариационных функций.
	\begin{multline*}
		R_{X}(t_{1} + h_{1}, t_{2} + h_{2}) - R_{X}(t_{1}, t_{2}) = 
		\E{\xi_{t_{1} + h_{1}}\xi_{t_{2} + h_{2}} - \xi_{t_{1}}\xi_{t_{2}}} = 
		\\ = \E{\xi_{t_{1} + h_{1}}(\xi_{t_{2} + h_{2}} - \xi_{t_{2}})} + 
		\E{(\xi_{t_{1} + h_{1}} - \xi_{t_{1}})\xi_{t_{2}}}
	\end{multline*}
	
	Далее, по неравенству треугольника:
	\[
		|R_{X}(t_{1} + h_{1}, t_{2} + h_{2}) - R_{X}(t_{1}, t_{2})| \leq 
		|\E{\xi_{t_{1} + h_{1}}(\xi_{t_{2} + h_{2}} - \xi_{t_{2}})}| + 
		|\E{(\xi_{t_{1} + h_{1}} - \xi_{t_{1}})\xi_{t_{2}}}|
	\]
	
	Теперь воспользуемся неравенством Коши-Буняковского-Шварца:
	\[
		|R_{X}(t_{1} + h_{1}, t_{2} + h_{2}) - R_{X}(t_{1}, t_{2})| \leq 
		\sqrt{\E{\xi_{t_{1} + 
		h_{1}}^{2}}\E{(\xi_{t_{2} + h_{2}} - \xi_{t_{2}})^{2}}} + 
		\sqrt{\E{(\xi_{t_{1} + h_{1}} - \xi_{t_{1}})^{2}}\E{\xi_{t_{2}}^{2}}}
	\]
	
	Осталось показать, что эта сумма стремится к нулю. Покажем, что первый член 
	уходит в ноль (второй рассматривается аналогично). Действительно, по 
	непрерывности \(R_{X}(t, t)\)
	\begin{multline*}
		\E{(\xi_{t_{2} + h_{2}} - \xi_{t_{2}})^{2}} = \E{\xi_{t_{2} + 
		h_{2}}^{2}} - 2\E{\xi_{t_{2} + h_{2}}\xi_{t_{2}}} + \E{\xi_{t_{2}}^{2}} 
		= R_{X}(t_{2} + h_{2}, t_{2} + h_{2}) + \\ + R_{X}(t_{2}, t_{2}) - 
		2R_{X}(t_{2} + h_{2}, t_{2}) \xrightarrow[h_{2} \to 0]{} 0.
	\end{multline*}
	Следовательно, \(R_{X}(t_{1} + h_{1}, t_{2} + h_{2}) \xrightarrow[h_{1}, 
	h_{2} \to 0]{} R_{X}(t_{1}, t_{2})\) и \(R_{X}(t_{1}, t_{2})\) непрерывна 
	везде.
\end{proof}
\end{small}
\end{leftbar}

Теперь посмотрим на три класса случайных процессов: IID, SSS и WSS. Есть ли 
между ними какая-либо связь? Есть. Начнём с очевидной цепочки вложений: IID 
\(\subseteq\) SSS \(\subseteq\) WSS. Хотя второе вложение не совсем 
корректно~--- не все сильно стационарные процессы являются процессами второго 
порядка. Если добавить это требование, то вложение станет корректным. Теперь 
покажем, что все вложения строгие.
\begin{itemize}
	\item Начнём с SSS \(\setminus\) IID. Возьмём случайную последовательность 
	\(X = (X_{t})_{t \in T}\), устроенную следующим образом: для всех \(t\) 
	\(X_{t} = \xi\), где \(\xi\)~--- это какая-то фиксированная случайная 
	величина. Она очевидно является стационарной в сильном смысле и все её 
	сечения одинаково распределены, но она не задаёт последовательность 
	независимых случайных величин.
	\item Теперь посмотрим на WSS \(\setminus\) SSS. Суть примера в том, что мы 
	будем брать разные распределения, у которых совпадают матожидание и 
	дисперсия. Например, пусть \(X = (X_{t})_{t \geq 0}\)~--- случайная 
	последовательность независимых случайных величин такая, что \(X_{2n} \sim 
	\mathrm{Bern}(p)\), а \(X_{2n + 1} \sim \mathcal{N}(p, p(1 - p))\). 
	Понятно, что ни о каком равенстве распределений и речи быть не может, а вот 
	слабая стационарность выполнена (почему?).
	\item Приведём ещё один пример процесса из SSS \(\setminus\) IID. Пусть 
	\(\{X_{n}\}_{n = 1}^{\infty}\)~--- последовательность iid случайных 
	величин. Построим по ней новую последовательность \(\{Y_{n}\}_{n = 
	1}^{\infty}\) по правилу \(Y_{n} = X_{n} + X_{n + 1}\). Понятно, что 
	полученная последовательность не будет состоять из независимых случайных 
	величин, но она будет сильно стационарной.
\end{itemize}

\subsection{Эргодичность случайных процессов}
Пусть \(X = (X_{t})_{t \geq 0}\)~--- какой-то случайный процесс. Как оценить 
\(\E{X_{t}}\)? Достаточно просто: берём \(N\) реализаций \(X_{t}(\omega_{1}), 
\ldots, X_{t}(\omega_{N})\) и берём их среднее арифметическое. Получается 
\emph{среднее по ансамблю траекторий}:
\[
	\hat{\mu}_{\omega} = \frac{1}{N}\sum_{k = 1}^{N} X_{t}(\omega_{k}).
\]

Теперь предположим, что у случайного процесса \(X\) постоянное матожидание 
\(\mu\). Можно ли тогда заменить усреднение по ансамблю траекторий на 
\emph{усреднение по времени}
\[
	\hat{\mu}_{t} = \frac{1}{n}\sum_{k = 1}^{n} X_{t_{k}}(\omega)?
\]

Если окажется так, что \(\hat{\mu}_{t} \sqmeanto \mu\) при \(n \to \infty\), то 
процесс \(X\) называют \emph{эргодическим в среднеквадратичном смысле}.
\begin{example}
	Возьмём последовательность iid случайных величин. Она эргодична в 
	среднеквадратичном смысле. Действительно,
	\[
		\E{\left(\frac{1}{n}\sum_{k = 1}^{n} X_{k} - \mu\right)^{2}} = 
		\D{\frac{1}{n}\sum_{k = 1}^{n} X_{k}} = \frac{\D{X_{1}}}{n} 
		\xrightarrow[n \to \infty]{} 0.
	\]
\end{example}
\begin{example}
	Возьмём случайную последовательность \(X = (X_{n})_{n \in \N}\), устроенную 
	следующим образом: для всех \(n\) \(X_{n} = \xi\), где \(\xi\)~--- это 
	какая-то фиксированная случайная величина. Она сильно стационарна, но не 
	эргодична, так как все траектории~--- константы.
\end{example}

Проверять эргодичность по определению~--- это явно занятие на любителя. Поэтому 
сформулируем без доказательства одну теорему, которая даёт критерий 
эргодичности (но для сходимости в среднем, а не в среднеквадратичном):
\begin{theorem}[условие Слуцкого]\label{slutsky-criterion}
	Слабо стационарный случайный процесс \(X = (X_{t})_{t \geq 0}\) является 
	эргодическим в среднем тогда и только тогда, когда 
	\[
		\lim\limits_{T \to \infty} 
		\frac{1}{T^{2}}\int\limits_{0}^{T}\int\limits_{0}^{T} R_{X}(t_{1}, 
		t_{2})\diff t_{1} \diff t_{2} = 0.
	\]
	
	Если же \(X\) является процессом второго порядка, то он будет эргодическим 
	тогда и только тогда, когда \(R_{X}(t) \to 0\) при \(t \to \infty\).
\end{theorem}