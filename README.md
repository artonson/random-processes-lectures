# Черновик конспектов лекций по курсу "Статистика случайных процессов".

Работы, на самом деле, ещё непочатый край.

Спимок претензий к этим конспектам:
* Общая претензия: критически не хватает примеров применения и примеров, которые позволяют понять определения.
* Лекция 1: в введение нужно добавить материал про условные матожидания относительно сигма-алгебр, а не только случайных величин. Это связяно с тем, что такие условные матожидиания вылезают в пятой лекции.
* (не критично) Лекция 1: хотелось бы добавить идею доказательства теоремы Колмогорова о существовании.
* Лекция 2: теорема о нормальной корреляции - нужно добавить слова про то, что это оптимальная оценка в среднеквадратичном смысле и обосновать это.
* Лекция 2: для чего была введена стационарность? Почему она вводится именно так?
* Лекция 2: эргодичность. Что это за зверь? Хотелось бы примеров.
* Лекция 2: хотелось бы увидеть доказательство критерия Слуцкого.
* Лекция 3: непонятка с неоднородным пуассоновским процессом. Как он определяется?
* Лекция 3: теорема Льюиса-Шедлера была НУ ОЧЕНЬ не очевидной. Хотелось бы добавить её обоснование.
* Лекция 3: Не очень понятно, нужно ли нормально рассказывать стохастическое интегрирование.
* Лекция 3: нужны примеры марковских цепей. 
* Лекция 3: смысл однородности? Смысл всех параметров состояний марковской цепи?
* Лекция 3: среднее время в нуле --- wut? Я не понял.
* Лекция 3: и снова эргодичность. Почему в этот раз определение принциально другое? Как оно связяно с ранее введённой эргодичностью?
* Лекция 3: первая эргодическая теорема. Опять без доказательства. 
* Лекция 3: для чего рассматриваем стационарный случай? Почему модель эргодична? Что нам это даёт?
* Лекция 4: нет примера марковского процесса.
* Лекция 4: недостаточно убедительно описаны дискретные скачки.
* Лекция 4: путаница с уравнениями Колмогорова, Маркова, Колмогорова-Маркова и так далее (кого там только нет, а).
* Лекция 5: не очень понятно - нужно ли рассказывать про мартингалы, непрерывный случай и прочие модели (наподобие TGARCH и HARCH)?
* Лекция 5: не очень понятно, почему мы отбрасываем простую гауссовость, но принимает условную гауссовость.
* Лекция 5: пример применения бы к каждой модели.
* Лекция 6: зачем функция Беллмана? Откуда она вообще взялась? Что она делает?
* Лекция 6: почему задача об объезде стран имеет оптимальные подструктуры?
* Лекция 6: общая путаница с обозначениями.
* Лекция 6: так как искать `phi_i` в задаче обучения с учителем?
* Лекция 6: слишком мало обоснования в алгоритме Витерби. Нет краткого заключения, описывающего основные плюсы и минусы.
* Лекция 6: ЕМ-алгоритм: почему он вообще сойдётся? Как быстро он сойдётся? Какие подводные камни? Какие плюсы? Почему нет алгоритма вперёд-назад и Баума-Велша?
* Лекция 6: спорное доказательство сходимости алгоритма. Нужно ли рассказывать дальше?
* Лекция 7: откуда ввобще вылезли эти пространства событий? Зачем? Что такое фильтр Калмана?
